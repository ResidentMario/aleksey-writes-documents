Dask is an increasingly popular Python-ecosystem SDK for managing large-scale ETL jobs and ETL pipelines across multiple machines. Albeit somewhat newer than Apache Spark â€” its best-known competitorâ€” Dask has captured a lot of mindshare in the data science community by virtue of its pandas and numpy-like API, which makes it easy to use and familiar to Pythonic data practitioners. In this tutorial, we will walk through setting up a Dask cluster on top of EMR (Elastic MapReduce), AWSâ€™s distributed data platform, that we can interact with and submit jobs to from a JupyterLab notebook running on our local machine. Weâ€™ll then run some basic benchmarks on this cluster by performing a basic exploratory data analysis of NYC Open Dataâ€™s 2019 Yellow Taxi Trip Dataset.  The Cloud Deployments page in the Dask docs covers your options for deploying Dask on the cloud. At the time of writing, the three options are: Kubernetes, EMR, and an ephemeral option using the "Dask Cloud Provider". My personal opinion is that EMR is the easiest way to get up and running with a distributed Dask cluster (if you want to experiment with it on a single machine, you can create a LocalCluster on your personal machine). Kubernetes is a complex service with a fairly steep learning curve, so I wouldnâ€™t recommend going that route unless youâ€™re already on a Kubernetes cluster and very familiar with how Kubernetes works. Note that itâ€™s also possible to deploy Dask on Google Cloud Dataproc or Azure HDInsight â€” any service that provides managed YARN will work â€” but there isnâ€™t any specific documentation on these alternative services at the moment.  EMR, short for "Elastic Map Reduce", is AWSâ€™s big data as a service platform. Hereâ€™s how it works. One of AWSâ€™s core offerings is EC2, which provides an API for reserving machines (so-called instances) on the cloud. EC2 provides a wide variety of options, ranging from tiny burstable shared CPUs (e.g. t2.micro) to beefy (and expensive!) GPU servers (e.g. p3.16xlarge). As a first step to launching an EMR cluster, consider what EC2 instance types you will use. For the purposes of this tutorial, I will launch a cluster with one m5.xlarge master node and two m5.xlarge worker nodes (m5.xlarge is AWSâ€™s recommended general-purpose CPU instance type). Note that when running on EMR, one of the instances will be reserved for the master node. The remainder become the worker pool. The next step is choosing your applications. The EMR API lets you reserve a set of EC2 machines that have been preconfigured with certain software onboard. The most popular application is probably HADOOP. Hadoop was an important predecessor to Apache Spark which introduced the world to MapReduce way back in 2006. Modern Hadoop consists of several reusable sub-components, many of which are now used by other tools. Two of these have proved to be particularly popular: the Hadoop Distributed File System, HDFS, and the Hadoop scheduling engine, YARN (short for Yet Another Resource Negotiator â€” very tongue-in-cheek). When you hear someone talk about a tool as being part of the "Hadoop ecosystem", they mean that it uses some Hadoop services as part of its service architecture. The last major thing to consider is your bootstrap script. The bootstrap script executes on every instance in the cluster immediately after machine initialization is complete, and itâ€™s how you configure the instances with any cluster-wide customizations you want to make yourself. Letâ€™s now look at a concrete example. This example AWS CLI script launches a transient spark cluster on three m5.xlarge instances on EMR that runs a pyspark_job.py and then auto-terminates: Thereâ€™s a few other things going on here worth understanding: That should be enough EMR to get started. ðŸ˜Š  The Dask API has a concept of a cluster â€” a backend powering Daskâ€™s distributed compute. Dask supports a few different types of clusters; the one we are interested in is the YarnCluster, which lets you set up Dask wherever Hadoop Yarn is up and running. You then connect a client to that cluster to expose an interface you can connect to. Hereâ€™s a minimal example showing this process in action: Notice that YarnCluster takes a variety of arguments configuring the cluster workers. Here we specify that we want each worker to get a single core and 3GB of memory, and that we want to scale this cluster up to 8 workers total. Dask will submit this application to Yarn, which takes care of scheduling these workers onto the machines. We have two m5.xlarge worker machines, each with four vCPU cores and 16 GB of RAM. This configuration will cause Yarn to schedule four workers per machine â€” this works out to one worker per core and 12 GB total per machine (this is close to optimal, as it leaves 4 GB of overhead per machine for the OS and other background processes). Dask will ultimately execute our jobs as Python code on the worker machines. How do we ensure that each worker machine has the exact same Python environment? Dask does something very clever here: it leverages the conda-pack or venv-pack tools to do it for you. These command-line tools allow you to pack your current conda or virtualenv environment into a portable tar.gz file. At client initialization time, Dask looks for this file (the default location is $HOME/environment.tar.gz), beams it to the worker nodes, and unpacks it in-situ.  With that background out of the way, we are ready to deploy a cluster. Before you begin, you will need: The Dask team maintains an example bootstrap script in the dask-yarn repo. To begin, download this script, modify it slightly to fix a known issue with the conda download URL inside, and upload the modified result to your S3 bucket: Next, we will need to create and configure an IAM role. The cluster will use this role for accessing the services it needs to run (primarily, EC2 start/stop and S3 read/write): The following incantation launches the cluster: Weâ€™ve already discussed EMR in some detail, so this should look familiar.Â :)  Before we start using the cluster, we first need to SSH into the master node to perform some further configuration. Once the cluster has finished bootstrapping, the EMR page in your web console will provide the public IP address of your master node. You can connect to the instance over that IP address by running a command like the following: Replacing /Users/me/Desktop/my-secret-key.pem with the path to the EC2 keypair secret you created earlier, and hadoop@ec2â€“18â€“223â€“211â€“150.us-east-2.compute.amazonaws.com with hadoop@YOUR_IP_ADDRESS.YOUR_REGION.compute.amazonaws.com. You will probably need to configure the security group associated with the instance to include your personal machineâ€™s IP address before this will work (if you donâ€™t, the connection attempt will just time out). See the corresponding page in the AWS User Guide for details (you can also try using this script). Successfully logging into the cluster prints a cute bit of ASCII art: Write an example client initialization script to disk and execute it: Assuming everything worked, youâ€™ve now successfully launched a Dask cluster on AWS EMR! However, we still canâ€™t interact with Dask outside of the EMR cluster. For that we will need to do one more thing: set up SSH port forwarding.  Dask has great support for integrations with the Swiss army knife of data science, Jupyter Notebooks. In this section we will see how we can use dask-labextension and SSH port forwarding to interact with and submit jobs to our cluster right from a Jupyter notebook on our local machine! When you launch, as part of the output you will see a logline that looks something like this: When you execute the cluster script, Dask launches two separate processes in the background: a scheduler that performs task management, and a dashboard connected to that scheduler offering a suite of visualizations for monitoring your cluster. Each of these processes listens on a different port on the cluster master node. However, these ports are local to the master node and are not exposed externally. To make Dask visible on our local machine, we need to use port forwarding. Port forwarding is an SSH feature that allows traffic from one port on a local machine to transparently (and securely â€” all traffic is encrypted) route to some other port on a remote machine. Hereâ€™s how: You should now be able to connect to your EMR Dask cluster from your local machine thusly: To verify that everything is working as expected, try printing the client object inside of a cell in a Jupyter Notebook â€” you should get a nicely formatted summary card that looks something like this: The dask-labextension provides a native Dask dashboard-like experience from inside of a Jupyter notebook. Once youâ€™ve installed and enabled the extension (see the repo README for instructions) you can paste the forwarded SSH address, localhost:8158, into the extension side panel to enable the connection to the cluster (see this SO question for details). To learn more about the features dask-labextension brings to the table, I highly recommend checking out the dask-labextension quickstart video. Hereâ€™s a screenshot from that video showing it in action: Youâ€™re now all set up and ready to get running with your new Dask cluster! ðŸŽŠ  To test out Dask performance, I downloaded a copy of NYC Open Dataâ€™s 2019 Yellow Taxi Trips dataset to my local disk. I switched the pickup and dropoff timestamps from string to datetime format, and partitioned the result into twelve parquet files, one per month: This is a good dataset to demonstrate the capacities of Dask because itâ€™s large, but tidy. Each row in the dataset is a single taxi cab trip, and thereâ€™s ~84 million such trips in the dataset. Unpacked into pandas memory, this dataset takes up ~16 GB of RAM total, which the 16 GB of RAM on my laptop can just barely handle (with the help of swap space).  This dataset is too big to work with on my local machine. Thatâ€™s where Dask comes to the rescue. I used AWS S3 Sync to upload the partitioned Parquet files to the cloud: The dask.dataframe.read_parquet method can then read partitioned Parquet files out of an S3 directory straight into Dask cluster memory. This is as simple as: A couple of important hidden "gotchas" to keep in mind here: It takes just about four seconds for Dask to value_counts all 84 million rows in this dataset. Other ETL operations in Dask will be slower or faster, depending on both the exact cluster configuration used and on the complexity of the (parallelized) algorithm. For more details, check out the DataFrame API or Best Practices pages in the Dask documentation for tips and tricks on performance.  In this tutorial, we configured and deployed a Dask cluster on Hadoop Yarn on AWS EMR, using it to perform some basic EDA on 84 million rows of data in just a handful of seconds. Distributed Dask clusters are one of the most popular and powerful tools for managing ETL jobs on large-scale datasets. Better yet, NVIDIAâ€™s RAPIDS initiative is bringing Dask to GPU clusters as well. This promises to push the envelope on whatâ€™s possible even further: the blog post "Dask and RAPIDS: The Next Big Thing for Machine Learning and Data Science at Capital One" demonstrates some really impressive speedups. We will discuss deploying a Dask cluster in a Spell workspace, and take a test drive through the RAPIDS cuml and cuml_dask GPU compute ecosystem in a future blog post. Stay tuned! Got questions or comments on this article? Contact the author at aleksey@spell.ml. Create an account in minutes or connect with our team to learn how Spell can accelerate your business. Privacy Policy | Terms of Service | Data Processing Addendum Â© Spell 2021. All Rights Reserved. 