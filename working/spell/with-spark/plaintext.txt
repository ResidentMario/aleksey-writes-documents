Many of the largest companies in the world use Apache Spark, a software system and a set of client libraries for executing large, highly parallelized data processing jobs against massive amounts of data, for their big data needs. Spark works by partitioning the data processing jobs across a large number of machines. In order to efficiently scale to queries touching petabytes of data or more, Spark queries (written in Scala, Python, Java, or SQL) are translated by a cluster master node into a query plan, which determines which machines contain data relevant to the query, what jobs those machines need to run, and how the outcomes of those jobs need to be collected and combined to ultimately generate output to the user. Most machine learning models are trained not directly on raw data streams but on highly processed, featurized datasets. A common workflow for managing machine learning model training on large-scale datasets is to build model training features in Spark, write the resultant datasets to object storage (e.g. Amazon S3), then train the models using this data in a Jupyter Notebook or a Python script. Here, I'll demonstrate this workflow in action on Spell. We will use data from the Liberty Mutual Property Inspection competition on Kaggle to build a simple baseline model in pure Spark -- using Databricks Notebooks and Spark ML -- then write that dataset to S3, mount the S3 bucket to a Jupyter notebook running on Spell, and build an improved model using xgboost on Spell. To follow along with this tutorial in code, check out companion GitHub repository here.  The core abstraction used throughout Spark is that of a DataFrame. The core idea of a table of rows and columns should be familiar to readers familiar with the pandas library in Python, but the API is totally different. The method for reading data into a DataFrame depends on how the data is managed within Spark, but in the simplest possible case - a CSV file uploaded to the cluster - the PySpark code required will look like this:  Running this code inside of Databricks pretty-prints a preview of the data to the notebook: 
      
     The Liberty Mutual challenge asked competitors to predict a Hazard value (a number ranging in value from 1 to ~40) based on a sequence of (anonymized) predictor variables. Because many of the columns are in a categorical format, with string values like "A", "B", and "N", this dataset is not immediately suitable for input to a model - we will need to transform these categorical variables into numeric ones first. To do this we will use Spark MLlib, a library of data preprocessing tools and machine learning models built right into Spark. The design and feature coverage of MLlib is extremely similar to that of the venerable scikit-learn Python library. The core difference is that because MLlib is built on top of Spark, MLlib is designed from the ground up to be extremely performant, making it a powerful tool for data preprocessing. Let's go ahead and use MLlib to prepare our dataset for learning: We will featurize this data in two steps. In the first, we use the StringIndexer transform to label encode the string categorical columns in the dataset into ordinal categorical columns, which our model knows how to use. Next, we use a VectorAssembler transform to "vacumn" the columns into a single column, indexedFeatures, containing a vector of values suitable for input to the model. We use a Pipeline to organize this process. Next we train a simple baseline model. And evaluate its performance, using root mean squared error as our metric. The data looks good, and the model is achieving a reasonable result! The natural next step towards improving this result is to switch to a more performant algorithm. Unfortunately, this is difficult to do using Spark and Databricks Notebooks alone.  The machine learning model implementations in MLlib are much less mature than those in the Python ecosystem. For example, the gradient boosted classifier in MLlib only supports binary classification (e.g. 0 or 1), e.g. it has no support for multiclass classification (e.g. {0, 1, 2, 3, ...}). Meanwhile, while it is possible to load libraries like scikit-learn or pytorch into Databricks and perform training from there, there are some practical limitations to developing directly on a Spark cluster: Databricks notebooks are a great tool for writing Spark, and Spark is a great tool for large-scale data processing, but these tools are not well-adapted to writing generic Python code.  Spell is a natural compliment to a Spark cluster and/or to Databricks Notebooks. The Spell platform provides an opinionated cloud-based Jupyter workspace environment designed to writing generic Python code, especially model training code, as easy as possible. Return to the Databricks notebook and write the featurized data we've constructed to an S3 bucket you have access to: Replace spell-share here with the name of a bucket you own. Now mount the bucket to your organization using the Spell CLI (note: for private buckets, this feature is only available on Spell for Teams): After going through this flow, the bucket will now be available as a path inside of the SpellFS filesystem: We can now create and launch a Jupyter workspace on Spell attached to our S3 bucket (and hence, to the dataset we just built in Spark) from either the Spell web console or the command line: This command creates a new Jupyter workspace named liberty-mutual-competition-workspace with XGBoost installed and the examples GitHub repo cloned. The spell-share bucket we wrote our Spark data is mounted into the workspace, and we are ready to get coding: 
      
     Let's try out a simple XGBoost model on Spell, using train-test split and early stopping to optimize our accuracy. If you are following along in code, this is the xspark/xgbost-model-on-spell.ipynb notebook in examples. After ~350 rounds of training, we achieve an RMSE of 3.73: This is a significant improvement on the performance of our baseline model (which had an RMSE of 3.91967). ✨  Now you know how Spark, Databricks, S3, and Spell can be used together to combine the powerful data scaling tool of Spark with the powerful development scaling tool of Jupyter notebooks. The pattern we've shown here works extremely well when the features generated using Spark are small to medium in size. However, if the features you build with Spark are themselves very large, they may be prohibitively expensive and awkward to work with through object storage. In a future blog post, we will explore using Python UDFs in Spark to score machine learning models trained on Spell on huge amounts of data, without the data ever having to leave Spark. Stay tuned! Create an account in minutes or connect with our team to learn how Spell can accelerate your business. Privacy Policy | Terms of Service | Data Processing Addendum © Spell 2021. All Rights Reserved. 