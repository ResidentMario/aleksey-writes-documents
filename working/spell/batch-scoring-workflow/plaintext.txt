One of the most common tasks in production ML is batch scoring. Batch scoring is the process of using a trained ML model to generate predictions on a concrete dataset. If youâ€™ve ever scored on a model on a test set to evaluate model performance or to save to disk for later use (perhaps to serve offline later), youâ€™ve performed batch scoring. Itâ€™s pretty straightforward â€” embed the scoring job directly into the model training job. However, mature pipelines on larger datasets almost inevitably manage batch scoring as a separate step in a pipeline. This is the principle of separation of concerns in action: it allows for the decoupled development of training and serving steps, eases job scaling, and makes the pipeline easier to debug if something goes wrong. In this article, we will learn how to implement a scikit-learn batch scoring pipeline using Spell workflows. Workflows are Spellâ€™s pipelines feature: they allow you to construct and execute a DAG of individual Spell runs. Throughout this article we will also discuss various tips and tricks for getting the most out of your batch scoring jobs. These tools and techniques are, taken together, the secret to running high-performance, scalable batch scoring jobs on Spell. Note that this article will assume familiarity with Spell runs.  For purposes of demonstration, we will use the wta-matches dataset from Kaggle. This dataset records matches played on the WTA womenâ€™s tennis tour in the years 2000 through 2016. Here is Kaggle's preview of this dataset:  A common technique when working with "big data" is training on just a subset of the data. This is because when the dataset is extremely large, itâ€™s rarely necessary to train the model on the entire thing. Usually a small (but representative) subset of the data can achieve the same performance in a fraction of the total training time. This relationship between converged model accuracy and the number of training set samples is called the learning curve of the model, and itâ€™s a function of the learning capacity of the model modulo the noisiness of the dataset. Deep learning models famously have extremely deep learning curves: you really canâ€™t feed them too much data. Meanwhile, classical machine learning techniques like regression and support vector machines, have very limited learning capacity, and will level out much sooner. To learn more about learning curves, check out my Kaggle notebook on the subject. In this article, we will demonstrate a pipeline using subset training on a sklearn LogisticRegression model using just 2015 match data. Another common big data technique is partitioning. Partitioning a dataset makes it easier to store and manage. The WTA matches dataset is an example of a dataset partitioned on year â€” each wta_matches_*.csv file corresponds with a single year of play on the tour. If you know that your dataset will often be grouped by a certain key, it makes sense to partition the dataset by that key, allowing a cluster computing framework (like Spark or Dask) to bucket data by that key "for free". E.g. if a user asks for every tennis match from 2000, the system can just read and return the contents of the wta_matches_2000.csv file. If we had partitioned the dataset using a different key, or not at all, the system would have to scan through the entire dataset to find and extract every single record from 2000. This obviously has significant performance implications for large-scale data queries. As a result, the manner in which you partition your datasets is a key component of your data warehousing strategy. wta-matches is not a "big data" dataset. However, it has already been partitioned for us, making it a convenient use case for our demo. One final thing to note is file format. In this demo we will be using CSV for both input and output. CSV is a simple, human-readable format, making it an appropriate choice for simple datasets like this one, but it doesnâ€™t have any compression (beyond in-browser gzip) or other performance-oriented features, so itâ€™s not a very fast file format. Almost all "big data" pipelines these days make heavy use of Parquet instead. Apache Parquet is a highly compressed columnar data storage format, part of the larger Hadoop ecosystem, that has better read/write performance (thanks to its columnar layout) and smaller on-disk size (thanks to its use of compression) than raw CSV files. To keep things simple, we will not be using Parquet here, but keep in mind that Parquet is the data format of choice at scale.  Spell uses S3 object storage as our storage layer. To prepare the data, we will download the dataset (using Kaggle's Python API client) and upload it to an S3 bucket we own: In the aws s3 sync command above, replace s3://spell-datasets-share with the name of a bucket you own. Next, mount the bucket into your Spell organization, if you haven't already: This will give your Spell cluster access to this bucket, allowing you to mount objects from this bucket into any of your Spell runs.  Here is the training script we will use: This training script loads wta_matches_2015.csv from the /mnt/wta-matches/ directory, trains a LogisticRegression model on it, and writes it to disk using joblib (if you're not familiar with joblib, it's the library sklearn uses for training parallelization and model I/O). Because weâ€™re training on the 2015 partition of the dataset only, this is an example of subset training in action. To execute this training on Spell, you would run: Next, we will write the scoring script. This script loads the filename dataset off of disk, runs a scoring job on it, and writes the result back to disk: Notice that this script is using Dask to do the heavy lifting. Dask is a cluster computing framework that lets you execute Python code on larger-than-memory dataset across one or many machines. Weâ€™re big fans of Dask at Spell. Whether working alone on CPU or paired with NVIDIAâ€™s RAPIDS on GPU, Dask makes it easy to scale ML jobs on huge workloads. In this script we initialize a single-node Dask cluster. Then, after loading the LogisticRegression model artifact into memory we wrap it using Dask's ParallelPostFit and score it on a dataset read into memory using dask.dataframe.read_csv (instead of using e.g. pandas.read_csv). This does two important things for us: To learn more about Dask check out some of our previous articles on the subject: "Getting started in the RAPIDS distributed ML ecosystem, part 1: ETL" and "Getting started with large-scale ETL jobs using Dask and AWS EMR". Hereâ€™s how we would run this code on Spell: This spell run command mounts the model saved to disk in our previous (training) run and 2015 partition of the dataset into the run, then executes our scoring job with that model on that data.  Now that we have working prototypes of our training and scoring runs, we are ready to combine the two into a pipeline using a Spell workflow! Spell workflows are defined using the Spell Python API and launched from the CLI using the spell workflow command. Here is our complete demo workflowâ€”weâ€™ll walk through how it works step-by-step: The first thing this script does is initialize our training run using the client.runs.new method, which is to the Python API equivalent to the spell run CLI command. We can't proceed to scoring until the training run has completed successfully. The next section of the script blocks until the training run completes, then checks the status of the run to ensure that it completed successfully: Assuming the run is successful, the workflow moves onto the next task: launching the scoring jobs! Notice that the code launches a new model scoring job for each partition in the dataset (e.g. for every year from 2000 through 2016 inclusive). At the end of the workflow, a combine run (code omitted; see here if youâ€™re curious) stitches the results of each individual scoring run back together again, and saves it to disk for long-term storage or upstream consumption. Astute readers will notice that weâ€™ve basically just implemented a MapReduce job of Spell runs. ðŸ™‚ This workflow demonstrates the power of using Spell for horizontal scaling: splitting the scoring job among many runs to speed up the scoring process. You can also just as easily use Spell to perform vertical scaling: improving the speed of scoring job by moving into one a more powerful instance. For example, in this run we scored our data using a baseline cpu instance. It would be trivially easy to do client.runs.new(machine_type="cpu-large",Â ...) or client.runs.new(machine_type="t4") instead, to move the job to a bigger CPU instance or a GPU instance instead, respectively. Which combination of horizontal and vertical scaling works best for you will depend on your exact use case. Spell is flexible enough to support either (or both).  With all of the code done, we can execute our workflow using the following spell workflow CLI command: You can monitor the progress of the workflow in the workflows page in the web console: And thatâ€™s it â€” thatâ€™s our complete scikit-learn batch processing workflow on Spell! Although this tutorial uses scikit-learn, the same basic principles discussed here apply regardless of framework. For example, Dask has a recipe for scaling batch scoring using PyTorch, and XGBoost has native support for training and scoring on Dask that works pretty much out-of-the-box. If you enjoyed this article, you may also like: Create an account in minutes or connect with our team to learn how Spell can accelerate your business. Privacy Policy | Terms of Service | Data Processing Addendum Â© Spell 2021. All Rights Reserved. 