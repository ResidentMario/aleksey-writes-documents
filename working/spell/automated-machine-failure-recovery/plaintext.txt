The core product at Spell is an opinionated platform for performing machine learning training jobs on cloud GPUs. This is made possible by abstracting over the big cloud providers, using a combination of EC2 on AWS and Compute Engine on GCP to check out the cloud GPUs that ultimately execute our usersâ€™ code. For the most part, itâ€™s a smooth and efficient process! However, there is one platform implementation detail that trips up our users more than any other: the mysterious, barely-documented, incredibly disruptive, REPAIRING machine state on GCP. Any GPU-based Google Compute Engine instance left running for over 6 hours is at risk of termination. The above realization was a surprising discovery we did not account for in our original implementation. Because of the poorly documented nature of this behavior, it took several months for our team to understand that this was a systematic limitation in how Compute Engine is implemented, rather than random noise or flakiness in our code. In this blog post we discuss what REPAIRING is, how it (seems to) work, its impact, and how we built an automated backup save feature into Spell to fix it.  In order to understand the GCP REPAIRING state, we need to establish some basic understanding of cloud compute. The two core abstractions central to all things software engineering â€” the building blocks from which all other systems are designed and implemented â€” are compute and storage. This maps to the Elastic Compute (EC2) and Elastic Block Storage (EBS) services on AWS, and to Google Compute Engine (GCE) on GCP. Users interact with these services by checking our virtual machines, or instances. An instance consists of three things: Once youâ€™ve put in the API request, assuming all goes well, you will have a virtual machine running on "the cloud" that you can SSH into and start using. Because Spell is fundamentally a service for executing arbitrarily complex and long-running user code, we donâ€™t rely on any higher-level services and instead perform all of our machine management ourselves. On the cloud provider side, this requires stepping machines through the instance lifecycle. Hereâ€™s the complete machine state diagram for GCP, take from the Instance Life Cycle page in their user documentation: A machine that has successfully booted up enters the RUNNING state. Once you decide that you no longer need the machine, you may stop it (in which case it can be relaunched later) or delete it (in which case it is permanently removed).  In addition to the two user-controlled shutdown APIs, stop and delete, it is also possible for the cloud provider to perform machine shutdown themselves, that is, without user input. On AWS this is known as instance retirement, and it will occur whenever AWS detects an irreparable hardware failure on the machine backing your instance. The instance it not actually retired right away: it is scheduled to be retired some number of minutes after the hardware failure is detected. This gives the end user time to save anything important on the disk before the instance is terminated by force, should they elect to do so. On GCP this is known as machine repairing. Like AWS, GCP states that it will transition the machine to the REPAIRING state whenever it detects irreparable hardware failures on the backing machine. The critical difference is that whilst instance retirement on AWS is extremely rare, machine repairing on GCP is extremely common! Whilst Amazon is happy to let you hold on to an instance for as long as you need it for, Google performs regular host maintenance on their machines and will not wait to do it. In our experience, any GCE instance left running for over 6 hours is at risk of forced termination. Itâ€™s impossible to know what the conditions for triggering machine repairing are. GCP does not provide much in the way of documentation on this subject: the relevant documentation page, GPUs on Compute Engine â€” Restrictions, merely states that "events typically occur once per month, but can occur more frequently when necessary". We can only report what we have observed: that GPU instances can enter the REPAIRING state and auto-terminate after anywhere from 6 to 72 hours in production. Many deep learning models train for 24 hours or longer. Launching a long-running machine learning job overnight and coming back the next day to find that it has been unexpectedly terminated is an extremely disappointing user experience â€” not to mention, potentially a very expensive one. Our AWS users donâ€™t have to worry about termination because AWS rarely terminates machines. On the other hand, almost all of our GCP-based customers have had their runs terminated prematurely at least once. Not great when youâ€™re still trying to convince folks of the reliability of GPUs on the cloud! ðŸ˜¬  To mitigate the adverse impact of unexpected machine termination on our users, here at Spell weâ€™ve incorporated an automated backup save feature into our product. Hereâ€™s how it works. Recall that compute instances on GCP are provisioned with a disk providing storage. When you initialize an instance, you are provided with the option to make the disk attached to that instance ephemeral, in which case the disk is deleted when the instance is deleted, or persistent, in which case you manage its life-cycle by hand. When a machine enters the REPAIRING state, any data still in memory is lost, but any persistent disks attached to the instance are detached, not destroyed. Its data is still accessible â€” provided you spin up a different instance to attach it to. When our machine orchestrator detects that an instance has been interrupted by the cloud provider, it spins up a new "backup save instance" and attaches the persistent disk from the interrupted instance to the new instance. The backup save instance takes the files on the disk and uploads them to our object storage bucket (we use object storage as the backend to our user-facing filesystem). It then signals to our API that it has finished its work, marks the persistent disk for deletion, and terminates. Before implementing backup saves, we used ephemeral disks and had the instance write any output files to object storage as the last step of its execution. We still do this for runs that do not get interrupted (and for runs that get interrupted by the user, not by the cloud provider), because this is much simpler and faster than spinning up a brand new instance, but it means any files generated by runs that got interrupted by the cloud provider were lost forever: Now, when the user goes to see the results of their machine learning training job, instead of staring at a blank page they will get a record of all of the files that the run saved to disk prior to termination: Needless to say, not losing all of your files is a huge improvement in our user experience! ðŸŽ‰  Of course even with backup saves configured GCP still doesnâ€™t "just work". It is still up to the user to make their long-running model training jobs reentrant. To explain what I mean, here is an example training loop for a typical PyTorch model, which should be familiar to the average data scientist: This is an example of a training loop thatâ€™s been updated to have reentrancy. In this example, instead of starting training from the zeroth epoch, we check a runtime variable (passed from above by the user) to see if there is a previous checkpoint file we should restart from instead: Reentrancy is a really nice property, a complement to existing best practices around intermediate model checkpointing. In most cases, it is not at all a big lift to achieve. Once youâ€™ve made your script reentrant, you can use the automated backup save feature to recover from unexpected termination very easily. Suppose youâ€™ve initialized a Spell run with a script featuring the model training loop we just defined: In the event of a failure, to restart model training without having to start from scratch you would run something like the following: For more details and a complete code sample be sure to check out the companion Jupyter notebook. Automated backup saves have ameliorated our run interruption issues and addressed one of the most common sources of user pain with our product. It is worth noting that we currently only see this behavior in our GPU instances. Google has built a feature called live migrations into Container Engine that allows users on CPU-based instances to quietly pause and restart their current execution on a new instance without having to explicitly save and reload application state. We are hopeful that in the future, Google will extend this feature to GPU instances as well. This will allow us to provide a GCP-based model training experience without the additional user complication thatâ€™s currently required. Create an account in minutes or connect with our team to learn how Spell can accelerate your business. Privacy Policy | Terms of Service | Data Processing Addendum Â© Spell 2021. All Rights Reserved. 