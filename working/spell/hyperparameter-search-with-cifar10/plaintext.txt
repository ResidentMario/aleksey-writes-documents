One of the most common operations in machine learning is hyperparameter search. Hyperparameter search is the process of finding the magic model tuning values — the hyperparameters —which enable the highest possible model performance. In this article I will showcase the core concepts and three most common techniques for performing hyperparameter search, and provide some tips for speeding up the search process that I have found helpful in my own projects. I will do so using the cifar10_cnn Keras demo model and the spell hyper Spell CLI command, but note that the same core ideas apply when using a different implementation or framework. You can follow along in code by checking out the Jupyter notebook in the examples repo.  Passing data into a machine learning model and getting a prediction back out ultimately boils down to multiplying, adding, and/or subtracting a bunch of numbers. Some of these values are from your input data and some are parameters learned during model training. The rest are hyperparameters — "magic numbers" you have to set yourself before training starts. Deep learning is filled with such magic numbers. Before you can start training a deep learning you have to first make a bunch of decisions like: Having good hunches (fed by knowledge of the network architecture and experience working with similar data) regarding which hyperparameter values are likely to work is a big part of the "art" of machine learning. However, given a large enough number of possible architecture choices, not even an experienced machine learning engineer can find the "right" combination of values right away. This is where hyperparameter search comes in. Hyperparameter search is the process of having a computer look through as many possible combinations of hyperparameters as feasible, trying to find the precise combination of values which maximizes model performance. The remainder of this article will walk through the three major forms of hyperparameter search strategies (grid search, random search, and Bayesian search) by using the spell hyper command to tune a version of the cifar10_cnn Keras demo model. This is a simple CNN model that learns to differentiate input images into ten different classes. The complete model definition: The example uses 64 for the size of 3rd and 4th convolutional layers, with a kernel of (3, 3), 512 nodes for the size of the dense layer and, a final drop out of 0.5. This model has the following hyperparameters: conv2_filter, conv2_kernel, dropout_1, dropout_2, dropout_3, and dense_layer. We’ve wrapped this model in a training script using the argparse module from the Python standard library, which passes these values as command line arguments. You can check out the training script for yourself on GitHub.  The simplest form of hyperparameter search is grid search. Grid search works by sampling points in the hyperparameter search space (the space of all possible combinations of values) evenly. To visualize grid search (in two dimensions), imagine an (x, y) coordinate plane. Each point in the plane is a combination of two possible hyperparameter values, and would result in a differently performing trained model. To perform a grid search, partition the space into evenly spaced points and train the model once at each point: In this example models 3 and 7 would perform the best and models 1 and 9 the worst. The good: simplicity! Grid search is a simple search strategy which is easy to understand. Here’s an example hyperparameter grid search launched from the CLI using spell hyper grid: This command checks out the examples GitHub repo and runs one copy of the python cifar10_cnn.py command per hyperparameter combination. Notice the use of the special :HYPERPARAMETER_NAME: syntax. This is what Spell uses to connect the list of values you provide (via the param flags) to the input you provide to the python command you are running (via the conv2_filter, conv2_kernel, dense_layer, and dropout_3 flags). The bad: cost! This code launches 18 different model training runs, one for each possible combination of values: 16,2,32,0.2, 16,32,0.5, 16,64,0.2, and so on. This is the biggest limitations of grid search — every additional parameter value doubles the number of search points. The larger the model, the more expensive the search, and the less useful grid search is for performing it.  If you’re looking to upgrade your hyperparameter search process, the next step up to consider is random search. In random search we pick points in the search space to test out completely at random: An example random hyperparameter search on Spell: Note that in this example we’ve switched from using a comma-separated list of key values, e.g. 16, 72, 128, to picking points out of a range, e.g. 16:128. I’ve also specified that the numbers chosen for conv2_filter and dense_layer must be whole numbers by specifying they be int type. The good: cost! As a rule of thumb, hyperparameter searches in more than three dimensions should be performed using random search, not grid search.  The reason why is the subject of a 2012 paper co-authored by arguably the most well-known machine learning researcher in the world, Yoshua Bengio, titled "Random Search for Hyper-parameter Optimization" The problem is that given sufficiently many parameter dimensions, selecting arbitrary hyperparameter values to test with has an extremely high chance to miss points of optimality. This problem is succinctly summarized in the following visualization: On the left is a grid search that misses the point of optimality completely: every point in the grid is "off-peak" with respect to important parameter x. On the right, the same search is repeated using random search. In this case the randomness of the points chosen ensures that at least some of the points end up on the peak, and in this case, two of them do. The paper’s authors discovered that the more dimensions you search on, the more this effect dominates, and the more likely random search is to outperform grid search, e.g. find the optimal point in less time. The bad: randomness! An appealing quality of grid search is that because you choose the points to check you can index the search values to well-known rules of thumb. For example, if you know that 0.2 and 0.5 are two very common values for a particular form of dropout, you might have the model optimizer choose from between the two. When performing random search the numbers are more, well, random: 0.27 or 0.43, for example.  The final, most powerful strategy for choosing model hyperparameters (besides graduate student descent) is Bayesian search. Bayesian statistics is the branch of statistics that deals with predicting the probabilities of things given limited existing observations. Bayesian search strategies choose points somewhat randomly, just like random search does, but it takes things one step further still by using the performance achieved using prior values to seed the choices for the next ones. Bayesian search, properly applied, produces results that look something like this: Notice how we started off exploring low-performing points 1, 2, 3, 4. After doing this for a while we discovered a couple of clusters of high-performing points. We then biased the rest of the search towards these points (5, 6, 7, 8, 9) instead. The catch is in "properly applied". Bayesian hyperparameter search is ironically highly sensitive towards its own hyperparameters. In particular, in cases in which there is a large broad area of moderately-valued points, and a different area of highly-valued points, Bayesian hyperparameter search is likely to get "sucked into" the moderately-valued space, never breaking out of it sufficiently long enough to explore the highly-valued space at all. As a result, a large number of Bayesian hyperparameter search algorithms exist, and there’s no "one algorithm" which is definitively the best for your dataset. Some libraries eschew Bayesian search completely: scikit-learn for example, which readily supports grid search and random search, has no Bayesian search facility. However, Bayesian hyperparameter search in deep learning is extremely attractive. Because deep learning models are very expensive any performance gains to be had with smarter point selection are likelier to be "worth it". At Spell we provide a form of Bayesian hyperparameter search from the CLI: This command is quite a bit more complicated than the other ones because Bayesian search is quite a bit more complicated. The new parameters are: We will explore how we implemented this feature and how it works internally in a future article.  Before concluding I think it’s also worth briefly highlighting three useful techniques for speeding up your hyperparameter search process:  That concludes our introduction to hyperparameter search! We’ve learned about grid search, seen random search in action, and tried out Bayesian search; and seen how they all work through the lens of the Spell CLI. The same general principles will apply to any other package or implementation. For more API specifics, be sure to also check out the companion Jupyter notebook if you haven't done so already. Hopefully you are now better prepared to use these techniques in your own machine learning experiments! Create an account in minutes or connect with our team to learn how Spell can accelerate your business. Privacy Policy | Terms of Service | Data Processing Addendum © Spell 2021. All Rights Reserved. 