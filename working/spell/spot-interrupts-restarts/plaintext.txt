The secret to cost-effective machine learning model training on the cloud is spot instances. Spot instances are an alternative to the standard on-demand instances on services like AWS EC2 and GCP GCE that come at a steep discount. The catch is that spot instances may be interrupted at any time: taken away from you and given instead to a customer willing to pay a premium (non-interruptible) for an on-demand instance. In exchange for this added risk, cloud providers provide steep discounts on instance costs â€” typically around 66% or so. This makes spot instances ideal for expensive compute workloads that are robust to failure. Machine learning training jobs (especially deep learning ones) definitely fit the bill when it comes to "expensive". And by incorporating model checkpointing into your training scripts, they can easily be made failure-tolerant as well: just restart your training run from a saved checkpoint file whenever an interrupt occurs. At Spell, we recommend all our customers use spot instances and reentrant training scripts whenever possible. In a previous blog post, "Reduce cloud GPU model training costs by 66% using spot instances", I go over the economic argument for and additional considerations around using spot instances for model training, including some benchmarks showing savings of up to around $200 on large training jobs on the cloud (24 hours on a V100x4). For anyone unfamiliar with spot instances, I recommend giving that article a read! In this article, I will introduce a new feature of the Spell platform that lets you go even further with spot instances on Spell: auto-resumption. To follow along with an interactive tutorial in code, check out the reentrancy demo in our spellml/examples repository.  The first step to using spot instances for model training is adding what we call reentrancy to your training scripts. A reentrant training script is one which saves model checkpoints as it goes along, and can be parameterized to restart training from an existing checkpoint if needed. This is already considered a best practice in the machine learning community, and making your training scripts reentrant is super-easy. Hereâ€™s a copy of the relevant code lines from our reentrancy demo: This training script saves a checkpoint file to disk every five epochs. Spell automatically backs these files up to our virtual filesystem, SpellFS, even if the run is interrupted, allowing you to easily recover your training progress at a later time: The spot instances demo Jupyter notebook in our examples repository has some visual examples showing this in action.  We implemented this feature in April and itâ€™s already proven to be quite popular with our users. In August we added a new feature that takes this one step further: the --auto-resume flag to spell run. Quoting from our docs: If your your script is written such that it idempotently resumes wherever it left of given a prior runâ€™s disk state, then you can go one step further using Spellâ€™s "Auto Resume" feature. [â€¦] When a run is interrupted by the respective cloud service and auto resume is enabled, Spell will create a new run with identical parameters, restore the interrupted runâ€™s saved disk state, and queue it up. When a machine becomes available again (usually when the demand lowers), the resumed run will execute, continuing the computation of the interrupted run. This is a super powerful feature because, subject to a little work on your part, it guarantees your runs on spot instances actually finish training without requiring any further manual labor on your part. This means that you can use spot instances everywhereâ€Šâ€”â€Ševen for infrastructure-critical runsâ€Šâ€”â€Špotentially making your model training jobs substantially cheaper across the board. We call training scripts that support this feature resumable. Hereâ€™s a PyTorch code snippet from our resumability demo script showing how you might implement this: This training script writes model checkpoint files to the /spell/checkpoints/ path incrementally. If the --resume flag is specified, and a /spell/checkpoints/ directory already exists, it uses a regex to find the checkpoint file with the largest epoch number and initializes the model from that one. Spell guarantees that this script will complete, even if run on a spot instance. Hereâ€™s how it works. First, we submit this run to Spell with the --auto-resume flag set: After letting the job run for a few minutes I manually terminated the backing EC2 instance in the AWS web console. Spell detects that the machine went down and kicks into backup save mode: spinning up a new CPU instance with the terminated machineâ€™s disk image attached that backs these files up to our virtual filesystem, SpellFS. The run logs show this process in action: Backup saving occurs for all runs on spot instances, not just ones with auto-resumption enabled. However, the next step is new! Immediately after backup saving is completed, Spell checks for the presence of the --auto-resume flag, and, if it finds one, automatically schedules a new run with the same command-line arguments: This new run receives a copy of its predecessorâ€™s disk state as input. Our resumable training script checks the /spell/checkpoints/ directory, finds some checkpoints, boots up the highest-numbered one, and resumes training from there: Allowing us to complete our training process with no further effort on our end. ðŸ”¥ To try this code out yourself, check out the spot instance resources in our example repo on GitHub. Create an account in minutes or connect with our team to learn how Spell can accelerate your business. Privacy Policy | Terms of Service | Data Processing Addendum Â© Spell 2021. All Rights Reserved. 