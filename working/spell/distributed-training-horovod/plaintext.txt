Distributed training is a set of techniques for using many GPUs located on many different machines for training your machine learning models. Distributed training is an increasingly common and important deep learning technique, as it enables the training of models too big or too cumbersome to manage on a single machine. In a previous article, "Distributed model training in PyTorch using DistributedDataParallel", I covered distributed training in PyTorch using the native DistributedDataParallel API (if you are unfamiliar with distributed training in general, start there!). This follow-up post discusses distributed training using Uberâ€™s Horovod library. Horovod is a cross-platform distributed training API (supports PyTorch, TensorFlow, Keras, and MXNet) designed for both ease-of-use and performance. Horovod has some really impressive integrations: for example, you can run it within Spark. And it boasts some pretty impressive results: In this post, I will demonstrate distributed model training using Horovod. We will cover the basics of distributed training, then benchmark a real training script to see the library in action. Click here to go to our examples GitHub repo so that you can follow along in code. Note that this blog post assumes some familiarity with distributed training. While I provide a quick overview in the next section, if you are unfamiliar with the idea, I recommend first skimming the more detailed overview in my previous post, "Scaling model training in PyTorch using distributed data parallel".  Horovod distributes training across GPUs using the data parallelization strategy. In data parallelization, each GPU in the job receives its own independent slice of the data batch, e.g. its own "batch slice". Each GPU uses this data to independently calculate a gradient update. For example, if you were to use two GPUs and a batch size of 32, one GPU would handle forward and back propagation on the first 16 records, and the second, the last 16. These gradient updates are then synchronized among the GPUs, averaged together, and finally applied to the model. Hereâ€™s how it works, step-by-step: The tricky part is maintaining consistency between the different copies of the model across the different GPUs. If the different model copies somehow end up with different weights, weight updates will become inconsistent and model training will diverge. This is achieved using a technique borrowed from the high-performance computing world: the ring all-reduce algorithm. This diagram, taken from Horovodâ€™s launch post, demonstrates how it works:  The ring all-reduce algorithm synchronizes state (in this case tensors) among a set of processes using a well-defined sequence of pairwise message-passing steps. The best part is that this is a well-understood algorithm thatâ€™s been in use in the HPC world for a long time â€” Horovod relies on Open MPI on CPU and NVIDIA NCCL on GPU to do the work under the hood. Facebookâ€™s Gloo is also supported.  The Horovod API is pretty easy-to-use. To better understand how it works, letâ€™s step through the PyTorch demo script included in the horovod GitHub repository (changes with the script are prefaced with a # Horovod comment): While the core of this training script is the same as it would be if you were to run it on a single GPU, adapting it to the multi-GPU use case requires a number of adjustments. Letâ€™s look at whatâ€™s changed, starting with the following new initialization code: After importing the Horovod PyTorch binding using import horovod.torch as hvd we need to call hvd.init() to initialize it. All of the state that horovod manages will be passed into this script inside of this hvd object. In this first bit of initialization we see the first of these local variables: hvd.local_rank(). The local rank is an ID number assigned to each GPU device on a machine, and it ranges from 0 to n - 1, where n is the number of GPUs devices on the machine. Horovod launches one copy of this training script for each GPU on the device, so we use torch.cuda.set_device to instruct PyTorch to run this code on the specific GPU Horovod has assigned this script to. The sampler component in DataLoader returns an iterable of indices from the dataset to be drawn. The default sampler in PyTorch is sequential, returning the sequence 0, 1, 2, ..., n. Horovod overrides this behavior with its DistributedSampler, which handles partitioning the dataset across machines. DistributedSampler itself takes two parameters as input: hvd.size() (the total number of GPUs, e.g. 16) and hvd.rank() (the ID assigned to this device from the overall list, e.g. 0...15). Note that the sampler also needs to know the current epoch. train calls train_sampler.set_epoch(epoch) on every training loop to achieve this. Horovod simultaneously trains as many batches as you have GPUs, and the gradient update that is made gets applied to the average of all of these different batch gradients. This means that we can speed up training by multiplying our base learning rate by the number of devices, hvd.size(). If you are using the Adasum learning rate scheduler, an advanced scheduler specifically designed for distributed training, there are some special rules to follow. See here in the Horovod docs for details. This training script uses default random initialization for the model weights. Each GPU initializes these random weights separately, so unless we synchronize the initialized weights between machines the training will diverge. The device with the rank of 0 typically has special significance in Horovod: it is the device responsible for this synchronization. These two API calls broadcast the model state from this "root" machine to the other machines in the list, ensuring that they are in sync. Non-root device training scripts will block on this operation until Horovod has performed the sync. This is where the magic happens. âœ¨ The Horovod DistributedOptimizer wrapper takes the optimizer (SGD in this case) as input, delegates gradient computation to it, averages gradients using all-reduce or all-gather, then applies those averaged gradients across all devices. In the single-machine case, to log the value of a metric we would simply ask for a vector, perform some computation on it, and print it. In the multi-machine case things are more complicated. We need each script's local copy of the value of the metric, and then we would need to average these values to get its cluster mean. hvd.allreduce returns the average of the local copies of a named vector. If an average is not appropriate, you can use the similar hvd.allgather method to collect the vectors into a local list instead, so that you can reduce the values needed. For clarity in the logs, we log the results in test() on the root machine (hvd.rank() == 0) only.  With our "horovod-ified" training script in tow, we are ready to launch our distributed training job. Horovod training scripts are not launched as Python scripts. E.g. you cannot use python train.py to run this training script. Instead, Horovod launches using a special CLI command, horovodrun: This example command launches four separate processes running this training script (-np 4). Horovod handles parameterizing all of the variables that need to be aware of the current processâ€™s position in the Horovod cluster, like hvd.local_rank() and hvd.size(), for you. Without this bit of indirection, you would have to write this process configuration code directly into the training script. For example, parallelizing a model training job using the DistributedDataParallel strategy built into PyTorch requires including something like this in the training script: This is really nice because it means you donâ€™t have to deal with initialization code like this yourself â€” Horovod uses the arguments you pass to horovodrun to figure out how to do it for you, savingÂ  you tons of time and energy otherwise spent writing process management boilerplate code. Note that this training script launches four processes, all running on the local machine (localhost:4). This is an appropriate setup for a machine with four GPUs attached, such as a p3.8xlarge or a g4dn.12xlarge (four V100s or four T4s) on AWS. To set up a multi-machine distributed training run, you will need to do a little bit more work. The host where horovodrun is executed must be able to SSH to all other hosts without any prompts. Assuming this is the case, you will then be able to execute your training job like so: To benchmark distributed model training performance I trained a DeepLabV3-ResNet 101 model (via Torch Hub) on the PASCAL VOC 2012 dataset (from torchvision datasets) for 20 epochs. I did so using the Spell API. (One of ðŸ˜‰) the great things about the Spell API is that we have built-in support for multi-machine training using Horovod. We handle the network discovery for you: assuming your script is properly configured, you can easily run a training job on as many machines as youâ€™d like with a spell run CLI command looking something like this: I launched five different training runs total: The results are not definitive by any means, but should nevertheless give you a sense of the time savings distributed training nets you. First, here are the times for the single-machine runs:  In this table, Vanilla is a single-GPU training job, DistributedDataParallel is the PyTorch-native data parallel API (to learn more, see "Distributed model training in PyTorch using DistributedDataParallel"), and Horovod is Horovodâ€™s PyTorch binding. You can clearly see the diminishing returns on multi-GPU training. Increasing compute power by 4x resulted in a ~3x improvement in training time, but doubling that compute further to 8x is only about a 33 percent improvement. Horovod is notably faster than native PyTorch data parallel, saving about a minute in training time on both machines. Nice! Next, letâ€™s look at multi-machine training times. This introduces network round trips between machines, which slows things down:  In this article we discussed distributed training and data parallelization, learned about the Horovod API, and applied it to a real model to get some time saves. To learn more about the distributed training API I recommend browsing the Horovod docs. If you found this post informative, you may enjoy some of our other posts on the Spell blog: Got questions or comments on this article? Contact the author at aleksey@spell.ml. Create an account in minutes or connect with our team to learn how Spell can accelerate your business. Privacy Policy | Terms of Service | Data Processing Addendum Â© Spell 2021. All Rights Reserved. 