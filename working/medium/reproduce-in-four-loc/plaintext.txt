Breaking changes (June 2020) (Now back to our regularly scheduled program.) Neural networks are, on the face of it, relatively easy to distribute. All of the major neural network libraries provide a way to save a weight matrix and/or architecture to a file, so to share that model with coworkers you can just share that file. But a matrix of weights is a black box! Other users can run your model and see the results it generates but they have no way to know how you got there. A lot of the time you want to share a retrainable CNN — something that others can train and modify and re-train themselves. How do you do that? In this article I will show you. We will build a CNN for image classification using the Fashion MNIST dataset, and see how easy it is to distribute a retrainable machine learning model when you think about your model in three parts: These three things are collectively the {code, environment, data} dependencies for your model. You can also follow along in the complimentary Jupyter notebook. Most machine learning projects start off with a hypothesis, a dataset (or datasets), and a breakout session in Jupyter. We begin by exploring the characteristics of the data salient to the problem at hand. In our example case this may mean plotting examples from the dataset, one class per row, to sanity check the visual distinguishability of the data: Each row in this plot contains ten random images from one class in the dataset. Looking at this plot, can you imagine which features of the images a classifier could use to figure out an item’s class. Here are some that I see: bags tend to be square, shoes have soles that run the full length of the shoe, and pants have two legs. All of these distinctions are features that our classifier could use to understand our image classes! The Fashion MNIST dataset is happily very well-formatted and requires very little cleaning. Once we are happy that we understand the challenge at hand, we can move on to iteratively designing, training, and testing our model almost immediately. After trying out a few different CNN architectures, here’s a model that performs well: We reach a happy state when we plot a training history like this one, demonstrative of a well-behaved, well-performing model: Once you have a model that is working well in your local development environment the next step is to share that work with others. This requires bundling and distributing your work in a way that works on everyone else’s machines. So let’s try instead breaking the model up into {code, environment, data} components and distributing those, so that you (or anyone else!) can try your hand at training the model yourself. git and GitHub is the ubiquitous choice for code. So to distribute our code: You’ve probably done this before. :) The resulting code repository includes the train.ipynb notebook we built the classifier in, a requirements.txt file defining our Python libraries, and a Dockerfile defining our runtime (more on this later). Notice the two things that we exclude from our code repository (by adding them to the .gitignore). These are our data dependencies. The first data dependency, fashion-mnist_train.csv, is the data that the model actually trains and tests on. Unless your data is extremely small and unlikely to change, checking this data directly into git wouldn’t work very well. Instead, you should save it to a persistent object store better suited for large files. The second data dependency is the model definition, clf.h5 (which I generated in the build notebook using the model.save function in keras). It is often tempting to check this file into a code repository directly, but for larger models this is an anti-pattern. Production-grade machine learning models contain millions of weights, which will quickly glut up any git endpoint you push it to. For non-trivial modeling projects its best practice to distribute your model definition the same way you distribute your data: by saving it to object storage. To distribute our data dependencies I will use Quilt T4. T4 is an API on top of Amazon S3 designed for bundling and distributing data project dependencies: In this code sample we collect our data dependencies in a data package. We then push that package under the name aleksey/fashion_mnist, which sends all of the files in the package up to S3. Each push generates a tophash, a SHA-256 content (and metadata) hash that permanently identifies the version of the package just pushed. Now the data lives on s3://quilt-example, and is available for download to anyone on the team with access to that bucket. Now that we’ve tidily put away our code dependencies and our data dependencies, the last step in {code, environment, data} is saving our environment. The most complete tool for versioning environments is Docker (though depending on your durability needs and complexity, you may get away with using conda or even pip alone). Here’s a simple Dockerfile defining our image: This Dockerfile: To make this image publicly available we will upload this image to DockerHub. Many readers will be familiar with how that works: That was a lot of work, but it was for a good cause, because now anyone in the world can re-train our model from scratch in just four lines of code: Then navigate to http://0.0.0.0:8080/ in your web browser and you are right back where I left off. (you can even optionally leave out git clone, if you don’t want to browse the code repository locally) With the data dependencies on T4, the code dependencies on GitHub, and the environment image stashed in DockerHub, we’ve achieved fully reproducible data science — our model runs and outputs are now reproducible by anyone. In fact we’ve gone even further than just making our model build reproducible. We’ve made it pipeline-ready. Like Legos in a brick wall, our model dependencies plug smoothly into any process we want to define on top of our model. We can hook into S3 to create a data quality monitoring service, or hook into Docker to build-test our environments, or hook onto git to run CI tests on our code. And we can use data system design patterns like continuous delivery and data quality circuit breakers to further improve the quality and durability of our pipeline. Versioning {code, environment, data} is a flexible, minimally intensive foundation for whatever service or process on top of these components is needed most. In future articles I will explore even more advanced things you do with these fundamentals! Written by Written by 