If youâ€™re building an image classifier these days, youâ€™re probably using a convolutional neural network to do it. CNNs are a type of neural network which build progressively higher level features out of groups of pixels commonly found in the images. How an image scores on these features is then weighted to generate a final classification result. CNNs are the best image classifier algorithm we know of, and they work particularly well when given lots and lots of data to work with. Progressive resizing is a technique for building CNNs that can be very helpful during the training and optimization phases of a machine learning project. The technique appears most prominently in Jeremy Howardâ€™s work, and he uses it to good effect throughout his terrific fast.ai course, "Deep Learning for Coders". In this article Iâ€™ll demonstrate how you can use progressive resizing to build an image classifier using Keras. Specifically, weâ€™ll using progressive resizing to build a CNN that learns to distinguish between 12 different kinds of fruits in what I call the Open Fruits dataset â€” an image corpus I built based on the Google Open Images dataset (to learn more about Google Open Images, read "How to classify photos in 600 classes using nine million open images"). You can follow along with the code and learn how to download the data on GitHub. This post assumes familiarity with CNNs. If youâ€™re unfamiliar, Brandon Rohrerâ€™s "How convolutional neural networks work" is excellent background reading. Recall that our dataset consists of images of 12 different kinds of fruits taken from Google Open Images, which is in turn based on permissively-licensed images from Flickr. To get a taste, hereâ€™s 25 random images from the dataset: Right away we see that this dataset is very problematic. It includes tiny images; occluded images that only depict parts of the sample; samples depicting groups of objects instead of individual ones; and bounding boxes that are just plain noisy and may not even be constrained to a single type of fruit. In a few of the cases itâ€™s difficult even for a human to distinguish what the target class is. To add to the difficulty, the dataset is highly imbalanced, with some image classes appearing far more often than others: Between the low image and label quality and the class sparsity this classification problem is a very, very difficult one. Now that we understand the contents of our dataset, we need to make choices about the network we will train. One trouble is that a single neural network can only work with standardly-sized images; too-small images must be scaled up and too-large images must be scaled down. But what image size should be pick? If your goal is model accuracy, larger is obviously better. But there is a lot of advantage to starting small. To understand why, we must first understand that the most important features of an image classification problem are "large". Properly tuned gradient descent naturally favors robust, well-supported features in its decision-making. In the image classification case this translates into features occupying as many pixels in as many of the sample images as possible. For example, suppose we teach a neural network to distinguish between oranges and apples. Suppose that one model classifies by distinguishing between "orange" and "red", and another classifies by distinguishing between "stem shaped like an orange stem" and "stem shaped like an apple stem". The first model is robust: any image we score, no matter how small or misshaped, will have orange pixels and red pixels usable by the model. The second model is not: we can image images so small that the stems are not easily distinguishable, or images with the stem cropped out, or images where the stems have been removed outright. The practical result is that while a model trained on very small images will learn fewer features than one trained on very large images, the ones that it does learn will be the most important ones. Thus a model architecture that works on small images will generalize to larger ones. Meanwhile, small-image models are much faster to train. After all an image input size twice as large has four times as many pixels to learn on! Since small-image models generalize well to larger input sizes, and since they take less time to train, and since the first batch of models we build are going to be highly experimental anyway, why donâ€™t we save time and just train our first few models on small data, and worry about scaling up the images and the models later? In fact, that is exactly what progressive resizing is! We now understand the main idea behind progressive resizing. Letâ€™s see how it works in practice. We start out by building our first small-scale model. Here is a smoothed kernel-density plot of image sizes in our "Open Fruits" dataset: We see here that the images peak at around 128x128 in size. So for our initial input size we will choose 1/3 of that: 48x48. Now itâ€™s time to experiment! What kind of model you end up building in this phase of the project is entirely up to you. Hereâ€™s what I did (with comments in the code): This model achieved ~53% validation accuracy: Not stellar, but remember, this is a very simple model trained on very small very noisy images, with a lot of classes to choose from. We could do better by working on this model further, but I only had so much time to iterate on this model. ðŸ˜… We now apply progressive resizing to the problem. We started by building a classifier that performs well on tiny n x n (48 x 48) images. The next step is scaling our model up to 2n x 2n (96 x 96) images. We do this using transfer learning. Transfer learning is the technique of re-using layers and weights from previous models when building new ones ("Deep Learning For Beginners Using Transfer Learning In Keras" provides a good overview). In our case, this will mean taking the model we just built, freezing it (so that further training wonâ€™t make any changes to our existing weights), and injecting its layers into a new model (one which takes upscaled 96 x 96 images as input). The work we do at this stage is limited to finding a configuration of good "feeder layers" we can prefix our old model with. These new layers can focus on finding the additional features findable in 96 x 96 pixels that werenâ€™t in 48 x 48 pixels. First I saved the 48 x 48 model to disk as model-48.h5. Then I created the following new model: This script: Basically, weâ€™ve created a new model that trains on 96x96 pixel images which reuses our old 48 x 48 classifier internally! This new model improves performance from ~53% to ~59%: We can apply progressive resizing one more time, this time moving from 96x96 to 192x192. The procedure required is the same, differing only in scale: This new model sees data that is fully sixteen times as large as our original tiny 48x48 model. This translates into further model improvement, but this time that benefit is more modest â€” up from 59% to 61%: To summarize what weâ€™ve done: Note that, in the interest of time, I did not spend much time experimenting with the new layers I added in steps 2 and 3. To improve performance further, here are some other things you could try: We started this article off by discussing the fact that convolutional neural networks trained on small images are both fast and easy to train and readily generalizable to larger image inputs. This makes them good models to build during the early experimental phase of a project, when youâ€™re still just trying to get to grips with a basic network architecture that works well. We can concentrate on dashing off quick one-off models now, and on scaling them up and fine-tuning performance later. Finally, we heard that models trained this way can often achieve equal or better performance than models trained from scratch. Consider the alternative: building and tuning a full-sized 196 x 196 model from the start. This model would have an unrestricted "model finding space": it could theoretically converge on any combination of layer weights that works best for the given problem. Progressive resizing is much more restrictive: it require that a model that works well on 2n x 2n images must subsume a model that works well on n x n. This could in theory mean that we "miss" an even better architecture that a model built from scratch could converge on. But in practice, models built using progressive resizing principles often actually do better than models built from scratch. The theoretical reason why is a mystery. One compelling theory, courtesy of Miguel Perez Michaus, is that it improves the ability of the model to learn "scale-dependent" patterns. He writes about this in the excellent blog post "Yes, Convolutional Neural Nets do care about Scale"; I recommend reading it if you want to learn more. Nevertheless, progressive resizing is an interesting technique and a useful approach to image modeling problems And hopefully now that you have read this article, another useful tool in your deep learning toolbox. ðŸ˜Ž Written by Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual.Â Take a look Emails will be sent to aleksey.bilogur@gmail.com.  