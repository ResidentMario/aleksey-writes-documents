It’s rare that raw data is a natural fit for data science. As a result, best practices tell us that raw data should be treated as read-only and archived, that analysis should be done on some form of processed data, and that said data cleaning is extremely tedious and time-consuming. Since data cleaning requires substantial time and effort in a data analysis project, once it’s done, it doesn’t make sense to make every consumer of the analysis have to repeat it. This is especially true when distributing results to the rest of your team at work, where recreating difficult-to-reproduce results is expensive work time. Modern object storage services like Amazon S3 are fast, easy-to-use, and cheap — perfect for throwing large reams of data against the wall (sound unfamiliar? read "Object Storage: What is it?"). They make redistributing data to your team a lot easier. But they still have two key disadvantages: Enter data packages. A data package is a list of files, file storage details, and user metadata. Data packages provide a compact name for data.Instead of obliquely passing around pointers to files we need, we ask for nice names like aleksey/october_sales_analysis. This makes reading, writing, and modify your data science data dependencies substantially easier because it provides: And it’s simple. Our t4 Python API uses data packages to manage data with four commands: At Quilt Data we’ve talked to data scientists and data engineers from 50 plus different companies. We saw a need for a data packaging service in many of these conversations. Some of the companies we talked to had even gone so far as implementing data packages internally themselves. Many of the companies we’ve interviewed spoke about how hard it was to share data in a dynamic team environment. Data gets overwritten and lost, or the original data sharer moves on and nobody knows where to find it anymore, or the data is shared in a place or format that isn’t easy to find or use. One anti-pattern is the "human search network": when data analysts are forced to email around the team to locate someone who can locate their data. Team members re-building the same or similar datasets, instead of re-using one another’s work, is another prominent anti-pattern. The larger and more diverse the data needs of the team, the worse these problems become. Unified data packages enable unified dataset search. And all of the sudden most of these problems go away, because now there is a single point of reference you can go to for all of your data and metadata: the data catalog. In Quilt T4, every version of every data package you ever publish is available in the data catalog: Individual data packages and files are searchable from a single, unified interface: These features makes it easy to find, share, and reuse data, making you more effective as a data science team in the process. When there’s no standard way of distributing data, team members are reluctant to write documentation as well. Working on docs is boring, and when you’re not even sure if anyone will read your documentation, it starts to feel like a complete waste of times. This creates situations where one team member knows things about a dataset that others don’t. When another team member starts working on a data science project involving the dataset, they end up having to work through the same stumbling blocks that the original team member dealt with — stumbling blocks they could have avoided, if only they had known to read the docs or to talk to their coworker. We call this the "tribal knowledge problem". It’s a nefarious, but preventable, idiosyncrasy of data-driven teams. The solution is partially cultural. Make it easy for your data scientists to know what everyone else is working on (and hence, who to ask for help) with things like code reviews, show and tells, and team lunches. After all, as Randy Clinton of Locally Optimistic writes, "Bad communication kills good analytics". But tech can help a lot, too. At Quilt we find inspiration in AirBnb’s Knowledge Repo, introduced with the saliently titled blog post "Scaling knowledge at AirBnB". The Knowledge Repo "democratizes" data access by providing users with a centralized service for presenting their analyses and insights to the rest of the team — like team lunches, but in a more scalable written form: Data packages put all of your data and metadata in one place: your data catalog. The data catalog, in turn, puts dataset documentation and metadata front and center in front of your users. This makes it easy to share your dataset knowledge — just present it alongside the data on the data package landing page. To this effect, in Quilt T4 we provide content cards beneath the data files on the data package landing page that allow users to display important data, metadata, and documentation alongside their data packages. We support a variety of file formats, including Jupyter notebooks: Markdown files: And Vega visualizations: (you can see this feature for yourself by browsing our demo catalog) We believe that presenting data alongside metadata in this way is an effective way of avoiding the tribal knowledge problem, allowing better and more effective scaling out of your data science team. Airflow creator Maxime Beauchemin argues in "Functional Data Engineering — a modern paradigm for batch data processing" that the descending cost of storage (and, to a lesser extent, compute) means that we rarely delete data anymore. Immutable, functional data infrastructure is the new normal. Although that analysis is specific to data engineering, the same facts rings true in data science as well. Data packages are immutable snapshots of the state of a dataset at a particular point in time. Immutable means the dataset is persistent: once we create a data package it will remain accessible forever after. The entire version history of an immutable dataset is always accessible and fork-able by collaborators and/or the world at large. Immutability is not free. Storing mutable data means storing a data object once and overwriting the object every time there is a change. Storing immutable data (in the simple case) means creating a brand new object every time a new version of the object is created. That comes at the cost of storage; but storage is so cheap nowadays that by and large it’s worth it, because immutability has many attractive properties: The advantages of immutability are already well known in data engineering. Data packages bring many of these advantages to the world of data science. And the advantages are real. We’ve already shown how data packages allow you to "Reproduce a machine learning model build in four lines of code". In future articles on this blog we will demonstrate how versioned immutability enables even more robust design patterns, like continuous delivery and data quality circuit breakers, on top of data. In this article we discussed the nascent concept of the "data package", and strove to show how they help provision effective data science teams along three dimensions: ease of data reuse, ease of documentation, and immutability of data. Publishing data packages is an accepted practice in the R community. The Frictionless Data group has a formal spec for data packages (which is used in the Kaggle API). Meanwhile, in the machine learning community, there is talk of feature stores and model zoos — both in essence specialized data package catalogs. Interested in data packages? Check out or contribute to Quilt T4 on GitHub. Written by Written by 