In this notebook I hand-implement a perceptron. The perceptron is the simplest possible neural network. It lacks any hidden layers, and uses binary threshold as its activation function. How do you train a perceptron? Here's the example for a binary classification problem: The geometric interpretation is that the set of weights that boundarize a correct solution for any given training sample form a separating hyperplane. Lots of training samples draw lots of separating hyperplanes. The solution space of acceptable weights is the set of coordinates in the affine space constructed by the half-planes defined by the hyperplanes. This is a convex optimization problem! Here's an application to a trivial binary-separable dataset. Completely correct! Here's an application to a dataset that is not binary distinguishable. As you can see it gets this one very, very wrong! 