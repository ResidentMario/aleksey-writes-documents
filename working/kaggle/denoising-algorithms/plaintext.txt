Basically any digital or analog process which produces a signal is vulnerable to noise. For example digital microphones get noise from random electron excitement (which occurs at any temperature above absolute 0), and analog film cameras get noise from differences in the sizes of the grains of the exposed film strip. When trying to quantify this variance there is additionally the measurement error of the instrument itself to content with; while the underlying . An important preprocessing step for any signal processing task is denoising. There is a broad literature on denoising algorithms and techniques dating back centuries, and a number of fundamental techniques (like Fast Fourier Transforms) that have fallen out of from this. In Python the principal library for performing signal processing tasks is the signal submodule in the scipy package. In this notebook I briefly go through the denoising algorithms available. This notebook is somewhat an elaboration on xhlulu -- exploring signal processing with scipy. We will use the following demo data: The simplest algorithm for denoising time-series data is taking a summary statistic using a rolling window. A rolling window collects observations into groups of $n$ size. The groups are shifted one observation at a time, creating a "window" that passes over the dataset, hence the name. Each observation is part of $n-1$ groups, except for observations very near the beginning or the end, which appear in fewer (although you may choose to anneal them). Any summary statistic can be used within the rolling window to aggregate the data, though the average is the most popular default option. Rolling windows have the following advantages: Rolling windows have the following disadvantages: The simplest rolling window technique is to apply an average on a neighborhood. However, you can apply any algorithm on the window, including a very complex one. pandas has very strong support for rolling window techniques built into the library (taken from scipy). You can define a more sophisticated window: e.g. an exponentially weighted window, which greatly reduces the contribution of points further off from the center of the neighborhood. For the most part more sophisticated windows mostly make the result more robust to outliers. For more details see the docs. One rolling window technique worth particular mention is the median filter. When applied to an image a median filter has the effect of smoothing (color) noise in the image whilst retaining boundaries in the image. This is obviously useful in image processing because noisy images will have many spurious boundaries, bad, but smeared images will have weak boundaries, also bad. Convolution is a mathematical operation which generates a new function (or pointwise distribution) that is a function of two prior functions (or pointwise distributions). Convolution is geometrically a graph of the area under the curve of two functions which are moved towards and then away from one another ("convolved"). For example, if the two functions are both squares, their convolution will be a triangle function beginning where the moving squares just begin to touch, maxing out where they overlap completely, and going back to zero where they just touch the other way. In the continuous case it is defined mathematically as: The visualization from the Wikipedia article for it has a great intuitive explanation of how it works. Any operation that can be expressed using a rolling window can be expressed using a convolution. For example, we can re-express a rolling mean with a window size of 10 by asking for the convolution of the function with a square window function (or "kernel") of length 10, divided by 10 (since area / length = average height): If this equivalence doesn't jump out at you right away, a good reference on it is this and this. In general you shouldn't need to use convolution to perform smoothing; the rolling window methods are a better "form factor", but you may be performing convolution under the hood, as convolution is an efficient array-linearizable operation and generic rolling window functions are not. A large number of algorithms have been developed and published over the years for denoising signals. These algorithms are generally known as filters, or sometimes "digital filters" (to distinguish from electrical analog filters). Denoising is one of the fundamental tasks of signal processing, and it seems that different fiends have developed slightly different approaches, dependent slightly on the particularities of the kinds of problems they tend to run into. For a list of filters available in scipy see the corresponding entry in the documentation. A lot of filters are based on rolling averages or convolutions under the hood because those are the core operations for operating on data. Good filters just make smart mathematical adaptations that, for some subset of tasks, outperforms simply taking the average of the window. The next section describes a relatively simple example of such a filter. B-splines or "basis splines" approximate data as a smooth sequence of piecewise exponential functions. The functions being exponential means that they have terms of the form: The functions being piecewise means that each one is only used for a particular subsequence of the data. For example $f(x)$ for $[0, 0.5]$ and $g(x)$ for $(0.5, 1]$. The points where functions intersect are known as knot points. The last major requirement of a B-spline is that it must be smooth, e.g. continuous, e.g. it must have a derivative defined at every point. Geometrically this means that the area around the knots must be smooth, just like the spline is smooth on the body of the function. So the functions must join together in a way that preserves local topology. Approximating a dataset using B-splines means solving for the exponential function which best fits the data within a local area between knots. You can use any general-purpose optimization algorithm to do this; stochastic gradient descent for example (see the "Curve fitting" section of the Wikipedia article for a bit more detail). More complex functions can theoretically fit data within a region better, but require more work to optimize, as they have more parameters to tune. In practice cubic curves are sufficient for almost all applications. Here is an example of a curve approximated using B-splines:  B-splines and their slightly more advanced cousin the Bezier curve play an important role in computer graphics. B-splines are more powerful and compact than rolling averages, as they generate a output function, not just a bunch of numbers. The problem with using B-spline approximation directly (besides the fact that it really isn't all that well documented; I don't quite understand the application surface) is that determining where the knots should be is difficult. A good durable procedure for doing B-spline approximation was discovered in the 1960s and applied to the field of analytical chemistry (where it is still very dominant). It's called a Savitzky-Golay filter; you can read about it in its Wikipedia article. Basically all it does is roll a window along the data, build a curve in the left-right neighborhood of the central point (e.g. if the window size is 9, there are 4 points to the left and 4 to the right), and then set the value to that curve's approximation of the value at the middle of the neighborhood. A good animation showing this in action is located here. This algorithm is that it has an analytical solution, which makes it quick to calculate. But it is starting to be a bit difficult to grasp mathematically. Complex filters take the idea of reducing the noise in a dataset far out into the realm of statistical modeling. However almost all of the relevant techniques accessible from e.g. scipy and to a regular programmer are relatively ancient, having been well-developed by the 1970s or thereabout. They predate the rise and rise of machine learning over the last few years. If you think about it, modeling data is not that much different from denoising it. No useful model captures all of the variance in a dataset, so it learns to find the explainable variance and omit the incidental variance. Building a machine learning model on a dataset is not that different from denoising it. Therefore we can use a machine learning algorithm as an alternative to a filter, e.g. instead of applying a filter to denoise a dataset we can apply a machine learning model. This is a lot like pretraining in neural networks! For example, here is a simple model curve using the K nearest neighbors algorithm: The KNeighborsRegressor algorithm is just a rolling window along the dataset that sets the value of the model at every point to be the (potentially smoothed) average of the surrounding points. If you think about it, that's not all that different from the Savitzkyâ€“Golay filter from earlier, which does almost the same thing; the only difference is that Savitzky-Golay builds a cubic spline and uses that instead. The biggest disadvantage of using machine learning algorithm to do smoothing like this is that machine learning algorithms (particularly parametric ones like kNN regression) are generally much slower than filters. Filters were designed in an era where compute was many orders of magnitude more expensive. They tend to make statistical assumptions about the underlying data which, although never entirely valid, leads to simple computations and fast implementations. Typical machine learning algorithms can take much longer to do the same work, but don't make assumptions about the data. Additionally they tend to be more familiar to the practioner. As the notebook VSB Power Line Faults EDA + Feature Engineering points out, likely the strongest signal in the VSB Power Line Fault Detection competition dataset is the level of noise in various segments of the dataset. Denoising algorithms are relevant to the dataset because you can back out the level of noise in the dataset by comparing the original dataset with the smoothed version. Try using that as a feature for this competition. 