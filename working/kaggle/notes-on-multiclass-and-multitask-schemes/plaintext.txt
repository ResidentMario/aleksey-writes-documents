In this notebook I discuss multilabel and multioutput schemes: what they are, how they work, and what problems they address. The content here is my notes on the sklearn user guide section on the topic here. In the common parlance we refer to problems in machine learning as belonging to one of two kinds: regression tasks, which involve fitting data to ordinal or interval values; and classification tasks, which involve fitting data to categorical values. Classification tasks can further be subdivided into binary classification and multiclass classification problems. The difference is simple: in the former there are only two possible classes, while in the latter there any many possible labels. For a human observer, these two problems are not so different; one just has more "stuff" you can bucket your data into. The same is not necessarily true for algorithms, however. For example, consider the support vector machine. SVMs classify data by attempting to divide the data into different class groups using separating hyperplanes. In its native form, a simple SVM is just a line splitting one group of points from one other group of points. If there are more than two sets of points, we're already stuck: one line cannot be used to divide more than two things!  On the other hand, consider the Naive Bayes algorithm. NB is principled on independently distributed probability functions. The probability functions do not care about how many classes there are; more classes just means more marginal probabilities that need to be computed. Thus we may say that naive Bayes is inherently multiclass, whilst support vector machines are inherently not. Support vector machines and other similar not intrinsically multiclass classifiers may be made multiclass in a variety of ways. In the native sklearn implementation this is done one of two ways. The first (and general preferred) methodology is one-versus-rest. In one-versus-rest classification we train binary classifiers pitting a sample class of interest against all other classes in the dataset. To cover every class, a binary classifier is trained $n$ times, where $n$ is the number of classes contained in the dataset. Once the model is fitted, new points are classified by finding (in cases in which there is prediction overlap) which of the $n$ classifiers assigns this point to a given class with the highest probability. This scheme is computationally efficient and also relatively easily interpretable, making it a good default. The alternative is one-versus-one. In one-versus-one you train classifiers on each pair of classes. Then, when predicting new points, each classifier votes on the class of the point, and the class with the most votes is chosen as the winner. In the event of a tie, you may select the class with the highest aggregate classification confidence by summing over the pair-wise classification confidence levels computed by the underlying binary classifiers. One-versus-one classifiers are both more computationally expensive, requiring far more classifiers to be trained, and less immediately interpretable. However, if the classifier being used scales poorly, and the dataset is sufficiently large, training this many two-class classifiers may be faster or provide better results than classification in the one-versus-rest scheme, which considers every point. In sklearn, SVM and NuSVM are the only classifiers which implement multiclass classification using the one-versus-one scheme. It is also an option for GaussianProcessClassifier, which supports both. I do not know why this, exactly, though I suspect it has to do with the fact that SMV does not natively support confidence scores without expensive bootstrapping. All other classification algorithms that don't support multiclass naively follow a one-versus-all scheme. For the complete list breaking algorithms down by which of these three classes they belong to (sorry), check out the official sklearn documentation on the matter. sklearn defaults to providing its own implementation of a multiclassification strategy directly in the classifier. However, you can choose to implement either of these strategies using external OneVsRestClassifier and OneVsOneClassifier objects that it also provides. Here are some recipes provided in the documentation: Implementing the same multiclass scheme used inside of the module offers little to no benefit, but potentially using an alternative scheme is an interesting path you can take. However, in truth this is mainly useful for experimentation, as the classifiers produce classification results that are not much different from one another, and usually the one chosen by the algorithm is there for a reason. In addition to one-versus-one and one-versus-rest, skelarn also provides an interesting more advanced strategy in the form of error-correcting output codes. The OutputCodeClassifier represents each of the possible classes using a binary code consisting of bits that may be only either 0 or 1; in sum you have a Euclidean space, a hypercube, consisting of these binary dimensions. The matrix of which codes correspond with which classes (including codes which correspond to no class) is known as the code book. Intuitively, each class should be represented by a code as unique as possible and a good code book should be designed to optimize classification accuracy, but the sklearn currently implements only a randomly generated code book (though more complicated schemes would be a welcome addition). At fitting time, one binary classifier per bit in the code book is fitted. At prediction time, the classifiers are used to project new points in the class space and the class closest to the points is chosen. A code_size attribute allows the user to control the number of classifiers which will be used. It is a percentage of the total number of classes. A number between 0 and 1 will require fewer classifiers than one-vs-the-rest. In theory, log2(n_classes) / n_classes is sufficient to represent each class unambiguously. However, in practice, it may not lead to good accuracy since log2(n_classes) is much smaller than n_classes. A number greater than 1 will require more classifiers than one-vs-the-rest. In this case, some classifiers will in theory correct for the mistakes made by other classifiers, hence the name "error-correcting". In practice, however, this may not happen as classifier mistakes will typically be correlated. The error-correcting output codes have a similar effect to bagging. Thus the utility of error-correcting output codes for multilabel classification is that, with smart design, we can build a multiclass classifier that interact more "smartly" than simply aggregate voting. However, the sklearn implementation doesn't feel ideal for this. Recall that the two most common machine learning tasks are regression and classification. These are the two core machine learning tasks, but we may construct some other ones by considering combinations of these two tasks. Multi-label clasification is the tasks of assigning multiple labels to records in a dataset. In classification the assumption is that every single data point should have a single class. Labels, by contrast, are more flexible: a data point may have one label, or several, or none. An example of a multi-label classification task might be suggesting tags to use for new questions being asked on Stack Overflow. Multi-label classification is supported natively by a subset of the algorithms in sklearn. All of these algorithms will accept not just a vector but an entire matrix as their y when calling fit. For example, the sklearn DecisionTree can seek and split the dataset in a way which produce the greatest reduction in class mixage, summed across all of the target variables. Alternatively, to assign multiple labels to our data we may train one classifier for each of the possible labels, with binary classes of "has this tag" and "does not have this task". Both OneVsOneClassifier or OneVsRestClassifier support this! If the correlation between the target variables (tags) does not contain significant information not already obtainable in the predictor variable matrix, then this likely simpler approach provides maximum flexibility. The recipe for turning your favorite classifier into a multi-label classifier is simple. In the following two code cells I demonstrate an application of this idea to a simple synthetic dataset. The tight dark blue circle is class 1, whilst the larger light blue circle is class 2. There are also some points in this dataset which have no label at all; these do have neither circle around them. As you can see, with this simple dataset our multi-labeler performs pretty well right off the bat. You can apply the standard one-dimensional classification metrics to each column of labels in the dataset individually, and then aggregate them however you like (e.g. taking an average). Computing labels using the one-versus-rest scheme follows this methodological path anyway. On a case by case basis sklearn metrics may also support broadcasting their result across multiple, where this makes sense. For example, here's a classification accuracy score: The classifier chain is an experimental but very interesting feature of sklearn. ClassifierChain may be used to perform multi-label classification, with a twist. Each label in the sequence of labels being computed is added on as a feature towards the next label being computed. In other words, the first label in the chain has only the base feature matrix to work off of, whilst the last label will also include every other label computed as a feature. If there's some natural way of arranging labels from "easy" to "hard", this unusual arrangement can improve the overall accuracy of the model. The boruta package implements something akin to this sequentially for the purposes of feature selection (described here). Multi-label classification is actually a special case of multi-task learning. Multi-task learning is anytime you are asked to solve a contiguous set of machine learning problems simultaneously. If the tasks are only regression tasks, this is a multi-output regression problem. If the tasks are only classification tasks, this is multi-label classification. If the tasks combine regression and classification, this is a multi-output multi-class problem; sklearn doesn't actually provide any algorithms for this last case, and you should just split the problem up into separate classification and regression tasks-sets. Let's now turn our attention to multi-output regression. For example, you might want to predict the wind speed and the wind direction for a certain set of records, simultaneously. The only algorithm in sklearn which implements multi-output regression natively that I'm aware of is sklearn.linear_model.MultiTaskElasticNet. Just like with decision trees, this implementation is ideal to use if there is informative covariance in the relationship between target variables not explained by the predictive features alone. The API is the same: the only difference is, again, that we can feed a matrix of dependent variables as the y parameter when fitting the model. As with multi-label classification, we may consider the multi-output regression problem as a sequence of subproblems, and solve them using separate classifiers. The MultiOutputRegressor may be used to do this. The documentation provides the following recipe: Hopefully you found this material useful! 