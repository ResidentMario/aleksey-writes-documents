Once we've built a model, it's important to understand how well it works. To do so, we evaluate the model against one or more metrics. This notebook is an overview of some of the most common metrics used for regression models. We'll implement the metrics and test them out on a mocked-up regression target. The first and most immediately useful metric to use in regression is the $R^2$, also known as the coefficient of determination. For a vector of values $y$, a vector of predictions $\hat{y}$, both of length $n$, and a value average $\bar{y}$, $R^2$ is determined by: The coefficient of determination is a measure of how well future samples will be predicted by the model. The best possible score is 1. A constant model which always predicts the average will recieve a score of 0. A model which is arbitrarily worse than an averaging model will recieve a negative score (this shouldn't happen in practice obviously!). In practice, it is a "best default" model score: other metrics may be better to use, depending on what you are optimizing for, but the $R^2$ is just generally very good, and should be the first number you look at in most cases. $R^2$ is such a popular metric that there are artificial $R^2$ scores, designed to work in a similar way but with completely different underlying mathematics, which are defined for other non-regression operations. The residual sum of squares is the top term in the $R^2$ metric (albeit adjusted by 1 to account for degrees of freedom). It takes the distance between observed and predicted values (the residuals), squares them, and sums them all together. Ordinary least squares regression is designed to minimize exactly this value. RSS is not very interpretable on its own, because it is the sum of many (potentially very large) residuals. For this reason it is rarely used as a metric, but because it is so important to regression, it's often included in statistical fit assays. There is no scikit-learn implementation. Mean squared error is the interpretable version of RSS. MSE divides RSS (again adjusted be 1, to account for degrees of freedom) by the number of samples in the dataset to arrive at the average amount of squared error in the model: This is easily interpretable, because it makes a lot of intrinsic sense. Ordinary least squares regression asks that we minimize quadratic error; MSE measures, on average, how much such error is left in the model. However, due to the squaring involved, it is not very robust against outliers. Mean absolute error computes the expected absolute error (or L1-norm loss). Because it involves means, not squared residuals, mean absolute error is more resistant to outliers than MSE is. Mean absolute error computes the median absolute error. Because this value is not only an absolute value, but also a median instead of a mode, this metric is the most resistant metric to outliers that's possible using simple methods. Root mean squared error is an error metric that's popular in the literature. It is defined as the square root of mean squared error: Since this is just the root of the MSE metric mentioned earlier, we will omit an implementation. RMSE is directly comparable to, and serves a similar role as, the MAE, mean absolute error. The difference between the two computationally speaking is that MAE takes the square root of the distance inside the sum, while RMSE takes the square root outside the sum. The computational effect is that RMSE is less resistant to outliers, and thus reports a poorer-fitting model when outliers are not properly accounted for. This is considered a good thing when doing cetain things, like performing hyperparameter searches. However, MAE is a more useful reporting statistic because MAE is interpretable, while RMSE is not. Context for this comparison here. Note that scikit-learn doesn't provide a RMSE evaluator directly... The explained variance score is a very clever (IMO) metric which looks at the ratio between the variance of the model/truth differences and the variance of the ground truth alone: Hence the moniker "explained variance". The best possible score is 1 (all variance is explained) and the score goes down from there. A further reference on explained variance is here. That concludes this metrics overview section!  Future notebooks will likely explore the comparisons between and decisions about what metric to use in more detail. There are also many other common metrics that we can use that we will consider later as well. 