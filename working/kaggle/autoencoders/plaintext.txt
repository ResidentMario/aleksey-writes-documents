Autoencoders are a type of neural network which learns a sparse representation of an input. In other words, an autoencoder, once trained on appropriate training data, can be used to generate compressed copies of an input data point which preserve most of the information (features) in the input using significantly fewer bits of information. Technically speaking, an autoencoder is any neural network with three components: an encoding function (which translates the data into a simpler space, e.g. a hidden layer with fewer nodes than on the input layer), a decoding function (which reverses this process, e.g. an output layer with equivalently many nodes as the input layer) and a distance metric (which measures loss as some distance between the original input and learned representation). The earliest type of autoencoders are Restricted Boltzman Machines, which are important enough to the history of deep learning that they get their own name cred. Autoencoders are the neural network equivalent of simpler variable compression techniques, like PCA and LDA. Autoencoders are practically used in deep learning applications for pretraining: determining the "right" weights for a neural network ahead of time, so that the algorithm doesn't have to work as hard to converge as it would if it had to start with completely random weights. For more on pretraining see my notebook on RBMs. In practice, the emergence of faster convergence algorithms has obviated the need and utility for pretraining. Thus autoencoders are no longer used in cutting edge deep learning work. They're also not notably better than simpler (usually information-theoretic) compression algorithms, like JPEG and MG3, in commercial applications. These days they're most used as a preprocessing step on high-dimensionality data before input to T-SNE. But they are nevertheless historically and pedologically important. This notebook based on this blog post. The simplest possible autoencoder is a single hidden layer of $n < \text{# of input pixels}$ nodes. The output layer of this model mirrors the input layer in size. In the next few code cells we train just such a simple autoencoder and demonstrate its output. We see here that the decoder trained in our autoencoder learns grainy (lossy) versions of the original images. This form of simple one-layer autoencoder learns a representation of the underlying dataset that is very close to what is learned by PCA. You can thus think of this simple autoencoder as being a kind of stochastic approximation of the more deterministic PCA algorithm. The idea is that an autoencoder ought to learn a spare representation of interesting features of the dataset, but when using standard convergence techniques more often than not it winds up learning how pixel maps moreso than interesting dataset features. One way to address this problem is to use regularization to introduce sparsity: make it so that only certain nodes fire on certain images, and the remainder stay silent. This way instead of learning "how to assemble fuzzy patches" into reconstructions of your inputs, the network will (hopefully) learn "how to assemble interesting features" into reconstructions of your input. You can induce sparsity in your connections using L1 regularization. You can also use deeper autoencoders. These have all the benefits and tradeoffs you would expect of increasing the number of layers in your neural networks, namely: longer training times and less easily decoded representations, but more accurate reconstructions. A good default choice for a deep autoencoder is to scale the image down progressively, than scale it back up again, using the same number of nodes on each layer on either side of the "most compressed" representation at the center of the hidden layers. 