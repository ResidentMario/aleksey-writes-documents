Naive Bayes is a machine learning model that's often used as a baseline for classification tasks. It's an interesting model because it performs reasonably well in a reasonably wide range of applications, but its underlying assumptions are very, very simplistic. Baye's theorem states that, for an observation $y$ with feature vector $(x_1, x_2, \ldots)$, the following relationship holds: Baye's theorem is one of the most fundamental theorems in all of probability theory, and one of the earliest things taught in a probability course in college. It is the foundation to the Bayesian side of statistics. Naive Bayes is "naive" because it makes the simplifying assumption that all of the underlying variables are independently distributed. If we make this simplifying assumption, then for any two $x_i$ and $x_j$, $P(x_i \cap x_j) = P(x_i) P(x_j)$. This is, intrinsically, a ridiculous assumption. Most any data that you run into will have variables that are correlated, even highly correlated with one another; and most any variable will have some degree of correlation to other variables in the dataset. A situation where none of the variables are interdependent in any way simply doesn't arise. But suppose that it holds. Then the relationship we started with simplifies (via the chain rule; see the derivation on Wikipedia) down to: In other words, the probability a record is in the class $y$ has a closed form solution that can be calculated relatively easily! In practice the probabilities generated by the naive Bayes classifier are highly suspect. However, when we use these probabilities to classify, assigning each point to the $y$ class that maximizes its likelihood, we get a result that works remarkably well remarkably often. Ultimately, a Naive Bayes classifier implements the following: There are a few different variations on the naive Bayes algorithm. The differences between them are differences in the assumptions they make about the $P(x_i \: | \: y)$ term, which allow us to adopt the naive Bayes algorithm for different kinds of data. $P(x_i \: | \: y)$ is the probability that a particular feature of a record will be some value, given that the record's class is ultimately $y$. In the following section I will briefly sketch out the three naive Bayes algorithms provided by sklearn. If the features are numerical, we may assume that they are normally distributed, so $P(x_i \: | \: y) \sim N(\mu_y, \sigma_y)$. The parameters $\sigma_y$ and $\mu_y$ (the variance and the mean of the underlying normal distribution) are estimated using maximum likelihood. We repeat this process for every feature $x_i$ and every category $y$ in the dataset, then plug the resulting values into the formula to get our result. This is known as the Gaussian Naive Bayes. Alternatively we may have binary features. This is known as "Bernoulli trials". In this case we can use the simple formula: Where $i$ is a feature of interest, so $P(i \: | \: y)$ is the probability of that feature appearing (being non-zero) in a record with the class $y$. This is known as the Bernoulli Naive Bayes. Alternatively we may have categorical (multinomial) features. This is the most mathematically complicated of these three simple models (at least, to my eyes). Let $\theta_y = (\theta_{y1}, \ldots, \theta_{yn})$, where each of the possible classes is $y_1, \ldots, y_n$. Let $\theta_{yi} = P(x_i \: | \: y)$, e.g. the probability of the feature $i$ appearing in a sample belonging to class $y$. We estimate: Where $N_{yi} = \sum_x x_i$, the number of times the feature $i$ appears in a sample of class $y$ in the training set, and $N_y = \sum_{i=1} N_yi$ is the count of all features for class $y$. This algorithm is weighted by an $\alpha$ parameter in order to avoid trivial 0-probabilities from messing up the output. (I omit a demonstration because a good demonstration of a Bayes classifier is document classification, which involves feature transformation that needs explaining in a separate notebook yet-to-be-created at time of writing) The salient points of the naive Bayes classifier is that it works remarkably well, given that the algorithm is so simplistic and has been around for so long (since the 50s apparently). It is the classical baseline of algorithm of choice for classifying documents for NLP tasks. Why does it do so well in that arena? Document classification tasks are extremely sparse, with many features (words) that appear only rarely. By modeling everything in only one dimension, the naive Bayes algorithm sidesteps the problems that this dimensionality causes (see Curse of Dimensionality). The other thing worth noting about the naive Bayes algorithm is that even though the probabilities it generates are shoddy, the classes it assigns are pretty good. This occurs because the algorithm is good at finding relative distributions, not Assigning a class is an argmax of a probability, so what this implies (correctly) is that the Naive Bayes algorithm is surprisingly good at determining the relative importance of various features, as opposed to their absolute importance. For classification, this often is good enough! Information criteria algorithms, useful for model selection, another arena where it's the relative values that matter, operate in the same way. 