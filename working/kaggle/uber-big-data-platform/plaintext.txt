https://eng.uber.com/uber-big-data-platform/ Initial data architecture was just a database. In fact, there were multiple databases, and no complete global view. The first generation of dedicated data software (2014-2015) focused on aggregating all of the data in one place, and on streamlining access for the various stakeholders (thousands of city operations people, hundreds of data scientists and business analysts, hundreds of engineers). Eventually this stopped scaling well. The data warehouse was used as a data lake, resulting in a lot of obsolete data artifacts taxing the query system. Also ETL jobs feeding into the warehouse were ad hoc and usually JSON-based, which led to data quality issues. Take two used the Hadoop ecosystem. Parquet files were written to object storage, giving high compression, schema enforcement, and low cost per store. ETL jobs became a bottleneck because each ETL run would have to recreate the table it was operating on, which became progressively more and more expensive as there were more and more raw parquet files that needed to be parsed to make this happen. This was also true on the output side; generating the necessary snapshots became very expensive. After some thought their solution was to implement an open-source library that abstracts over Spark called Hudi, which optimizes certain elements of query runs to match the Uber job profile. 