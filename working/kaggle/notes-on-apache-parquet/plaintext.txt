The following is a transcription of what I gleaned from their "Documentation" page. parquet is a modern columnar data serialization format that was originally created for use in the Hadoop ecosystem. It is an open source implementation of a scheme originated at Google and made public in the Dremel paper.  It supports complex nested data types, efficient compression and encoding schemes, and backwards and forward reverse compatibility using the Apache Thrift message protocol. Parquet files are defined using a record shredding and assembly algorithm. First a word on the format. Parquet file defintion include groups and fields. Groups are repeatable named pointers that point deeper into the stack. Fields are key-value pairs that hinge off of a group. The root group is the document root. Fields and groups alike can be required, optional, or repeated. So e.g. here is an example parquet file serialization format: Here's the corresponding document example: Serializing a record is a depth-first tree traversal. Whenever you reach a non-zero leaf, record to the column store the value, the repetition level (initially zero, but this number will grow if there are multiple instances of this field in the record tree), and the definition level. When traversal bottoms out an an empty field, no value is written, but the definition level for the last defined level for all leaf nodes that could appear below the node does get written. The vaue and repetition level are obvious, but the definition level is more nuanced. The insight is that in a tree following a schema, the non-presence of a parent field immediately implies the non-presence of any and all child fields as well. For example: When parquet serializes a document fragment, as a natural consequence of its depth-first traversal order it stores the maximum depth of each group alongside that group's name. When parquet deserializes a column of data (corresponding with a field), it sequences through the groups in depth-first order. Any parent groups that have maximum definition levels that are too low for the desired columnar value to appear do not need to be parsed any further and can be skipped in the iteration order. This allows parquet to skip the bulk of the tree parsing operations when the column being requested is sparse (as would be typical for some Hadoop workflows). Note that required fields are obviously non-sparse, and so this machinery does not kick in. Similarly, repetition levels are only necessary for fields which may be repeated, as they inform the construction algorithm when to stop looking for more values with the same field. parquet serializes to file using the following top-down scheme: The PAR1 sequence is a file signature. The data is divided into individual columns and groups of rows (row chunks), which major ordering on the chunks (e.g. group row-major ordering) and minor ordering on the columns. Data from the same chunk is colocated on the file, e.g. file access against chunks is disk seek free. The file metadata includes the location of all metadata start locations, making row chunk access by offset efficient (requires a disk seek and not a file scan). It is included at the tail end of the file in order to allow single-pass writing. The expected reader behavior is to access the file metadata first, determine desired row chunks, and then access those chunks sequentially. In a distributed setting data of interest may be partitioned into multiple parquet files in storage, and the read process may be parallelized via an external coordinator (e.g. a Hive metastore). Here is the detailed format diagram:  Notes: Physical types are: parquet provides a selection of logical types which map on top of these: https://github.com/apache/parquet-format/blob/master/LogicalTypes.md. 