The reparameterization trick is a "trick" (really a technique) which became popular around the year 2015. This technique extends backpropogation to layers or features producing random noise. This allows a neural network to shape the output (the hyperparameters, e.g. the mean and standard deviation) of e.g. a Gaussian normal sampler. First a simpler explanation. This explanation is drawn from this blog post: "The Reparameterization Trick". Imagine you have the following TensorFlow code: The objective of this computational graph is to minimize the residual between the input value and the output value, where the output value is the input value plus some random noise. The "tool" for doing this is the standard deviation hyperparameter, delta. The optimal solution is to set this value to zero, and the hope is that the model will learn to do this. However, as things stand this graph cannot be backpropogated through: This occurs because the output of the random layer is a stochastic variable. Backpropogation over a stochastic variable would result in a stochastic gradient, which is not what we want‚Äîwe need a deterministic gradient. The reparamaterization trick is to move the hyperparameter outside of the random sampler: This can be backpropogated over successfully. In the case of the first code snippet, it isn't discoverable to the backpropogation algorithm what manipulating the delta value would do to the output value. In the case of the second code snippet, the backpropogation algorithm knows that, since the delta is related to the "black box" normal sampler by a multiplication operation. Backpropagation knows that to reduce the contribution of the random normal it can reduce the delta, and vice versa. A more complicated explanation that shows the math involved is available at this StackOverflow question-answer pair: "How does the reparameterization trick for VAEs work and why is it important?". In this Q&A we are asked to calculate $\min_{\theta}E_q[x^2]$, where $q_\theta(x) = N(\theta, 1)$. To do this "better", we rewrite x as $\theta + \epsilon$, where $\epsilon \sim ùëÅ(0,1)$, and do some mafs. I'm not going to understand this independently... In GuaGAN (paper), the reparameterization trick is used to make the noise input to the generator a function of the original image. The image is encoded (using a CNN) into a set of 256 learned pair-features. The values of these features are used as the mean value and the standard deviation value parameters of a set of Guassian random functions which generate random noise input to the model part of the generator. 