Logistic regression is an adaptation of linear regression to binary classificationâ€”predicting 0-or-1 target variables. Linear regression can technically be used for this task, but many of the fundamental assumptions built into linear regression are violated when you do this: Logistic regression returns a result that's interpretable as an exact probability because it's in the range $(0, 1)$. This form of regression returns a result based on the logistic function (hence the name): Logistic regression is premised around the squeezing that this function applies to data towards 0 or 1:  Here is the full logistic regression expression, expressed in terms of its fundamental unit, the logit: Where $P$ is the prediction (the probability), $\beta_0$ is the constant coefficient, $\beta$ is the vector of non-constant coefficients, and $\varepsilon$ is the error term. The term on the left is known as the "logit", and it does all of the heavy lifting. Notice that the term inside of the $\exp{\cdot}$ term is the linear regression formula! The logistic regression model is simply a non-linear transformation on linear regression. To get the result $P$ (or equivalently $y$), we do this to it: What we've achieved here is, via the application of the logistic function to the linear regression result formula, a transformation from a line to logistic curve:  Let's use the scikit-learn logistic regression function to predict which of two players will win a match, based on what each player's rank is. Here is the raw result in the record, which shows that the player with more ranking points wins somewhat less than two-thirds of the time. But our classifier classifies every single record as a greater-ranking-points-wins outcome! This results in the following propensity for error: We can understand this better by looking at predict_proba, which returns the probabilities that the result will be a 0 or 1 response, respectively. This shows that logistic regression can't find any response in the data; the most likely winners-by-ranking points are just 5% more likely to win than the most likely losers-by-ranking points. This is interesting, it shows that this model needs further diagnosing and problem-shooting. Looking at the coefficients (just one, since this is a univariate regression): This coefficient is the log-odds: the effect that a point in the difference between the two players has on the logarithm of the odds that the winner will be in class 1 (the higher-ranked players). To get the effect that one point has on the odds, we need to reverse the logarithm, by taking: Applying this to our result, we find: According to our model each point of advantage in the rankings results in one more point in odds favoring that player! Since the difference in points is distributed in the hundreds, or even thousands, this is more evidence that the model is simply taking the mean prediction, e.g. splitting the data at 0. 