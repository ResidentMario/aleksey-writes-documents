Note: this notebook is a continuation on Pumpkin Price Linear Regression. You should start there. Suppose that we have a target variable $y$ and, for a single records of interest, a set of predictor variables $x_n$. A linear regression model solves for a sequence of weights, $w$, which when multiplied against the data values, produces an estimated $\hat{y}$: In OLS, this equation is solved to optimize a squared distance fitness metric. An optimal solution to this equation will minimize the square of the distance between $y$ and $\hat{y}$, for some large number of records. There are two ways of changing things around. The first way is to change the metric that we optimize for. For example, what if instead of solving for least squares, we solved for least absolute values? It's just a question of how mathematically difficult this is to do. Mathematically, least squares turn out to be the easiest metric possible to solve for. The other way to change things is to change the model equation. This equation is linear because all of the weights $w$ are first-order. This makes it easy to solve this equation using linear algebra matrices (see the previous notebook for this solution). However, this also assumes that our features are related in a linear way. Oftentimes, this is not true! An easy way to extend regression to more complex feature relationships is to use a polynomial model. A second-order polynomial model (for two variables in these examples) looks like: I said earlier that equations are easiest to solve when they're linear, and this equation is no longer linear. What now? We can use a cute trick to make it linear. Just define the following variables: Then, relabeling the points: Tada! The equation is linear again. We can solve this equation using ordinary least squares, same as before, then "downcast" the $z_n$ variables into $x_n$ ones. That's how polynomial regression works. Now let's look at the scikit-learn implementation. We'll use polynomial regression to estimate the size of pumpkins sold in New York City, given their average price. The next code cell transforms the data into the shape we need it in: Now the implementation follows. scikit-learn implements polynomial processing as a general-purpose preprocessor on the data. Here's how you would create and run the model: For practice, we implement this pipeline by hand. The following code block does that. This is a wee bit of an algorithmic adventure. If you're interested in understanding how this implementation works, try poking around in the fit_transform function yourself. I recommend putting a import pdb; pdb.set_trace() statement on the first line of the __init__, and then running PolynomialFeatures().fit_transform(prices) in a separate code block. This will drop you into pdb debug mode, which will let you figure out how this thing goes. Note that the LinearRegression implementation is the same one we used in the previous notebook. Let's now look at our polynomial model performance. Here's a plot of the classification errors: If our goal is to be accurate within plus-minus one class, then our model is actually fantastic! That concludes this little two-notebook primer on regression. 