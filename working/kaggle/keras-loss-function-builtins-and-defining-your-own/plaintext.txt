A neural network learns by optimizing the loss that the network configuration has at the end of our optimization passes. That loss is determined by a loss function, and the interaction between the loss function and the dataset creates the loss surface that we are performing gradient descent on during the building of the network. Thus the choice of loss is an important one. Losses optimize for different things. Whilst simple losses like pure accuracy just penalize right and wrong answers, more complex loss functions like log loss penalize important underlying characteristics like misplaced confidence. Losses are common to all machine learning algorithms. sklearn calls them by the more general term "metrics", as a metric is only really a loss when the learner is using a stochastic or gradient descent approach. Different loss functions result in different probability surfaces with different characteristics, so changing the loss function fundamentally changes the learning process. Additionally there are interactions between different choices of loss and activation which have interesting effects. Meh this is not so interesting. 