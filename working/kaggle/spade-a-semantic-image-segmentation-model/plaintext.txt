This notebook contains my notes on the paper "Semantic Image Synthesis with Spatially-Adaptive Normalization".   Here's the full generator architecture. Gaussian noise is the input to a linear layer that transforms 256 input values to 16384 output values, which is reshaped into a (1024, 4, 4) random matrix. A SPADE ResNet block operates on this input, using convolutional layers with 1024 nodes (every Conv2d layer is fed a 4x4 input). Nearest-neighbor upsampling is then applied to the output to double the size of the output, which is propogated to the next SPADEResBlk. There are seven such layers, so the output is 4*2**7 = 512 in dimensionality. The output layer is a 3x3 Conv2d with three filters (R, G, and B) and tanh activation. Again the diagram:     First the image is decomposed by the image encoder into a 256-arity mean-and-variance feature vector. These vectors are used to compute the noise inputted to the generator via a "reparameterization trick". Basically, the reparameterization trick is a way of making the output of a random node, e.g. a Gaussian normal, backpropagation-friendly. I have a notebook on this here: "Notes on the reparameterization trick". The reparameterization trick is used to make the noise input to the generator a function of the original image. The image is encoded (using a CNN) into a set of 256 learned pair-features. The values of these features are used as the mean value and the standard deviation value parameters of a set of Guassian random functions which generate random noise input to the model part of the generator. Recall that a simple generator may take purely random noise as input. Shaping the noise using image input (such that images/segmentation map pairs that are close to one another in the feature space are close to one another in generator noise input) has been shown to have performance benefits in practice however. The generator operates on this noise inputâ€”plus the segmentation mask at many steps, via the SPADE normalization blocks. The SPADE normalization blocks are the only place in the model architecture where the segmentation maps are used. In other words the denormalization part of SPADE normalization is the only part of the model that is segmentation-aware! OK, recall that GANs in their pure form are an unsupervised learning algorithm, and this is what makes this one into a supervised one. The discriminator is fed (and operates on) the concatenation of the generator output plus the segmentation mask, and it outputs a "real or fake" classification. Another effect is that an information encoder segment, typically present in the GAN architectures that are designed for this task, is absent: With the SPADE, there is no need to feed the segmentation map to the first layer of the generator, since the learned modulation parameters have encoded enough information about the label layout. Therefore, we discard encoder part of the generator, which is commonly used in recent architectures. At test time, the encoder is repurposed for style transfer. The style image is fed to the encoder (seemingly in place of the original image input), from which the style random seed data is generated, from which a generated output image is drawn. 