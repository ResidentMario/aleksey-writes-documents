This notebook is my personal reference on debugging machine learning model GPU utilization. This is necessary when you discover that a model is using too little GPU (potentially none), or too much (potentially resulting in OOM due to memory leaks?). NVIDIA owns the machine learning on a GPU vertical at this time. That means that if you're running a PyTorch model or whatever on a GPU, you're doing so on an NVIDIA card and using the NVIDIA toolchain. The interface to running general purpose non-graphics programs on a video card is an SDK (card firmware support, toolchain, and application progamming language) known as CUDA. CUDA is proprietary and baked into NVIDIA GPUs. There are some competing frameworks, including a couple of open source graphics engines from the OSS community, but none have real traction thanks to NVIDIA's market control. The CUDA SDK assigns every CUDA device (GPU or maybe TPU) a sequential ID number counting up from 0. A global environment variable, CUDA_VISIBLE_DEVICES, controls the global visibility of these devices. The first step to debugging a model is verifying that this variable is set to what you expect it to be set to, or to set it yourself: -1 is a special flag saying no devices are visible (e.g. this tells models to use CPU). You can also theoretically use empty string ("") but this is sometimes buggy. This setting is respected at the library level by PyTorch. torch includes functions for checking up on the devices visible to it. From the StackOverflow Q&A: Best practice for PyTorch models is allowing the specification of the devices to be used by ID from the CLI, and perhaps defaulting to all visible GPUs (if parallelization is supported by the model) or to the first visible GPU (if it is not). Because device visibility is enforced at the library and firmware level, it is not possible to specify that a model use a device that is not visible. The best tool for checking device usage interactively is nvidia-smi. nvidia-smi is a command-line device monitoring toolkit installed as part of the CUDA SDK. Example output: nvidia-smi will have a separate output for every visible card on the machine. The list of visible devices should be the same as that visible to PyTorch via the torch.cuda library commands (if it's not you have a serious problem in your torch install!). The best solution is to use watch -n 2 nvidia-smi or similar to track GPU usage with periodic updates. However this command annoyingly fails when run in a Jupyter notebook; it displays only the first line of output (I don't know why). So you will have to run this command from a terminal instance. 