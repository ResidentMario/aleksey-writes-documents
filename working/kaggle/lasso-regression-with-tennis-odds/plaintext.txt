In a previous notebook we explored how the ridge regression technique can be used to reduce the complexity of a model. In this notebook we'll do the same thing using a different but related technique: lasso regression. Lasso (Least Absolute Shrinkage and Selection Operator) regression, like ridge regression, functions by changing the model fitness metric. The difference is that lasso regression applies what is known as an L1 norm: it penalizes variable coefficients on the order of their absolute value, instead of on the order of the square of their value (an L2 norm, like ridge regression). How it does this is mathematically involved. We'll focus first on understanding why it's useful through an application. The World Tennis Odds Database contains records of match outcomes and bookie betting odds for thousands of tennis matches. One interesting question to ask is this one: which of the dozens of betting exchanges is most or more accurate than the others? Sure we expect performance to be pretty broadly similar, assuming all of these exchanges are still in business, but I think it's interesting to ask, given the recent record of matches, which one of the exchanges is asymptotically closest to the match day reality. Notice that the Lasso  API is the same as the Ridge one. Unlike ridge regression. lasso regression does not shrink the coefficients down to near-nothingness when it determines a variable is not adding worthwhile information to the model. Instead, it performs variable selection: it takes variables that aren't significant, and sets them to zero! Looking at the coefficients, we see that, with an $\alpha$ of 0.00001, there are multiple exchanges that have already been "cut", and don't participate in the model at all. These two exchanges are the ones that provide the least unique information to the model: either they are the ones that most strongly follow the rest of the exchanges, or they are the ones that are wrongest. Neither regular linear models nor ridge regression will perform this variable selection directly. This is an extremely useful property of ridge regression: it can be used to determine what variables provide the least information, and hence, dropped from further models that you build. Let's take a quick look at model accuracy (ancillary, but kind of interesting to me): In the above plot, 1 means player 1 won, while 0 means player 1 lost (or, equivalently, that player 2 won). Green means that our model predicted the match outcome correctly, while red means it predicted it incorrectly. We're not going to get very deep into model performance. Building a betting model isn't really the point of this exercise. But since we're already here: Let's get back on track now. Because it reduces the number of coefficients, lasso regression, unlike ridge regression, changes the order of the curve. Here's an off-site example of what this looks like.  Eventually, with a high enough alpha, we get back down to a linear model with one variable! Overall, lasso regression is an alternative to ridge regression which has, for operational purposes, roughly the same use advantage (it corrects for the effect of colinearity in the model), but one important additional advantage: it can be used to remove highly colinear variables from the model. Let's go ahead and finally answer the question we started out with: which exchange is the most informative one, pound-for-pound? It looks like 10bet is the winner! 