This notebook presents a proof of ridge regression. It is a follow-up to some practical notebooks I wrote on the subject: this one. This proof relies on a couple of facts from ordinary least squares regression. Theorem 1: Given an $n \times p$ feature matrix $X$, a response vector $y$, and a parameter-coefficient vector $\beta$: The objective of OLS is to minimize the left-hand objective function for the parameter $\beta$, and it can be shown that the value of $\beta$ which does so is the one which solves the right-hand matrix equation. Ridge regression adds another term to this equation. Definition 1: Given the same features and parameters as Theorem 1, a ridge regressor for some parameter value $\beta$ is one which minimizes: To solve for what minimizes this value, we will need the following theorem. Theorem 2: The normal equation of linear regression. Now we can prove a closed form solution for the ridge regression equation. Theorem 3: The closed form solution for ridge regression is: In other words, the normal equation for ridge regression is: Notice how this just adds a single bonus term to the matrix equation! As discussed in the prior notebook. This is what makes a ridge regression estimator easy to solve, algorithmically. Now the proof: Due to our augmentation, there will be $p$ additional $(0-\nu \beta_i)^2 = \nu^2 \beta_i^2 = \lambda \beta_i^2 = \lambda \beta^T \beta$ terms over what we would get with $y$ and $X$ alone. The $\beta$ which minimizes our original normal equation will equivalently minimize this second matrix equation. It may be shown (from linear algebra) that for any choice of $X$ and $y$, $\exists \beta$ such that $(y_{*} - X_{*}\beta)^T(y_{*} - X_{*}\beta) = 0$. Hint: the resulting matrix is full rank. This is unsatisfactory, I admit... Therefore: This choice of $\beta$ will necessarily minimize the equation because it will take it to 0. We may apply theorem 2 to this matrix equation to find that: Great! With the math done, here's an implementation. Hardly any more complicated than regular OLS regression! Note: this notebook references the math here and here. 