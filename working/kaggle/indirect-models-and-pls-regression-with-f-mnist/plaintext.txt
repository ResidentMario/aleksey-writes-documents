No shirts no shoes no service. An indirect model is any machine learning model which doesn't learn using the target variables, but instead learns (internally) using something else. They are called "indirect" because instead of modeling the response variables (directly), we model some modified or manipulated version of those response variables. A trivial example of an indirect model would be using ordinary least squares regression to distiguish between shirts and shoes in this (fashion) dataset, by encoding the shirts and the shoes using a boolean value: True if it's a shirt, and False if it's a shoe. This is encoding our variables. To recover the "true class" we assign to the item in question, we will need to, after modeling, transform our model output back into the categorical number (6 or 7) it was originally assigned in the dataset. A more complex (and realistic) example is partial least squares regression. In ordinary least squares (OLS) regression, the objective is to construct a model on the data which has the smallest possible squared errors, or residuals. In partial least squares regression, we instead try to maximize covariance between the feature matrix $X$ and response matrix $y$. We do this by weighing the $y$ values in a certain mathematically-determined (and linear) way, and building a model based on those weighted values. To produce a prediction, at the end of the process we convert our weighted values back into the underlying response values, and produce those as the model results. Another example of an indirect model would be principal components regression, which applies PCA to a dataset before running OLS regression on it. Again, an inverse transform (literally: inverse_transform in sklearn) is applied to the results to transform them back into the "actual" response variables. These indirect models tend be useful when there groups of variables in the dataset which are heavily correlated or colinear (for which they are essentially an alternative to penalization techniques in the form of ridge, lasso, and elastic net regression), and datasets which are highly sparse (meaning they have more variables than records). Both sets of tools are useful. PCA and PLS regression are potentially better for understanding dataset effects (the variable coefficients are more meaningful), while penalization approaches are better for eliminating unimportant variables and, industry consensus seems to say, for modelling (at least given large enough datasets). The idea to PLS can be grasped by considering it a compromise between ordinary least squares and principal components analysis. PLS regression is the vector of the "PCR eclipse" upon which an ordinary least squares regression line has the largest projection. The following diagram makes this clear:  OLS tries to explain the most it can about the data with one fitted line through the space. PCA finds the vectors (two in this two-dimensional case) which maximize the explained variable in the dataset (I talk about PCA a ton in this earlier notebook). We could use the PCA output to model the dataset, but doing that would require understanding what the underlying vectors represent, which, depending on the dataset, could be hard and could hurt the interpretability of the model. PLS strikes a compromise position: it finds a regression fit to the original coefficients which lies in between the obvious OLS regression solution and the non-obvious PCA regression solution. Hence we can think of the PLS model coefficients as being OLS model coefficients, but "bumped" somewhat towards the variance-maximizing (PCA-determined) direction, and away from highly colinear variables. To demonstrate an application, let's pick a trivial small problem out of the Fashion MNIST dataset: is an item a shirt, or is an item a pair of shoes? Here are the  $r^2$ scores for the partial least squares and then ordinary least squares solutions, applied to the test (not train) set. Notice that, as expected, the fit for the PLS model is not as good. However the differences between the coefficients, shown below, are not too big. Overall the PLS API is basically exactly the same as the OLS one. 