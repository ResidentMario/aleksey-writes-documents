An entire model can be made parallel in one fell swoop by running the following code: DataParallel can be applied to any PyTorch module (any nn.Module object, which can vary from "a layer" to "a model"). Useful methods for understanding the device environment are on the torch.cuda path: Devices with the same major revision number are of the same core architecture. The major revision number is 7 for devices based on the Volta architecture, 6 for devices based on the Pascal architecture, 5 for devices based on the Maxwell architecture, 3 for devices based on the Kepler architecture, 2 for devices based on the Fermi architecture, and 1 for devices based on the Tesla architecture.   This discusion of this is: In the ring-allreduce algorithm, each of N nodes communicates with two of its peers 2*(N-1) times. During this communication, a node sends and receives chunks of the data buffer. In the first N-1 iterations, received values are added to the values in the node’s buffer. In the second N-1 iterations, received values replace the values held in the node’s buffer. Baidu’s paper suggests that this algorithm is bandwidth-optimal, meaning that if the buffer is large enough, it will optimally utilize the available network. In addition to being network-optimal, the allreduce approach is much easier to understand and adopt. Users utilize a Message Passing Interface (MPI) implementation such as Open MPI to launch all copies of the TensorFlow program. MPI then transparently sets up the distributed infrastructure necessary for workers to communicate with each other.  All the user needs to do is modify their program to average gradients using an allreduce() operation. This architecture essentially communicates gradient updates across the entire cluster using generation-demarkated network transfers between nodes. I can see how this could work for forward propogation, but this brief blog post doesn't discuss how the backpropagation is handled. 