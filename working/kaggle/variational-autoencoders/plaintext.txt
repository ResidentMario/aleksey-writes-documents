(note: these notes are based largely on the blog posts "Building autoencoders in keras" and "Variational autoencoders explained") The variational autoencoder is an advanced subtype of an autoencoder. An autoencoder is a type of neural architecture which compresses and then decompresses input, generating a sparsified representation of the input data in the middle and a "cleaner" (albeit fuzzier) representation of the input at the end. My notes on autoencoders are in another notebook. Autoencoders have many variations, like sequence encoders, which are relatively easy to understand. One variation which is more complicated but nevertheless interesting is the variational autoencoder, which is the subject of this notebook. The cliff notes version of what VAEs do is: VAEs, comparing to AE, compress probabilities instead of features. Despite that simple change, when AEs answer the question "how can we generalise data?", VAEs answer the question "how strong is a connection between two events? should we distribute error between the two events or they are completely independent?". Variational autoencoders are just vanilla autoencoders which impose one additional constraint on the encoded representations that they pass between the encoder and decoder units: that the distribution of the encoded representations, known as the latent vectors, has a roughly Gaussian distribution. For example suppose that we compress cat and dog images to a three-value vector. If we train an autoencoder and plot the latent vectors for each image in the dataset, we will get a point cloud in three-dimensional space. A vanilla autoencoder imposes no restriction on the size and shape of the point cloud. A variational autoencoder forces it to be unit Gaussian. To do that the variational autoencoder adapts the loss function used to train the neural network to be the sum (or perhaps some other formulation) of a standard loss function, e.g. mean squared loss or categorical cross-entropy, and a latent loss calculated on the latent vector point cloud. For the latent loss an information-theoretic calculation of probabilistic distribution divergence known as KL divergence is used. Minimizing KL divergence would be equivalent to setting every value in both the mean and std vector to 1. However this would fail to approximate the input data, and result in an extremely high MSE. Thus the trick to a VAE is to solve roughly for both simultaneously. The reward for doing all of this additional work is that whereas an autoencoder can only encode and decode data (oh, and lend its weights to model pretraining), a variational autoencoder can sample the input space, creating fake output similar to those found in the original inputs. This is because we can reconstruct a random image using the decoder sub-unit of the autoencoder by feeding it a random (normal) latent vector. We could not have done this with a normal autoencoder because the distribution of the latent vector is unbounded and unknown, and hence, unsampleable! The code that follows is taken from the blog post "Building autoencoders in keras" with minor adjustments. First we import the names, import the data, and set some parameters. Recall that to optimize a variational autoencoder you need to optimize the sum of two loss functions: a regular loss function like e.g. mean squared error and KL divergence, a loss function defined on the shape of the point distribution. The methodology for doing this isn't to create a custom loss function and stop there: it's to use a reparameterization trick. Instead of building an encoder that generates straight vector compresions of input data, we build one that generates two separate vectors: a mean vector and a standard deviation vector. In other words, on a per-record-value (or per-pixel) basis, the encoder is tasked with outputting an estimate of the region of value for that record value or pixel that it is confident in. Put another way, in a vanilla autoencoder we generate a point estimate for every input value whilst in a VAE we generate an entire distribution. KL divergence can be calculated directly and quickly using a closed-form solution using this mean and variance information. To calculate MSE, we sample from the distribution, generating a point estimate, and calculate loss relative to that point. This requires a complex (non-sequential model). We need a sparsifier that generates encoded representations, then two more dense layers on top of the sparsifier: one generating mean estimates and one generating variance estimates. A custom non-trainable lambda layer uses these distributions to compute a point estimate, ultimately transforming the input into a value in N-dimensional space (2, in this example). We could theoretically omit this additional complexity, but the advantage of having it is that it gives us an entry point for specifying our own random samples—just pass mean and estimate vectors to the z_mean and z_log_var layers. It's also much faster and more accurate to compute KL divergence this way than in a computationally approximated manner (which would necessitate large batches). The decoder model is comparatively simple. It just reverses the sparsifier. To instantiate the full autoencoder we use functional composition. In the code above the encoder is defined in such a way that it returns three things on output: (0) the mean vector (1) the variance vector (2) the point sample. The decoder uses the point sample, so we ask for that tensor output specifically—hence encoder(inputs)[2]. Here's a plot of the encoded representations of the various digits in the dataset, e.g. the values assigned to each type of integer (color-coded) by the latent vector: To generate our own random noisy digit we use the decoder, passing in either random Gaussian values for random values or region-specific values if we want examples of particular digits. The following plot is a grid search through the latent vector space (remember, our latent vectors in this example are two-dimensional) and it demonstrates how different points in the space correspond with different digits. The plot below should roughly mirror the one above, as it's just a different way of representing ~the same data. Fin. Note: this model stared failing when I tried to do a clean run due to inscruitable tensorflow errors. So annyoing. Just read the keras blog post instead. 