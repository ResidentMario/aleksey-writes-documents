In this notebook we will explore what kinds of chess openings tend to go together. Suppose, for example, that a user like to use the Sicilian Defense. Are they more or less likely to also be users of the King''s Gambit? The idea is that this should be an interesting application of recommendation. Before we go through with building a recommendation engine lets explore the feasibility of doing such a thing with the data that we have. We will need to bring a couple of new fields into our frame. opening_archetype is the overall "type" of a opening that is used. Since chess openings have been studied for centuries, all of the relatively popular ones have many and many well-studied variations, information on which is included in opening_name. We don't need this additional information, so let's pull it out. Now, if a player has only ever played a single game on the platform, then we do not learn anything about their opening preferences, because there is not enough data. Only players who have played N>=2 games are helpful to us. How many such players are there? How many games do those ~4500 players provide us with? We can think of our dataset as having ~4500 raters with ~30000 ratings amongst them. That seems like a decent enough sample size. But what about the strength of player preferences? The more strongly our chess players tend to use the same opening moves—that is, the less preference they express, and the more standard the strategy they follow—the less interesting any recommendations will be. So next, let's sense-check whether or there is significant playstyle variation between players. After doing some domain research, I decided to see what would happen if we pit players using the Sicilian Defense strategy (a standard defensive opening) against King's Gambit players (a standard offensive opening). Because of how commonly played the Sicilian Defense is, I refer to the first of these distributions as the Sicilian mean, because it's reasonably close to the mean of all games played. The King's Gambit, meanwhile, is an also common, but significantly more "scipy" move. Hopefully we will find that, in aggregate, players at least occassionally using these different strategies also use very different strategies overall. There are many differences to take note of. A King's Pawn Game is about 50% more likely for a King's Gambit player than it is for a Sicilian mean player. Another difference is that the English Defense is much less popular with the latter. Other differences are also apparent. Overall, there is decent evidence that recommendations built on chess openings would be useful. Overall, the variation in play that we found with this inspection seems significant. Now that we've gathered evidence that doing so is useful, we move on to building our actual recommendation engine. The simplest recommendation engine algorithm is collaborative filtering. In collaborative filtering, we can use one of two approaches to recommendation. Each one is an inverse of the other, and both are based on similarities between "stuff". In the first approach, users are measured for the similarity between their ratings using some normalized distance metric. Then a new recommendation for a user is generated based on the weighed popularity of the items that similar users like. Let's try this approach out. We already have users, but we haven't implemented ratings yet. Indeed, our dataset doesn't have so-called explicit ratings: our players don't "rate" chess openings on a five-star ranking system, for example. Instead, players are generating implicit ratings: they are choosing to use certain openings, or choosing not to use them, and those choices encode preferences, but not explicitly. To proceed, we need to convert these implicit ratings to explicit ones. Since we're not building any serious system but merely demonstrating what is possible, let's use a simple exponential threshold approach. If a user has played N games, and used the opening in question more than N / 4 times, then that opening is a five-star for them. More than N / 8 is a four-star, more than N / 16 is a three-star, and anything less than that is a two-star. Warning: this cells has not been vectorized and so takes about a minute-and-a-half to run. This gets us a result which is relatively similar to what you find out "in the wild": a distribution dominated by 5-star ratings (relevant XKCD). To keep going, we need to know how to measure how similar two users are. One of the simplest metrics for measuring this is cosine similarity. To compare the similarity between two items (or users!) we do: The math is a bit intimidating, but the fundamental concept is relatively straightforward. Each of the possible items (openings) is a plane in N-dimensional space. High ratings contribute a lot of strength to the vector in that direction, low ratings contribute less, and null ratings contribute none at all. By taking the cosine of the angle formed between two of these crazy N-dimensional vectors, we get a "distance" that's between -1 and 1 (between 0 and 1 if the ratings are strictly positive). sklearn comes with a built-in cosine similarity metric as a part of the metrics.pairwise.pairwise_distances function. We will use that to generate an item-item cosine similarity matrix. Now we have item_similarity, a big item-item similarity matrix that tells us how similar each of our 143 strategies is to every other strategy in our strategy set. To generate predictions for our users, we take the dot product between this matrix and the user's rating matrix, and divide that by the sum of the similarity columns (to normalize). This nets us our predictions. Which finally nets us our recommended openings! What openings does our simple classifier recommend most often? It's important to note here that we are excluding System, which seems to function as more or less a default option. A chess system is an opening strategy where a player follows a set methodology for developing their pieces irrespective of what their opponent does. This doesn't strike me as being a very interesting recommendation, so I've excluded it from the results. Note that we can also take the second-best recommendation and use that instead in those cases. This notebook demonstrated building a simple implicit ratings collaborative filtering recommendation engine. While the little engine we built isn't terrifically good, it does demonstrate the key concepts behind building a neat little tool. Perhaps now you should try building an off-the-shelp recommendation engine using the surprise package (link), also available here on Kaggle! Collaborative Filtering Similar Notebooks 