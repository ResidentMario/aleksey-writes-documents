The API has two data namespacing APIs. The low-level one is contexts. Contexts are globs rooted at filepaths. For example, you start at /a/b/ on the filesystem, and that's your data context; from that context you can retrieve datasets /a/b/c/d, /a/b/e, etc. Contexts have a list_datasets() API: Why use contexts? Because they have situational awareness of the data hierarchy. They look like data packages to me however. Most users are going to want to use the pandas-like reader API: An immediate question for later, since this is a superclass on pandas.DataFrame compatibility with geopandas.GeoDataFrame seems questionable? The output to JSON is great. Look at all of the parameters in this docstring! Beautiful, and demonstrative of a mature library. Let's look at some schema assertions that are harder to test... How would we assert something about the time-series schema of this dataset? Let's get back to that later. For now, on to statistical tests! All of the statistical measures use partitions and weights to do their work. This differs slightly between the continuous and discrete case. In the continuous case bins are used. In the discrete case, bins are per-category. E.g.: KL divergence is an information-theoretic measurement of the difference between two probability distributions. It is non-symmetric. It was also used by the talos guy in his post. In order to actually fail a test, we would need to either construct a partition object by hand, or test a dataset that has skewed from some past configuration. Understanding these statistical tests is non-trivial. All of the other tests you can pretty much write and run off with, but to get the statistical tests you need to fine-tune (1) your understanding of what characteristics of the dataset constitute failure and (2) your understanding of the algorithms themselves. If I write about ge specifically I'm going to want to start by artificially generating some datasets and trialing them out via ge statistical tests. They need more visibility anyway. :) 