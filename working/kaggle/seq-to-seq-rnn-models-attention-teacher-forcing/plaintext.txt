A sequence-to-sequence (sometimes called seq-to-seq) model is a particular type of recurrent network that takes one sequence as input, and outputs a different sequence as output. This is is a common task; it is e.g. what you do when you want to train a machine translation algorithm, which takes a list of tokens in one language as input, and provides a list of tokens in a different language as output. This use case in fact is what motivated much of the research on this particular architecture. Seq-to-seq models are composed of two pieces: an encoder module that reads in and builds an intermediate representation of the token stream, and a decoder module that reads in this intermediate representation and turns it into a token stream in a different language. By separating the solving of input normalization from output generation based on that normalized form, seq-to-seq models decorrelate the length of the output from the length of the input. This is important in the machine translation domain, where input sentences and output sentences may have wildly different lengths and structures. The encoder and decoder modules have the same basic layout: Input --> Embedding --> LSTM. A decoder module has a TimeDistributed(Dense) layer as well as its last layer. The encoder module takes the tokenized input sequence (e.g. X) as input and returns an information vector as output. The decoder module takes a bit-shifted version of the output sequence (e.g. y) as input at training time, e.g. with the first token position's value replaced with the null vector and everything else moved over one index position, and returns the target sequence. It takes masked null vectors (e.g. all-zero vectors with mask_zero=True) at prediction time, and returns the target sequence.  The encoder and decoder modules are virtually independent, except in one linkage point: the final output weights of the encoder LSTM layer are the first two input weights of the decoder LSTM layer (it is provided as input to the first two layers, not just the first one layer, because the first time-step is training on the clear with masking, so it doesn't do anything). This technique is known as teacher forcing. Teacher forcing is the use of actual or expected output from the training dataset at the current time step $y(t)$ as input in the next time step $X(t+1)$, rather than the output generated by the network. In the case of our model, the network is started off with a null token, based off of which it is expected to generate the next token. That token is then used as an input for the generation of the token after that, and that token is applied towards the token after that, and so on down the line, recursively. The decoder module is trained using teacher forcing target sequence input in addition to user token input, but the predictions occurs with every one of these connections that proceeds after tokens that have already been found completely masked out, e.g. relying just on the combination of user token input and the list of tokens discovered thus far. In this way, teacher forcing allows the model to use its entire precedent of tokens already generated in generatating the next fresh token. A side effect of this construction is that performing a single translation task requires running the model as many times as there are token spaces. Teacher forcing as it is applied to the seq-to-seq model creates the constraint that backpropogation has to generate weights on the last time-step of the encoder module LSTM layer that are equal to the weights on the first "actual" (non-masked) time-step of the decoder module LSTM layer. Even after much thought, I wasn't able to reconstruct what this constraint means from the perspective of the data. It doesn't actually place much of a restriction on the model's respective training space, since neural networks are general enough to be able to correct any weirdness this restriction creates in the hidden vectors. It mostly seems to function as a way to introduce the advantages of teacher forcing into the model, namely (reputedly) model skill and model stability, as described in this machinelearningmastery blog post on the subject. From the perspective of training, the linkage between the encoder and decoder module forces backpropogation to solve for the encoder and decoder simultaneously. Passing the encoder weights to the decoder weights is conceptually different from passing the encoder output to the decoder input, but computationally this is nevertheless a simultaneity restriction you can be trained on. An example model follows. This model definition was taken from the following blog post: "English to Katakana using Sequence to Sequence in Keras". The invention of LSTMs revolutionized RNNs because they solved fundamental problems that vanilla RNNs had with utilizing information embedded in the sequence that is contained in tokens that are far away from one another. Attention was the next big "thing" after LSTMs, and caused the next largest jump in RNN performance on real-world problems. The bottleneck that attention addresses is the fact that each time-step in an LSTM only gets to work with the output of the previous time-step. Attention adds an additional intermediary step: each LSTM steps additionally gets input from each previous LSTM time-step as an input. There are several different ways of implementing this additional data flow. The most conceptually intuitive implementation is a weighted sum of the previous layers' data. This is what is meant by attention: the layer learns what previous states to pay attention to, and which previous states to ignore. With attention, it's even possible for a layer to completely ignore the layer immediately previous to it in favor of layers earlier in the time sequence. Hence the name "attention". Presumably because "attention" is not a standardized layer, Keras does not provide a prebuilt Attention layer. If you want to incorporate attention into your network, you instead need to go ahead and define it yourself. The following code is a copy of the same network we defined above, with the addition of the attention module. This attention module was implemented by the author of this model in a follow-up blog post: "Attention-based sequence-to-sequence in keras". Here's another example implementation of an Attention layer, taken from the notebook keras baseline model + attention 5-fold, which was a popular baseline model in one of the Jigsaw toxic comments competitions: An RNN model need not be seq-to-seq to have an attention layer; attention has been shown to produce improved results in all kinds of RNN models. Seq-to-seq RNN models are simply the model archetype for which attention has first developed. This seq-to-seq model is an especially simple one provided in the Keras documentation as an example implementation. Teacher forcing improves on the stability of a machine learning model that is applied recursively, but it doesn't solve all of the problems with it completely. It still has issues generalizing to target sequences that differ significantly from those which were seen in training. This occurs because recursively feeding in prior token input is a greedy operation; taking the optimally scored output at a particular time point lead to a local optimum instead of a global one (in terms of the total fit of the generated output sentence). There are a couple of techniques that are used to correct this effect. The first is beam search. In beam search, instead of taking the highest-scored token at each time step (argmax in the code above), we take the top n tokens at each time step. We then generate the top n probabilities of the next tokens (so n * n possibilities), and keep only the best-scoring n of them. This repeats across time steps. Greedy search is just beam search with a beam size of 1. Benchmark problems in machine translation are solved with beam sizes between 5 and 10. Optimal search is not possible because it is exponentially hard: e.g. it requires n_dict ^ seq_length operations. The machinelearning mastery blog has a post dedicated to the subject of beam search titled "How to Implement a Beam Search Decoder for Natural Language Processing". It provides the following toy implementation: Another, far more complicated form of fine-tuning not covered by a code sample here is curriculum learning. This is the subject of a future notebook however. 