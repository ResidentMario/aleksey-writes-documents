The difference between a layer 4 and a layer 7 load balanced came up in the technical discussion about Spell's infrastructure. Ian stated that we used to use ELB (layer 4) and now we use ALB (layer 7) on AWS because ALB is more flexible. What does that mean? Let's find out. Load balancing is the distribution of compute and/or network load between multiple machines. A simple application with no load balancing might accept, manage, and yield all requests on a single machine. However, when you do this, it becomes very easy to saturate your instance, and very hard to scale it (requiring operational downtime). An application with load balancing will instead partitioning incoming work across multiple machines in some way. This means that you can easily add and remove machiens as needed. Bam, scalability! Load balancing is such a common, useful, and easy design pattern that it's borderline required in Kubernetes (where it's best practice to use a deployment, which is what configures the load balancing). kube-proxy and nginx load balancing are two examples of L4 load balancing. They both (the former via IP table rules, the latter via entrypoint indirection in the frontend) take an incoming request and use something ranging from random selection to user-specified business rules to select which service endpoint to pass that connection through to. In other words, L4 load balancing is TCP level load balancing (on AWS this is usually refered to as "ELB" for "Elastic Load Balancer"). This means: Once a service is assigned, assuming the connection is keep-alive (which, given HTTPS 2.0 speed-ups keepalive gives you, is a good guess), the traffic will continue to be routed to that port. In that case the minimum unit of routing is one entire server-client connection. This is pretty inflexible, and you can see how with certain workloads this is not good enough. In Spell's case we were load balancing the connection from the user instance to the logs storage machines on our side. Log traffic can potentially be very volumnious, so you can see how   smarter, application-aware handling to deal with noisy neighbors would be necessary. Here's the UI for specifying an L4 load balancer in Rancher:  Rancher does something else interesting here. They use a service I've just discovered, http://xip.io/, to deal with DNS resolution. Basically, DNS resolution on the public Internet is very good (because it's consumer-facing) whilst DNS resolution on the local network is very bad (it's a developer-only interface between your router and your computer). I've struggled with this before: when I was tried and failed to use mDNS to set up a Raspberry Pi on my local network at a fixed address. http://xip.io/ is a public website which resolves subdomains to IP addresses. For example 10.0.0.1.xip.io resolves to 10.0.0.1. This means that you can use the public internet's DNS resolution (which is good!) to serve your IP addresses in a platform-agnostic way. They use this website to do DNS resolution, as an alternative to configuring DNS rules yourself, which is a clever hack. L7 is application load balancing, or ALB. This means a set of application-controlled rules are used to determine connection routing. For example, a simple way to do ALB would be to have each service periodically report its current usage state to the load balancer. The load balancer uses this information to weigh where to route incoming requests. If handing off the entire connection is too much overhead, an alternative is to have the load balancer itself endpoint the connection (so to the client, it looks like the load balancer is the endpoint to their connection), partition the connection on the level of an API request, and apply its routing rules on a request-by-request basis. And if you saturate the load balancer service, simply stack load balancers. Heirarchical load balancing. :) 