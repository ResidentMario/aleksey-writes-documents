These notes are based on the following blog posts: https://medium.com/@Alibaba_Cloud/elasticsearch-distributed-consistency-principles-analysis-3-data-a98cc436bc6b ElasticSearch is based on the Lucene information retrieval library. ES makes the decision to (asynchronously?) write master metadata receipts to disk on data nodes, in order to provide integrity in case there is only a single master node (so that master meta may still be recovered). Cluster updates are ensured to be well-ordered through true total ordering: e.g. many threads may send cluster updates to the master, but master will linearize these cluste updates in a single sequential queue. Personal comment: this is a pretty serious bandwidth restriction! 2PC (two phase commit) is used to garauntee that state updates from master are rollback-safe and recoverable, e.g. in case of a network partition. First, master sends the cluster state transcript to all nodes, and waits for at least a majority quorum to ACK. It only then sends an execute command. What's interesting is that the primary blocks until all nodes explicitly success or fail. An asychronous model was used in the past, but had the issues you'd expect it to have (data loss), so they swapped to a strongly consistent sharding model. There is weak read-your-own-write consistency. If a node fails to replicate, this is reported by the primary shard node to the master. The master then removes that node from the replica list (potentially replacing it with another node), and sends this updated status as a new cluster state message. Once that message reaches the node that a user is trying to read on, that node knows not to use the legacy shard. However, until then, the user is still able to read legacy data. This is nevertheless weakly consistent because normal use of ES requires a page refresh anyway. In normal operations, the state update will probably outrace the network request...though this can change if the system is stressed. ES makes the unusual decision to write to Lucene in-memory before writing to the transaction log. This is because messages may fail validation in Lucene, which would cause inconsistency in the transaction log, which now requires a corrective log item or a delete op, both of which suck in what's supposed to be an append-only log. 