I've written some previous notes on the design of the PyTorch API in general. This notebook contains some deeper notes on PyTorch autograd specifically, because understanding how this works at a slightly deeper level is valuable. When working with a variable, it was possible to get a view of the underlying tensor using the .data accessor. The tensor retrieved is a view: it has requires_grad=False and is not attached to the computational graph that its Variable is attached to. This is necessary because arbitrary operations on a tensor are not supported by autogradâ€”only supported operations defined by the PyTorch API are. After Variable was deprecated, the properties of the Tensor object were changed to those formerly assigned on Variable. The data accessor was retained, for backwards compatibility, with much the same behavior: it returns a view on the tensor that has requires_grad=False and is detached from the computational graph. However, actually using this attribute is considered an anti-pattern. You should be using detach() instead. More on that later. The rules for is_leaf are more complicated. All Tensors that have requires_grad set to False will be leaf tensors. Tensors that have requires_grad=True will be leaf tensors IFF they were created by the user (and thus have grad_fn=None). Only leaf nodes accumulate gradients on the grad attribute. Essentially what the PyTorch API is doing is that it's assuming that you will initialize your weights tensors directly in your code, e.g. you will not make them dependent on prior computations (because there is no reason to do that). As long as you do this, the leaf rules will have the behavior that you expect: all of the tensors that you initialize (via torch.tensor init) will be subject to backpropogation. Unlike requires_grad, you can escape this pattern if you wish, by using the retain_graph method on a non-leaf tensor. This will then also let you set requires_grad, if you want.  Running backward calls the MulBackward operator with an initial value of 1. The MulBackward operator has an internal list of prior operations I needs to run. MulBackwards computes the derivative of C w.r.t. A and B: $$\frac{d}{da}(c) = \frac{d}{da}\left[a * 3\right] = 3$$
$$\frac{d}{db}(c) = \frac{d}{db}\left[a * 3\right] = 0$$ Notice that the derivative w.r.t. B is zero because B is constant because requires_grad=False. The other value, 3, is passed to an AccumulateGrad handler for A. A's grad attribute then is updated to this value. One interesting detail of backpropogration is that under certain operations, the value of a constant tensor doesn't matter, but under other operations, it does. For example suppose a gradient-requiring tensor B and a gradient-nonrequiring tensor A. If we have C = A + B, the value of A doesn't matter because the constant factor will always be eliminated. If C = AB, then it does matter, because its value will be. PyTorch prevents in-place operations (like A += 1) from causing users to accidentally incorrectly accumulating gradients they've already computed using an internal _version attribute, which is incremented every time a tensor is operated on. backwards checks this value against an expected version stored in the backwards graph and raises if it notices a version mismatch. 