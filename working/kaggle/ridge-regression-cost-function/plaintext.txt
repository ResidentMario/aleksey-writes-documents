In an earlier notebook we proved the correctness of the ridge regression algorithm (and provided an implementation). However this proof is somewhat unsatisfactory, in that it doesn't immediately give us intuition on why subtracting that random term from OLS creates the effect we want. One way to get more intuition is by considering ridge regression as a problem in gradient descent. Recall that when we do ridge regression, we're trying to minimize the following cost function: $\lambda$ is a value given by user input (or by a grid search, or whatever). Note that here we use $\lambda$, scikit-learn uses $\alpha$. $\beta$ is a vector of weights, $\beta_i$, assigned to each of the features to produce a finished model. Rewriting in terms of sums instead of matrices: In each step of a gradient search we move the model parameters in the direction of the gradient (we know that such a process is asymptotically convergent because the parameter surface is convex because ridge regression is OLS slightly modified, and we know OLS is convex, so...). We move by an amount dependent on the gradient of the cost function. Since this model has $M$ different parameters, this means we have a gradient with $M$ different partial derivative components. Each one of these will look like: Each iteration of a gradient-descent-determined parameter coefficient (weight) is known as an "update rule". The update rule is a function of the previous weight, the learning speed $\eta$, and the gradient (obviously). The gradient descent update rule looks like: Which simplifies to: But the right hand side of this equation is just the ordinary least squares update rule! It's something we found in gradient descent notebook. Hence the difference between ridge and OLS is all in the first term, $(1-2\lambda\eta)\beta_j^t$. Hence, ridge regression is equivalent to reducing the weight $\beta_j$ by a factor of this multiple of $\lambda$ and $\eta$, then applying the same update rule used by OLS. Given the numbers involved, this term is going to be some number smaller than 1, meaning the whole term is going to be some decimel. The larger $\lambda$, the smaller the decimal; and the more the weight gets cut every iteration! The OLS update rule will come up with a bigger gradient than it would if it was acting alone, because the ridge part makes everything "further away" from the truth. This pushes the parameter coefficients back up; which is what we wanted all along, and is (recall) the main thing that ridge regression "does". 