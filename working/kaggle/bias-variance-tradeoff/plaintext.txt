The bias-variance tradeoff is a fundamental property of machine learning algorithms. Bias is the tendancy of an estimator to pick a model for the data that is not structurally correct. A biased estimator is one that makes incorrect assumptions on the model level about the dataset. For example, suppose that we use a linear regression model on a cubic function. This model will be biased: it will structurally underestimate the true values in the dataset, always, no matter how many points we use. Given points $x$, a true value $f(x)$, and a model $\hat{f}(x)$, bias can be expressed mathematically as: Where $E[\cdot]$ is the expected value function (e.g. the mean value). The key to understanding bias is to understand that, given enough "perfect" data points sampled from a compatible distribution which has no errors or variance in it (such as a line, in the case of linear regression), an unbiased model will fit every point exactly correctly. Bias is also known as underfitting. Once we've selected a model bias becomes something that we want to, within reason, reduce. Variance is error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting). It can expressed mathematically as: Variance is slightly more fundamental than bias to understanding ML, and I covered it here. Bias and variance are linked with one another. Recall that mean squared error or MSE (covered in Model Fit Metrics) measures the square of the average amount of error made by our model. Let $f(x)=y$ be the true target value of a point, and let $\hat{f}(x)=\hat{y}$ be its model-predicted value. Then we may write: We will use the follow lemma: Lemma: $E[(X-E[X])^2] = E[X^2] - E[X]^2$ Proof: We use this fact in the following proof. This result shows that squared estimator error is the sum of the variance of the estimator (how poorly it generalizes; its level of overfitting), the bias (how structurally underfitted it is), and an irreductible error in the underlying dataset, $\varepsilon$. The takeaway from all this is that overall error in a model is the combination of these three components. The fact that purposefully biased models like elastic net regression (itself a combination of L1 and L2 norms) outperform unbiased models like linear regression in mean squared error at least some of the time stems from the fact that it's oftentimes possible to increase the bias of a model in a way that strongly decreases the variance, resulting in a better model overall. For example, a good blog post on the topic includes the following graphic demonstrating training a polynomial regressor:  In this case bias fell off a cliff on the third iteration, all the way to almost zero, because the underlying data was a third-degree polynomial. Variance on the other hand climbed every iteration, and as a result, the best model (according to MSE) occured at degree three. 