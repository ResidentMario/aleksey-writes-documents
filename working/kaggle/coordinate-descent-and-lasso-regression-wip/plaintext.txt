I need a complete reference on implementing this algorithm, and haven't been able to find a good once. Hence this is currently stalled out. Some reference: Lasso regression is a modification on ordinary least squares regression which introduces an additional L1 penalty in the regression cost function. Because this penalty involves an absolute value, the resulting cost function is impossibe to solve analytically or using gradient descent. Instead lasso regression can be solved using coordinate descent. In coordinate descent, instead of taking steps in the direction of a gradient, we loop over the variables and take steps in the direction which reduces the cost incurred by the variable in question. Coordinate descent is a more time-consuming but also more generalizable technique because it can be extended to functions that do not have a derivative (and therefore, do not have a gradient). Like gradient descent, coordinate descent works best when the cost surface is convex (or psuedo-convex, if no inverse exists thanks to "bad" points like absolute value minima). The chief difference between gradient descent and an ideal-ish coordinate descent algorithm is that the former will take N steps, once for each directional vector, for every 1 step taken by gradient descent (you can see an example of what that looks like here). In this notebook, we'll use coordinate descent to implement lasso regression. This is the implementation that is used most often. For example, it is the default lasso regression implementation used in scikit-learn. In a previous notebook I implemented univariate lasso regression. This notebook will now extend this result to the multivariate case. The reason that we're able to do coordinate descent even without differenciability is via the concept of subgradients and the L1 soft-threshold (the technique being soft thresholding). The derivation of the L1 soft threshold is in the previous notebook. Let's look at the math. The lasso regression cost function is: Where $w$ is the vector of linear regression coefficients (the weights; previously $\beta$) and $h_j(x_i)$ is the relevant slice of the observation matrix (previously $X_j$). The terms are bouncing around a bit because I'm using a different source this time. Our objective is to find a partial derivative look-alike (a subgradient) for this entire function for some weight $w_i$. We showed in a previous notebook what the partial derivative of the RSS term is: We can call the two interior terms $p_j$ and $z_j$ respectively. Then this equation simplifies to: In the previous notebook we derived the soft thresholds for the second term, the L1 term. They were $-\lambda$ if $w_j < 0$, $[-\lambda, \lambda]$ if $_wj = 0$, and $\lambda$ if $w_j > 0$. The subgradient of the lasso cost function at large is just the one plus the other. Since the solution surface is concave, a minimum (optimal) cost will exist where the subgradient approaches 0. Hence we may set the subgradient of the whole function each to 0, and examine the three cases. Case 1: $w_j < 0$. $p_j < \frac{\lambda}{2}$. The L1 term subgradient is therefore $\lambda$. Hence we must solve: Case 2: $w_j = 0$. $-\frac{\lambda}{2} \leq p_j \leq \frac{\lambda}{2}$. The L1 term subgradient is anywhere in the interval $[-\lambda, \lambda]$. This is the degenerate case. The L1 choice in the interval which minimizes cost is obviously 0. The zero weight multiplied by the other terms in the RSS term will compute to 0 as well. Hence $\hat{w}_j = 0$. The specificity of this case is the reason that lasso regression acts as a feature selector! Case 3: $w_j > 0$. $p_j > \frac{\lambda}{2}$ The sign of the $\lambda$ included in the cost is flipped and we get: This is what we need! A sketch of the algorithm is as follows: The "while not converged" condition is tricky. We won't treat it in our implementation, we'll just specify a number of iterations manually. 