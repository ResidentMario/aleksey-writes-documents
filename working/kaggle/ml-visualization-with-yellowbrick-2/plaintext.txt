... For this exploration we'll use the tennis odds dataset. For regression we'll explore how well bookies predict one another's odds, picking on betrally as the target variable (a case when one bookie is systematically lowballing or highballing would be an arbitrage opportunity!). For classification we'll classify whether or not the player winning the match gets predicted from the betting odds. An important assumption behind linear regression is that of homoskedaity: that the error, or residuals, between the predicted values and the actual values is random. Non-random error indicates that there is a pattern to the dataset that the linear model is not sufficiently complex to capture, for example, you are trying to apply linear regression to a pattern which is occurring in a polynomial feature space. Data that does not exhibit randomly distributed residuals is thus a poor fit for linear regression (though it can still be modeled this way, in a pinch). This plot type is a neat and tidy built-in for plotting this yourself. PredictionError summarizes the level of fit of your plot. It's similar to a pre-configured seaborn jointplot (see here for that). In this case bookies predict each other really, really, really well; the amount of error in the model is so tiny that the $R^2$ score is 1 to three decimal places! Here's a more illustrative example of what this will look like on well-posed problems, taken from the yb docs:  The identity fit is meaningful because its $R^2$ score is exactly 0 (read more on that here). The last visualization in this chunk of yellowbrick is an alpha selection utility. $\alpha$ is a parameter controlling how much regularization (L1 regularization for Lasso, L2 for Ridge) the given model is given, and is used to tune a model that overfitting. I demonstrate cross validation in this past notebook. This method requries the version of the linear regression models with built-in cross validation—RidgeCV for ridge regression, LassoCV for lasso regression, and ElasticNetCV for elastic net. In this case we see that the best $\alpha$ value is 2.835. Notice the y-axis, however. Regularization is only affecting the overall fit in the fourth decimal place! For what it is this is a pretty performance chart which solves a need. However, I am again left with the desire to go further than this. How about tuning two hyperparameters simultaneously? Three? Why does it have to be $\alpha$—can't I use this to e.g. visualize myself tuning l1_ratio in an ElasticNet instead? Hmm. A classic of the genre. The confusion matrix simply states the number of times each of the possible configurations of actual and predicted classes occurred. By inspecting these counts we can see what classes get "confused" for one another. In the binary case, as here, the boxes translate directly to the True Positive, False Positive, True Negative, and False Negative metrics. For more on this topic see this notebook. This visualization is just a convenient wrapper to seaborn.heatmap. This one outputs the performance of the data on a per-class basis for three of the most common and important, and inter-related, classification measurements: precision (TP / (TP + FP)), recall (TP / (TP + FN)), and the F1 score (the precision-recall harmonic mean). Again this notebook for more details. It's interesting that our model performs marginally better on Player 2 wins than on Player 1 wins. In other words it predicts 0 with more accuracy than it predicts 1. A classic of the genre. ROCAUC is probably the most heavily used more-complex-than-a-matrix classification report out there, and it's fantastic to have this available as such a convenient built-in! Again this notebook for more details, if you're not familair. Looks like the AUC for both P1 and P2 classes is the same, and the curves are almost the same as well. This method simply plots the number of times either class is predicted by the model. I don't find to be very useful, as you're not really saving yourself that much effort with this one—it's really just a simple bar chart... This chart is similarly problematic. Not only is it easy to generate yourself, I also think that its current layout could be vastly improved! I find it very confusing, wouldn't use it. Precision and recall are fundamentally related to one another, with an increase in one coming at the cost of a decrease in the other (read here for a quick discussion on why this is and why it matters). This is a very useful visualization, but takes a bit of thought to understand. To understand it, imagine that we are building a search engine, and that this classifier is being used to determine whether or not some set of records is relevant or not relevant to the given search query. The y-axis is how the given metric (precision or recall) scores (1 being best performance and 0 worst). The x-axis is the percentage of records withheld. Thus 0 in the x-axis is the case when 100% of all records are being returned; in this case we will trivially have a recall score of 1, as if we return everything, every relevant result will be in the user's hands. Simultaneously, precision will be the balance of the classes. Close to 1 on the x-axis corresponds with very few records being returned; in this case we will be extremely good at returning only relevant records, resulting in a precision score of 1, but our recall will be near 0 because very few of the total relevant records are returned. At each step on the curve in between, the next remaining highest-probability point is chosen, thus we are choosing sure winners at first, and sure losers later on. The shape of the lines tells us something about the dataset overall. The high slope to precision at the edge of the graph tells us that odds extremely heavily in favor of one player or another at the betting booth, once interpreted by our Naive Bayes classifier, do not actually correspond with better chance the player will win. The curve incredibly quickly stabilizes on a slightly-better-than-random precision score of 0.65, and only dips to 0.5 at the other extreme end. Winner classification seems to be about that performant across all matches (remember: not that many Federer versus nobody matches in this dataset!). The recall curve tells a similar story. The queue rate is just 1 - precision. Very useful! The adventure continues in https://www.kaggle.com/residentmario/ml-visualization-with-yellowbrick-3/ 