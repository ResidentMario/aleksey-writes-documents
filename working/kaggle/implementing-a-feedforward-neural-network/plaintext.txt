In this notebook I implement a simple linear feedforward neural network. A feedforward neural network is one which doesn't include, or allow, any loopbacks (a neural network with loopbacks would be classified, at a minimum, a recurrant neural network). In other words, nodes are organized into sequential layers, and nodes on an individual layer take input from the layer before and pass it to some layer after. A feedforward neural network is the simplest model more complicated than the trivial perceptron, and thus the second simplest neural network overall. It is distinguished by the presence of at least one hidden layer of neurons between the input layer and the output layer. A perceptron, by contrast, has no hidden layers. In this context I am using the word "simple" to refer to a neural network with just one layer. A linear neural network is one which uses linear activation functions. Activation functions are weighted functors which are applied to each input to the model. The simplest possible functor is a linear one, that is, one which just multiplies the input by a weight. The weight is randomized at the beginning of the training process, but is adapatively refined and hopefully made better and better as the training process continues. Linear activators are simple to understand, but not very practically useful, as they are still a linear transform on the data and cannot be used to achieve results better than those typical of regression. The binary activation function used in the percetron notebook (here) is arguably even more simple, however. To solve the feedforward neural network we need to perform backpropogation. Backpropogation is automatic differenciation in reverse, and is a procedure that follows the gradient of the cost function down to its lowest point (which is hopefully the global cost minimum). There are technically other ways of training the model, like e.g. randomized trials, but backpropogation is easily the best approach and the only one used in production. 