In some classification tasks we use classes that are not mutually exclusive. In other words, given two possible classes, it is possible for both clases to be assigned, and it is possible for neither class to be assigned. Thus this problem is not strictly a multi-classification task. A multi-classification task assumes that exactly one label is assigned out of all possible labels; that is, either the record belongs to the {0, 1} class, or it belongs to the {1, 0} class. This problem on the other hand also allows for {1, 1} and {0, 0} outputs (both evidenced in the short snippet above). That makes this problem what is known as a multi-label task. I dicussed multi-class and multi-label schemes in a previous, sklearn focused notebook. This notebook is a breif continuation on this subject specific to neural networks. It was written on something that tripped me up while I was working on a bigger model. To start with, here's the example data we'll be working with: The X features consist of a cluster with three easily distinguishable class clusters. The output y consists of two columns: one which is true if the record is assigned to class 1, and one which is true if the record is assigned to either class 1 or 2. This is a multi-label classification task because it's possible to have any of {0, 1}, {1, 0}, {0, 0}, {1, 1} appear in the output. We'll start with the following model, which works: Now here's a model that doesn't work: You can see here that this did not work. Neural networks intrinsically support multi-label tasks in their design. Each possible class or label is just another node in the output layer of the net. However, not all neural network output layer activation functions work with multi-label tasks. An example of an activation function which does not work with multi-label tasks is the softmax activation function. A softmax normalizes the value it assigns to a particular class against the value assigned to each other class also being predicted. In other words, if we have a three-class problem, the softmax output for class 1 will take into account the outputs for classes 2 and 3. In order to do this effectively, softmax makes the assumption that the set of classes being considered are mutually exclusive. By comparingclf.predict(X) to y in the code cells above, you can see the effect this has on our outputs. Records with mutually exclusive classes are predicted correctly. Records where multiple classes are true (e.g. {1, 1}) split the confidence of the prediction between them. And in records where both classes are not true ({0, 0}), the classifier will just go ahead and pick a winner anyway! With a little bit of post-prediction manipulation we could deal with the {1, 1} issue, but the {0, 0} issue is unfixable. The lesson here is to be very careful about whether your problem is multi-class or multi-label. The assumption of mutual exclusivity is an important component of why softmax is so effective for classification tasks, as it is essentially an additional piece of information the network can use during the optimization step, one which other popular activation functions do not get. The sigmoid activation function, which we used in the immediately previous model, doesn't have this problem. That's because the sigmoid activation function evaluates each class separately. It has no problem assigning low values to records with double falses, and high values to records with double trues. So in summary: do not use softmax with multi-label problems! 