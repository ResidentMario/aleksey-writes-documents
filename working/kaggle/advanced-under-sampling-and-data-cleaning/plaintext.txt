In two previous notebooks I introduced undersampling and oversampling and then further discussed advanced oversampling techniques. This notebook is the third in this series, covering advanced undersampling techniques. The techniques covered in these three notebooks comprise the whole of the ones available in the imblearn sklearn-contrib module. To recap, sampling is a general-purpose preprocessing technique for turning imbalanced datasets into balanced ones, and thus, imbalanced machine learning models into balanced ones. There are other approaches to achieving this end, like changing the algorithm cost function, but sampling is a general-purpose technique that works reasonably well everywhere. The simplest, most explainable, and oftentime most effective techniques for performing sampling are random under- and over- sampling. These two sampling methodologies have broadly simpler characteristics. More complex adaptations of over-sampling exist in the form of the SMOTE and ADASYN algorithms and variants. For some datasets, these approaches may have better performance. Of course there are advanced under-sampling techniques as well. imblearn divides these into two broad classes. The simple RandomUnderSampler that we saw in the first notebook is an example of prototype selection: it attempts to select points which are "representative prototypes" for the cluster at large. Sampling the points randomly is just the most naive way of achieving this goal. A competing idea is prototype generation. Samplers which are prototype generation algorithms will not select exact points out of the dataset, but will instead generate new points somehow representative of existing ones. Additionally this notebook will look at some data cleaning algorithms provided in imblearn. We'll use the same synthetic dataset as in the two previous sections. The ClusterCentroids algorithm is the one example of a prototype generation algorithm provided in imblearn. This algorithm performs K-means clustering on the majority classes, then creates new points which are averages of the coordinates of the generated clusters. Here it is applied to our example dataset: ClusterCentroids comes with the implicit expectation that the points being sampled belong to moderately well-defined clusters. In other words, this algorithm will not capture outlier points that are not sufficiently dense, as these points will be pulled back towards the inner points by the cluster mean operation. As a result the clusters generated by ClusterCentroids will thus be marginally tighter and better-defined than the ones generated by RandomUnderSampler. That is not to say that this algorithm will completely eliminate outliers; we can see in the plot above that there are outlier points from the white and red classes represented in the blue cluster. Overall I find the idea behind ClusterCentroids to be quite appealing. NearMiss is a prototype selection undersampling technique which creates significantly more radical class clusters. There are three modes available. The imblearn documentation includes a section of this algorithm's mathematical formulation, which I won't copy (you can read it yourself here). But basically, all three versions of this algorithm specifically select points that are near the minority class. They have pretty radical effects. This is version 1 of NearMiss. We can see that the points sampled out of the white and blue classes are those which are nearest to minority class (red) outlier points in their respective point clouds. Version 2 of NearMiss is similar. Version 3 is different. This version will pick balanced clusters of points that are closest to outlier points from other classes, instead of building giant blob(s) around one or a handful of such points. It's mildly interesting that these algorithms exist; perhaps they could be adapted for some kind of data visualization exploring where the difficult points are? But I can't see myself using any of them anytime soon. Undersampling may be used to clean data—that is, to remove outlier points. Machine learning algorithms which are strongly affected by the presence of outliers, regression fo example, can see significant improvements from this type of cleaning being performed before model training. To deal with this problem, the current recommendation in practice is to use regularization; that is, to modify the cost of the machine learning algorithm being trained so as to reduce its tendancy to overfit (and hence, to pick out outlier points). This approach has proven historically to be the most robust and least prone to errors than the alternative in this section—undersampling with the purpose of data cleaning. It is generally not recommended to clean your data before generating models on it, though you certainly can. Undersample data cleaning has great value as a separate preprocessing technique. Most of the time it comes in handy during the process of de-junking the dataset, that is, dealing with datasets which have many spurious observations in them, due to the way they were generated, collected, etectera. If these points are considered "out-of-band", undersampling may be used to find them (you will almost certainly want to look through at least some of these cases by hand, to understand what's going on) and then get rid of them. Tomek links are points in the dataset whose nearest neighbor is a member of a different class. This includes outlier points embedded in a point cloud from another class, and boundary points in regions where it is unclear which of two or more classes is dominant. The TomekLinks undersampler can be used to remove these points. Here's an illustration of what this looks like, from their documentation: By default it will remove only links from majority classes, but I suggest passing ratio='all' so that all of the classes will be considered and treated. Here's the result of an application to our sample dataset. It doesn't look much different overall. But if you look at the number of points we've removed 42 outlier points. In the following plot I show which points from the minority class were removed, to better illustrate what kinds of points get cleaned from the dataset: The TomekLinks algorithm has the advantage that it is quite conservative; that is, it is good at finding and removing points we are quite certain are outliers (though it pays to inspect the result yourself afterwards, just to be sure the right things were removed). It's not easily tunable however; we can't tell the algorithm to be more or less aggressive. Also Tomek's Links retains some outliers. In particular, points which are outliers which are closest to other points that are outliers will be retained. A handful of them show up in this example. EditedNearestNeighbours (and a couple of similar algorithms which are tweaks thereof) use the K nearest neighbors algorithm to determine which points to move out. The algorithm allows you to remove points which contain a majority of points not in the same class as the point under consideration. Alternatively, and even more strictly, you may choose to remove points containing at least one neighor not in the neighborhood. This algorithm provides the flexibility that TomekLink lacks because it allows you to set the size of the neigborhood under consideration yourself. In fact, a TomekLink is actually just a special named case of this K nearest neighbors approach where k=1. The bottom left panel in this demo is the same output we got out of the TomekLinks run in the previous section. Otherwise we can see that by increasing the sizes of the neighborhoods and setting model=all we can start to peel back quite a lot of boundary points in the dataset. This algorithm gives us the maximum control over our data cleaning, and it's the one that I recommend using. There are a couple of other algorithms which build on top of this one. First of all there is RepeatedEditedNearestNeighbours, which runs this algorithm a user-specified number of times. Each time the algorithm is rerun more outlier points will be removed, so this is a way of whittling down on poorly-behaved points that is an alternative to the somewhat totalitarian n_neighbors approach. I can't comment on the specific advantages of either approach, however; I suppose that you'll just want to try them both out. AllKNN is a modification of RepeatedEditedNearestNeighbours which also expands the size of the neighborhood being considered each time it is run. Overall I'd prefer to just use and tune EditedNearestNeigbours. There are even more undersampling algorithms available in imblearn.  I won't cover them here as they start get pretty nuanced however. If you're deep in the weeds on a data cleaning problem, you should check out the official documentation with the rest of the owl here: link. EditedNearestNeighbors fits most of the cases I run into however. 