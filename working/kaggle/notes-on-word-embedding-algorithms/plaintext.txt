Words cannot naively be passed to a neural network (or other machine learning algorithm), they must first be encoded into some kind of numerical form fit for input. We can think of words as being distinct inputs, which nevertheless have some kind of relationship with one another in some kind of abstract space of meanings. For example the words "princess" and "queen" are close to one another in meaning, while the words "ball" and "square" are far away. There are a number of different approaches to generating word embeddings, whose relative merit is based on how good they are at placing words in vector space close to one another. The approaches can be split into roughly two categories: probabilistic approaches (e.g. using a neural network to optimize an embedding), and classical "count-based" approaches. The choice of word embedding used is important to network performance; it is effectively the most important preprocessing step that you perform when performing an NLP task. Any algorithm that performs dimensionality reduction can be used to construct a word embedding. Historically single-vector decomposition was applied to the dataset in a paradigm that took advantage of TF-IDF known as latent semantic analysis, or LSA. This was the best-performing general purpose algorithm for word embeddings, and it's a "count-based" method that involves vectorizing a co-occurance matrix. scikit-learn provides an implementation of LSA called TruncatedSVD: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html. nltk probably provides an implementation as well. Parts of this high-level description from here. word2vec is a particular neural network build which has been found in practice to be good at extracting semantic vectors from words. It can applied to any kind of input (not just words) that has intangible topical relationships. For example, gene2vec is also a thing. There are two types of word2vec architectures. The first is known as "continuous bag of words". Words are one-hot encoded. The input to the neural network is a set of words prior to the current words of some length. The words are processed through the network and used to predict that next word. The word vector learned is the projection is the output of the layers immediately before the output layer. The second is known as "skip-grams". In this architecture, the word itself acts as input, and it is used to predict the surrounding context: e.g. the previous N words and the N words that follow. This is an eyebrow-raising approach, but it turns out to be more performant on very large datasets, because it tends to result in a word vectorization that is roughly equally inaccurate with respect to the possible contexts in which that word may appear. word2vec is implemented in Python in gensim: see the gensim documentation for details. This implementation can be trained on whatever corpus you'd like to train it on; the docs demonstrate on the Brown corpus, taken from nltk. Ths implementation has some hyperparameters you can tune, but it's word2vec is not an autoencoder because it does not attempt to reconstruct the input word vector, but it it is very similar to an autoencoder in that it builds a compressed representation of an input as its primary output. One interesting fact about word2vec is that, for computational efficiency, the learning task is not actual prediction of the word, but selection of the word from a bag of that word plus a variety of made-up and/or sampled words. This technique is known as noise-contrastive estimation. For a bit more detail on word2vec computatation, see this Medium.com post. However, I think that to really understand the implementation itself, you'd have to read the original paper. For feature learning in word2vec we do not need a full probabilistic model. The CBOW and skip-gram models are instead trained using a binary classification objective (logistic regression) to discriminate the real target words (wt) from k imaginary (noise) words ~w, in the same context. The history of word2vec: it was published circa 2013 by Google researchers, and got traction as an improvement on the techniques up to that point. These days word2vec is no longer state-of-the-art; for that see ELMO and BERT, which build on word2vec. Pretrained word2vec models: probably the best general-purpose one is wikipedia2vec, which is freely downloadable. In contrast with word2vec, GloVe is an evolution on earlier count-based approaches. GloVe pairs a massive co-occurance matrix with a specialized machine learning model known as a log-bilinear model (whose formulation is explained in general terms here) with weighted least-squares objective function. So it's like LDA, except that it uses a model instead of matrix factorization to generate its end representation. The homepage for GloVe is here; this page includes a full representation trained on the Wikipedia corpus. ELMO was the next step up in terms of word embedding algorithm performance. It came out in 2018, out of the Allen Institute. It uses an architecture which is fundamentally based on bidirectional LSTM layers, and can perform the usual set of embeddings but was built in particular for sentence embedding. Unlike BERT, it outputs a single embedding representation (see next section). Like BERT, it is easily importable from Tensorflow Hub, and can be used in a plug-and-play manner inside of a Keras  model. A good high-level summary of ELMO, and the basis for this article, is available here. BERT is a further improvement to ELMO which is also 2018 vintage. BERT is an improvement on word2vec that includes the context of the sentence in its evaluation of the vector. word2vec will give the same word embedding for the word "bank" in "went to the bank for cash" and "went to the back to go fishing", BERT will assign different embeddings. This description of BERT courtesy of the following blog post: https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/. BERT uses a tokenizer known as "WordPiece". WordPiece creates a fixed-size vocab of all of the letters of the English alphabet plus the 30,000 most common English words and subwords, then decomposes the words that occur in a sentence into those words and subwords. Any words that fail to decompose according to these rules will decompose into individual characters. For each separate token, BERT outputs 12 length-768 vectors. Each of these vectors represents a different "kind" of encoding of the words. Although the vectors are all broadly "good", the best utilization of BERT in terms of what to do with them (pool them all, pick one, average a subset, etcetera) depends on the application, as there are meaningful differences in the way information is mapped in the encodings that. To generate a sentence vector, pool the token vectors somehow (lots of choices as to how, no obviously wrong or right way). Using BERT with Keras: BERT is available as a Tensorflow definition on Tensorflow Hub, and can be plugged into a Keras model that way. Here is a blog post with the requisite code samples showing how this is done. 