Novelty detection and outlier detection are two distinct but inter-related problems. In outlier detection, we are interested in finding records in a dataset which are abnormal, according to some definition of abnormality. For example, when we specifically mark off points more than 1.5 times the interquartile range away from Q3, as we are told to do when building a box-and-whiskers plot, we are performing one fairly simple example of an outlier detection task. Novelity detection, on the other hand, is used for flagging incoming data, or packets of data that we get later on, and by doing so detect whether or not the underlying distribution generating the data has changed (e.g. whether incoming data is novel or not). Another way of thinking about it is that in novelty detection we assume the data we have is a strong base and would like to build on it, while in outlier detection we think the data we have is noisy and we need to prune it down. sklearn implements a number of strategies for these two tasks, summarized in this fantastic page in their user documentation. 