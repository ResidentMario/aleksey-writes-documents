Ridge regression is a type of regression that modifies ordinary least squares regression in a way that's helpful for avoiding overfitting. Mathematically speaking, the TLDR is easiest to express in matrix form. In OLS, you solve for weight estimators using: In ridge regression, you solve for: Where $I$ is the identity matrix and $\lambda$ is a new input variable that we are introducing into the model. $\lambda$ is controlled by a hyperparameter, $\alpha$, that controls how strongly we penalize overfitting. To demonstrate this, let's invent a problem to solve. Unfortunately this is going to be a pretty convoluted example, but it should get the point across. Suppose that a new video game is to be released in Japan. We have, via e.g. Steam, the entire ranked history of video games released by this maker in North America. We'd like to build a model that predicts the sales volume for this video game in Japan given its current performance here. Here's what we're going to be working with: Let's go ahead and apply polynomial regression to our problem (we gloss over the details of how polynomial regression works, see this previous notebook for more on that). It's pretty clear from these plots that polynomial regression with a degree of three is best way to go. Linear regression is no good because it doesn't capture the underlying pattern of the data, which is exponential; meanwhile, regression with degree 10 creates a very complex function that's strongly and very obviously overfitted. Let's take a closer look at the coefficients (the weights $w$) assigned to the variables as a part of these models. What we see here very clearly is the fact that the more complex the model, the greater the magnitude of the weights assigned to it. We start with a relatively sane number, 0.007. The smallest coefficient in our best model (in m3_coef) is much, much smaller: just ~0.000006. And the least sigificant coefficient in the ten-model is incredibly tiny, just ~0.0000000000000002! This kind of progression in degrees is typical of what happens when you overfit a polynomial model. The more degrees in the model, the larger the coefficients, because the greater the "skews" between variables necessary to optimally solve the matrix equation. Now, if our starting point was the degree-10 function, we would "fix" our model by relaxing the number of degrees afforded to it. Back down to 3, ideally. Ridge regression is an alternative way of achieving this coefficient shrinkage. It imposes something called an L2 penalty by changing our metric so that we add a penalty on the order of the square of the magnitudes of the coefficient. You can think of it as "smoothing out" the regression curve. In the next three plots, we'll use ridge regression to squeeze the numbers back. Note that we're using very large $\alpha$ numbers here b/c I haven't normalized the data. Overall, Ridge(degree=10, alpha=10**40) is pretty comparable to LinearRegression(degree=3). Ridge regression will never take any of the model numbers to 0 (e.g. it will never cancel variables): Ridge regression is theoretically most valuable when it is used to counteract the effect of colinearity. Colinearity is when two supposedly independent variables are actually scalar multiples of one another, or close to it. In other words, two variable matrices $A$ and $C$ will be colinear if there is some scalar qauntities $\alpha$ and $\beta$ such that: In practice, completely colinear variables are rare. When they do occur, they'll explode your model when you try to use it, so you'll know about them quickly. However, it's totally possible for two variables to be highly, but not totally, colinear. In this case they "share" the information that they provide to the model, to some strong degree. Regression does a poor job optimizing such models because it will exaggerate correlated variables: the model will want to optimize for such variables more, and for standalone variables providing actually unique information less. This is undesirable, and not something you can fix (as here) by just reducing the complexity of the model. Going even further, in practice you will most need to use ridge regression to account for this problem (as opposed to, say, feature engineering, or reducing model complexity) in the case of very large datasets with a large number of variables. Why is ridge regression useful in practice? Because it's mathematically easy, at least insofar as such complex things go. The best reference on that IMO is this StackOverflow answer. In implementation terms it's computationally effective, as the modifications necessary do not extend the big-O complexity of the algorithm at all. 