 You start up a Jupyter notebook on your local machine for maximum convenience or, for maximum resource flexibility, on some compute instance on some cloud service. You do a bunch of work in your notebook, iterating on the dataset, doing EDA, and building up to some initial tiny model. Now you've gotten pretty far, and you want to run a training job on that model. You use the Jupyter cell-copy command to extract just the cells necessary for model definition out to a separate notebook file. Then you run the alekseylearn CLI tool, scheduling a training job for your new model. You continue tweaking the code, making a few more adjustments to your model as you iterating, firing off new training jobs for the updates. Eventually you go out to lunch. Midway through a Sprite can your watch buzzes you; it's a notification that, hey, your models finished training! Now you know you can go back to your original notebook and extract the model to test how well it did. As of 2019, it is impossible for a user with my low level of technical aptitude to build a Docker image that is GPU-compatible without having access to a local GPU. As a result, the work model that I really want to happen with alekseylearn is only possible if running on a local machine which has a GPU, or if running notebooks in an environment which has the entire toolchain already provisioned. SageMaker does provide that toolchain! However, I must be running a notebook on a GPU-accelerated instance, which costs ~1$/hour and up. SageMaker actually uses a "Machine Learning" themed AMI that Amazon releases via its Marketplace. I had anticipated that you could perform model training using Docker images and remote compute instances. What I discovered was that when the GPU is involved, there are two sources of difficulty regarding this computational model. One is that building a GPU image requires having a particular GPU toolchain set up, and that toolchain is down to the level of the hardware (you must have a GPU) and the drivers. Luckily that toolchain is accessible via SageMaker and/or related machine learning -focused APIs. The other difficulty is that running a GPU image requires passing a --runtime="nvidia" command-line argument to docker run. It's not possible to pass such a flag via the SageMaker API. Therefore, it is not possible to build custom GPU training images via the SageMaker API. It is not likely that other APIs are more loose here. As a result, I am not going to develop alekseylearn any further than it already is, because it's clear that my preferred computational model is not really supported by today's compute environments. Oh well, I tried! So instead I will continue to use the current preferred computational model: running model training in uncontainerized Jupyter notebooks running directly on high-GPU compute instances. Defer containerization until productionization. 