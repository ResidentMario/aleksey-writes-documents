This dataset contains a record of every patent assignment which took place on October 18, 2016. A patent assignment occurs whenever a patent is either issued experiences a change in ownership. There are apparently perhaps 2 million or so active patents in the United States, and this dataset is a window into what a day in their movements looks like. This notebook has three parts: flatenning; general exploration; and, finally, topic modeling. The goal is to outline the basics of how natural language processing, specifically topic modeling, works. Click here to skip directly to the topic modeling. The dataset is provided in an XML format, which is a bit awkward to work with. Before doing anything else with it, I've written a pipe that turns it into a more easily workable CSV file (note, however, that this CSV file's format is not ideal for all possible applications...). The details of this code are explained in another notebook, and we will gloss over them here. Let's get to building a topic model around our patents. To do this we're going to use nltk, the "natural language toolkit", and gensim, a popular Python topic modeling and querying framework. Topic modeling is a set of tasks in the natural language processing field organized around classifying individual documents according to their topics. The idea is that if you have, for example, two sets of documents, one about football and one about tennis, for example, we should be able to use a computer to seperate them into two different topical "piles". In this case, we're going to try out classifying our patents based on their titles. Note that this is a bit of a stretch, as topic modeling works best when given fulltexts of things (IBM's Watson tools for example always recommend you input at least 250 words per document, or thereabout), but it nevertheless makes for a good demonstration. The first thing we have to do is tokenize our words. A naive way to do this would be to split our string based on spaces (e.g. str.split(" ")), which is sometimes OK but has many edge cases (alternative punctuation marks like —, for example) and will fail to work as expected for larger problems. nltk comes with a built-in word tokenizer that we can take advantage of. Next, we will stem our words. Stemming is a procedure in natural language processing where we chop off everything except for the root of a word. So for example, the words go, going, and gone will all map to the same root—go. This is a good thing to do, particularly given the small size of our documents, because it increases the accuracy of classifications—more things end up being the same. nltk comes with several stemmers installed, we'll use the PorterStemmer. If we examine a list of words, however, we see that the most common English-language words dominate: These words carry no meaning and aren't very interesting. They're known as stopwords in NLP, and we're going to once again use nltk builtins to remove them from consideration. Next, let's consider the opposite problem: words that occur to infrequently to be useful. Words that only ever appear once, for example, don't carry any information. Remember, we're going to split all of our patent titles into some small number of classes; just as in any other dataset, a data point which is only populated once isn't interesting, and can be safely dropped. In fact, we could probably drop a lot of words from consideration, not just ones appearing once but ones appearing tens or even hundreds of times. This would speed up our algorithms and won't significantly impact our results. After a certain point words do start to matter, however; figuring out where that point is is up to you. In our case we'll just be lazy and cut off at words that appear only once, and leave words appearing twice or more intact. Notice that discarding words from our set has resulted in a handful of empty titles. Apparently a few patents have nothing but unique words! With our titles adequately processed, now we switch over to gensim. The first thing we have to do is build a dictionary of words, which associates each word [stem] with a particular index number: Why are we doing this? Because shortly we're going to throw our corpus into a tf-idf algorithm. TF-IDF is an algorithm in information retrieval which converts a list of word "vectors" to a scaled Euclidian normal vector. It turns a count of the number of each word in our document into a unit vector in N-dimensional space, where N is, believe it or not, the number of individual words that we have in our dictionary (above). That means that, in this case, we have a "dataset" matrix with hundreds of thousands of columns in it! The beauty of TD-IDF is that it scales the words according to how frequent or rare they are. Words that appear a lot in your text but also appear a lot in the rest of the corpus are weighed less heavily than words that appear a lot in your text but more rarely outside of it. Thus we first use gensim to convert our words to word incidence vectors... ...then run TfidfModel from gensim on them to turn them into our word vectors! Note that gensim doesn't follow the scikit access pattern, if you are familiar with it. It instead (1) defers computations on individual entries until necessary and (2) provides access to data using bracket indexing notation ([]). By contrast, scikit will run everything immediately by default, provides results using a .values_ attribute, and seperates model initialization from runtime (the latter doesn't occur until you fit() your model). With our words suitibly datified, we can now move on to fitting a model. Since our words are now, effectively, a very large dataset, it's possible to use any general purpose classifier to fit it. An earlier notebook on this dataset, for example, uses a scipy KMeans clustering algorithm to arrive at its topics (you should go read that after you're done with this one). We'll instead use a model specifically adapted to natural language processing from the gensim built-ins, LsiModel. Here's how we run it: Here's a printout of what words are important to our various topics. Notice that certain extremely common words, like semiconductor, appear in different positions in multiple classifiers. Also, note that this display is cut off at a certain number of displayed words; in reality the model considers far more than these (you can specify how many to display here, however, using the num_words parameter). Here are the scoring outputs for the first five documents: Let's use these scores to fetch best-fit classifications for all of our (classifiable) patents: Certain topics that our classifier arrives at are much more common than others. Let's see what our classes look like. It's uncertain that our classifier found a "good" representation of our title data. Certainly I think that METHOD OF TREATING INTRAOCCULAR TISSUE PATHOLOGIES and Selecting graphical component types at runtime should find their way to seperate classes. However, given the low volume of information contained in a simple patent title—in some cases these titles are just one or two words long!—this is clearly just about the best that we can do. TF-IDF is not unique to gensim; it is also available in scikit-learn, among other places. gensim is a high-capacity but low-level library, and a classifier with very performance in much less code also on this data is available here. Nevertheless, with "wider" datasets gensim models should be the most performant. 