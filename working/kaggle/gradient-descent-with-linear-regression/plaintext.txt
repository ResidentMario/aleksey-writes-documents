Gradient descent is a name for a generic class of computer algorithms which minimize a function. These algorithms achieve this end by starting with initial parameter values and iteratively moving towards a set of parameter values that minimize some cost function or metric—that's the descent part. The movement toward best-fit is achieved by taking the derivative of the variable or variables involved, towards the direction with the lowest (calculus-defined) gradient—that's the gradient part. Gradient descent is an important concept in computer science, and an illustrative example of why CS has kind of overtaken statistics in importance when it comes to machine learning: it's a general-purpose tool that can be used to "brute force" an optimal solution in a wide range of scenarios, which doesn't have the elegance, closed-form solution, and unfortunate sheer mathematical inpalatability of a statistical solution. Ordinary linear regression is a good and simple way of demonstrating how gradient descent works. We start with some error function. We could use any metric we want, but in OLS the obvious one is the residual sum of squares. Given a sequence of points, $y_i$, and a sequence of points predicted by our model, $\hat{y}_i$, RSS is: Our objective is to minimize this value. Inserting our linear regression model in for the $\hat{y}_i$ predictions, and assuming (for the sake of simplicty) that we're doing regression on only one variable, we get: Where $b$ is the intercept and $m$ is the slope of the line of best fit. Now we need to take the gradient. Since this is an equation of two variables ($b$ and $m$) the gradient will consist of two partial derivatives. Hence the gradient is: To solve, take a step in the negative gradient direction every iteration. Eventually we will have something that converges. Let's implement and test this (note that for the implementation we'll actually use MSE, mean squared error. MSE is just RSS divided by the number of points, $n$. We do that because it leads to "nicer" input numbers, as RSS is a really big number). That's all we need! OK, let's see how this performs on some example data. We'll generate a cloud of points that's normally distributed around the line $y = x$, and see what our algorithm cooks up. Success! Our model solution is very close to the ideal solution of $m=1$ and $b=0$. The biggest advantage gradient descent has is that it requires no knowledge whatsoever of the fundamentals of the model. We can apply the classifier we've built without knowing anything about linear regression. In particular, we don't need to know that linear regression has a closed-form solution, or what that solution looks like, or how to derive it. Instead we just pick a metric, compute its derivative, and then use a computer to brute-force a solution. This is wasteful for simple ordinary least squares, because we have a way of solving this already, sure. But it's a boon if we start to come up with custom metrics, or do other similar things. A gradient descent solution to modeling can be applied to any model metric, so long as the metric has two properties: it's differentiable (most things are) and concave. Concavity is the property that no matter where you are on the metric surface, the derivative will point towards a point with better "fit", right up until you get to the bottom of the thing. Things that are concave include funnels, eye contacts, and, it turns out, the linear regression parameter space. In the case of neural networks, come up with a parameter surface so complex no statistical theory can contain it, and brute forcing a solution becomes the only way to build a model. There are many, many ways in which this process can be optimized, generalized, and tweaked, most of which find their way into practical ML. There's also a whole mathematical subfield, convex optimization, concerned with this very practical problem space. See this blog post for the reference for this notebook. 