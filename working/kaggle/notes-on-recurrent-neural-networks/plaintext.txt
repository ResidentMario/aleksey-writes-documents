These notes are based largely on Colah's excellent blog post "Understanding LSTM Networks". Recurrent neural networks, or RNNs, are the top-performing model architecture for sequential data. They work very differently from feedforward neural networks and convolutional neural networks, which are both fundamentally linear, with data passing through layered steps. RNNs are fundamentally sequential: they may have one-to-many, one many-to-one, or many-to-many input structures. Conceptually speaking, you can think of an RNN as a sequence of feedforward neural networks. The output from the first feedforward network is used by the second feedforward network, and that output is used by the third feedforward network, and so on down the chain. Practically speaking, the way that this is acheived is that each output node on each layer serves simultaneously as an output node for the current sequence layer as well as an input node to the next sequence layer. In visualization this dynamic is usually modelled as a node that is connected to itself:  An RNN is thus representable as a huge fold-out feedforward network that has this special self-referential structure. All of the normal learning routines continue to operate as you would expect them to, albeit with scoring that is multiplied across the many elements of the total output sequence. RNNs are natively applicable to data structures which are sequential in nature. However, they are also applicable to any dataset that you can restructure in a sequential manner. For example, you can train an RNN to recognize and read digit sequences on license plates or door decals by feeding it sequential pixel-by-pixel input in a left-right manner. All of the cutting edge recurrent neural networks used in practice are a particular subtype of neural network known as an LSTM. LSTMs solve the long-term memory problem. RNNs suffer from the limitation that they often cannot connect old but relevant information to new insight. They have a limited "memory" for the information that informs their decisions, and data that extends past that memory window is forgot and left unused. In theory RNNs can learn to propogate messages across long periods of time, but in practice they simply do not learn to do so in a convergent way. LSTMs solve this problem, and in doing so, greatly enhance the usefulness of the models. In ordinary RNNs there are layers in between the output nodal layers. In the simplest case these layers may be a single tanh activation layer. In LSTMs these are replaced by a structure of four layers which interact in a special way. Colah's blog has the following visualization of how it works:  The core of the LSTM is the topmost structure, the cell state. The cell state runs linearly through the entire sequence of neurons, but its intake of information is regulated by the rest of the structures in the diagram---the gates. In a vanilla LSTM training proceeds in three steps, corresponding with and regulated by three gates: LSTMs have many variants. Some of the major ideas are: 