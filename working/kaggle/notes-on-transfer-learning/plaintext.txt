Transfer learning is a technique that enables training powerful machine learning models on marginal amounts of data and compute by reusing weights learned by an existing complex model. Despite the scary-sounding name, technically speaking it really is that simple. There's a lot of flexibility in how transfer learning can be applied. We may use an entire already-trained model as a subnet in our own neural network architecture. Or we can pick just a few layers from that model and use those. We may choose to make the model layers trainable or not trainable. If the model layers are marked untrainable we will not backpropogate on them, and so we will avoid a lot of additional computational work. Transfer learning is a sort-of example of a pretraining technique: a way of determining relatively good weights for a model without performing the full computations from a random start. Other pretraining techniques include using autoencoders and Restricted Boltzmann Machines. The difference is (basically) that transfer learning uses results that are already known, which can mean reusing Google-designed models or whatever that took thousands of dollars of compute to train...so it's something that's done quite often, in contrast with RBMs and autoencoders, which have fallen out of favor. The following code is based on this blog post on transfer learning. No model is actually trained here; I'm just defining one to see how it works. Note that this code cell fails because Kaggle blocks Internet access by default and we need to download MobileNet before using it, but it will work locally and it will work if we use an Internet-enabled kernel. This "fine-tuning" strategy is presented by Francois Chollet in his blog post "Building Powerful Image Classifiers with Very Little Data". That post does two things: it shows an alternative approach to doing the above, and shows how you do the above even further with "fine-tuning". Instead of stacking the models and turning off learning on some layers, Chollet runs model data through the pretrained model's convolutional layers, then uses the raw outputs as raw inputs into the next model layers. He calls this approach "bottleneck features". I don't see this being notably advantageous, except in that it allows something more advanced: "fine-tuning". Fine-tuning means unfreezing one (or potentially more) of the layers of convolutional part of the network and retraining those alongside the re-emplaced dense layers. To fine-tune properly: More details in the blog post. TODO: apply transfer learning to something not Cat/Dogs myself. 