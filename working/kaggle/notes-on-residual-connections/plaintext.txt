This notebook contains my notes on a couple of articles discussing residual connections and ResNet: "Residual blocks — Building blocks of ResNet" and "An overview of ResNet and its variants". ResNet was a seminal convolutional neural network architecture published in 2015, which won a number of major image classification challenges at that time. It introduced a new concept of residual connections, which was key its performance improvements over past networks. Generally speaking, the deeper a neural network is, the more able it is to learn to perform a specific task (e.g. the higher its expertise). However deeper neural networks are also more and more vulnerable to vanishing gradients, due to the behavior of the propogation of the signal. Due to this limitation AlexNet, which won ILSVRC in 2012 (beating the next best effort by 10%; credited as a touchstone which set off the deep learning revolution in earnest), has just five convolutional layers. By 2014 the best models, VGG and GoogleNet, had improved to 19 and 22 layers respectively. Residual connections have provided a huge boost to networks' ability to overcoming vanishing gradients, and hence, to ever-deeper networks' expertise on the problem. They allowed ResNet, which won ILSVRC in 2015, to go even deeper—to 34 or 50 layers deep. A residual connection connects the output of one earlier convolutional layer to the input of another future convolutional layer several layers later (e.g. a numer of intermediate convolutional steps are skipped). The input of the mainstream model and the input of the convolutional prior are combined using a simple sum. The connections on the mainstream branch are called residuals because, with the bulk convolutional feature map carried off onto the skip branch, the mainstream convolutional layers are left learning the residual feature map, e.g. the negative of the skipping feature map. This arrangement has advantages for the learner. For one thing, there is evidence to suggest that convolutional layers are better at learning on the residual of a feature map instead of directly on the feature map. For another thing, preservation and propogation of a finely-tuned signal through the network is made much easier: propogating the input to the residual block forward to the next block unmodified (e.g. learning the identity function) is as simple as setting all of the weights on the residual connections to zero. This makes residual blocks naturally friendly to sparsification. There are a few different methods for laying out the residual block. The following diagram provides some examples. The layout which has been found to work best in practice is the one of the far-right, "full pre-activation":  Residual connections are a type of connection known as a highway connection: they incorporate a pass-through "memory highway". They are actually releated to LSTM and GRU modules in the RNN world because of this. The following architectural diagram lays out the difference between ResNet and its proginitors, like VGG. Notice that:  One of the most important updates to or adaptations of residual connections is dense residual connections. A dense residual connection replaces the sum operation at the end of the residual block with a concat operation. This effectively doubles the size of the feature vector; when this proces is repeated, it results in an n-fold increase in feature vector size. Deep residual connections result in much larger neural networks, which take much longer to train. However they have the advantage of allowing
each additional layer to the network to "see" some subset of prior residual block outputs (kind of like with RNNs and attention). That's more information that the model can use, and hence, hopefully a better model fit. While it's out of scope for this notebook, there has obviously been a lot of research into residual connections since their introduction in 2015. One interesting idea that has been tried: adding skip connections between different convolutional blocks. For some background reading highlighting the most interesting examples of architectural evolution and pointing you to the right papers to read see "An overview of ResNet and its variants". 