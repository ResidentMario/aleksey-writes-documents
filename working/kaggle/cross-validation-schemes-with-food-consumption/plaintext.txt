In this notebook I'll use the food consumption dataset to quickly leaf through different cross validation techniques. Cross validation is a technique in machine learning for estimating the reliability of a model result. The problem when computing a model based on all of the data available is that it is very easy to generate an overfitted model, and generally (especially in many dimensions) hard to tell when we do. Cross-validation solves this problem by only training the model on a part of the data, then exposing the model to "the rest of the data" and checking to see the results. There are many ways to perform cross validation. This notebook leafs through the options for doing so made available in scikit-learn. The model in this notebook will a simple linear regression model trying to approximate next year's normalized food consumption across all the different categories present in the dataset. I won't draw too much attention to it because this turned out to be a crummy thing to try to predict. Oh well! The train-test split is the simplest form of cross validation: cross validation with one "fold". Some do not even consider this "real" cross validation. In the train-test split we break the data out into two sets, a training set (typically 60 to 80 percent of the data) and a test set (the rest). The model is trained on the training set, and then its performace is evaluated on the test set. That's it! train_test_split in sklearn is what we need. If we compare this with the result from the training set, we find that we are slightly but acceptably overfitted: Commentary (from here): The holdout method is the simplest kind of cross validation. The data set is separated into two sets, called the training set and the testing set. The function approximator fits a function using the training set only. Then the function approximator is asked to predict the output values for the data in the testing set (it has never seen these output values before). The errors it makes are accumulated as before to give the mean absolute test set error, which is used to evaluate the model. The advantage of this method is that it is usually preferable to the residual method and takes no longer to compute. However, its evaluation can have a high variance. The evaluation may depend heavily on which data points end up in the training set and which end up in the test set, and thus the evaluation may be significantly different depending on how the division is made. KFold cross validation is what most people think of when they think "cross validation". In k-fold we divide the dataset into $n$ even sets. The model is trained on all the folds bar one, the "holdout set". We iterate through the folds, with which of the folds is the holdout set and which ones are not changes each iteration, until we've gone through all of the folds and generated a score for each one. Commentary: K-fold cross validation is one way to improve over the holdout method. The data set is divided into k subsets, and the holdout method is repeated k times. Each time, one of the k subsets is used as the test set and the other k-1 subsets are put together to form a training set. Then the average error across all k trials is computed. The advantage of this method is that it matters less how the data gets divided. Every data point gets to be in a test set exactly once, and gets to be in a training set k-1 times. The variance of the resulting estimate is reduced as k is increased. The disadvantage of this method is that the training algorithm has to be rerun from scratch k times, which means it takes k times as much computation to make an evaluation. Repeated k-fold cross validation is k-fold cross validation done more than once. We run a $k$ fold cross validation, and get a bunch of results; then we independently pick new folds, and repeat the process again, generating another $k$ results, and so on. Repeated k-fold is useful for datasets where the number of observations available to us is relatively low. Leave-one-out cross validation is an extreme form of cross validation where we have as many folds as we have data points. In other words, we train the model on every observation except for one, and then test the model on the one remaining point. This has two interesting effects: 5-to-10-fold cross validation can behave in practice better than LOOCV does (according to the sklearn documentation). LOOCV tends to be more effective for smaller datasets, however. Leave-one-out cross validation is K-fold cross validation taken to its logical extreme, with K equal to N, the number of data points in the set. That means that N separate times, the function approximator is trained on all the data except for one point and a prediction is made for that point. As before the average error is computed and used to evaluate the model. The evaluation given by leave-one-out cross validation error (LOO-XVE) is good, but at first pass it seems very expensive to compute. For more background and when to use k-folds and when to use leave-one-out, see here and here. LPOCV computes every possible combination of train and test data where the test data consists of $p$ points. This results in $\left(\frac{n}{p}\right)$ train-test pairs. This is like an even more extreme form of LOOCV, effectively. Shuffle and split is a cross validation scheme that doesn't use folds but instead merely shuffles and partitions the data $n$ times. This is basically repeated holdout set generation. The difference between this technique and k-fold validation is that since this technique is a permutation, an individual data point is trained multiple times, usually. k-fold, on the other hand, is designed to avoid training on the same data point more than once. Shuffle-and-split performs worse than k-fold validation in terms of its accuracy for datasets with a large number of observations, but may perform better for small datasets. For extremely small datasets there may be no other choice besides using shuffle-and-split! The stratified k-fold classifier is designed to purposefully oversample extremely rare points in the dataset. This is used to make k-fold cross validation more indicative of performace on observations that we care more about. For example, if our dataset is cancer diagnoses, we care more about (rare) positive results than hu-hum negative ones! (for more on this topic read up on alpha and beta risk) It exists. 