Mixed precision training is, in a nutshell, the use of reduced precision values for calculations involving intermediate variables. Downcasting vectors to reduced precision enables enormous speed ups in training, modulo hardware support of course. Typically "classical" deep learning backpropogation is conducted in FP32, aka single precision. The most common form of mixed precision training downcasts this to FP16, aka half precision, during the backpropogation process, and then casts it back up to FP32 just before the final weight update. This enables speedups of up to 60%: perhaps 50% in savings for doing math in values half as many bytes in size, and perhaps 10% additional savings in inter- intra- GPU communication costs. In practice, due to the presence of fixed costs the mixed precision speedup is most significant in large to very large networks. Smaller networks do not benefit from it nearly as much. Here's the paper introducing the idea (more or less) in its current incarnation. Multiplication by small learning rates and small intermediate values in FP16 swallows any numerical value smaller than -24, which the authors state (without referring to any specific network?) corresponds with 5% of all total weights. This fundamental issue is solved, in part, by keeping both an FP16 and an FP32 copy of the network weights in memory. Whilst training is performed fully in the FP16 domain, updates are performed on the FP32 copy of the weights (the master copy) and then transferred back. Maintaining a whole second copy of the network sounds expensive memory-wise, but it's actually cheaper than doing it in FP32. Most of the weights in memory during training are intermediate values from gradient updates, so halving the sizes of these values actually reduces the memory footprint by up to 50%. The paper authors further ameliorate problems caused by the reduced precision by scaling the loss by a constant factor (they suggest 8, e.g. 2^3, as a starting point). This produces gradient updates which are scaled up by a constant factor as well. This precision-preserving scaling can then be reversed after the fact. The exact treatment suggested depends on the type of operation: Mixed precision training has technically been possible for a long time, as NVIDIA GPUs have had special support for FP16 operations for several generations now (since CUDA 6.2; see here). However, it never worked meaningfully well because that support was still fairly limited and because there were a lot of bugs at the firmware into framework levels that made gains hard to actually get. According to a random commentator on the Internet and NVIDIA marketing mixed precision training is only meaningful on GPUs with tensor cores and CUDA 10+. Tensor cores are in the Turing architecture (the T4 is the only generally available cloud GPU of this generation), with an improved version coming in the forthcoming Ampere architecture. See my Cloud GPUs notes for slightly more details. PyTorch 1.4 added a GradScalar object. The GradScaler can wrap losses and scale them by some internally determined scaling factor, implementing the loss-scaling behavior described in the paper. The issue of dealing with gradient updates that cause divergence in large gradient updates, e.g. takes them to inf or nan, remains. To deal with this problem without burdening model authors with behavioral fine-tuning, the GradScalar object raises and lowers loss scaling using built-in exponential backoff: scaler approximates the optimal scale factor over time by checking the gradients for infs and NaNs during every scaler.step(optimizer) (or optional separate scaler.unscale(optimizer), see unscale()). If infs/NaNs are found, scaler.step(optimizer) skips the underlying optimizer.step() (so the params themselves remain uncorrupted) and update() multiplies the scale by backoff_factor. If no infs/NaNs are found, scaler.step(optimizer) runs the underlying optimizer.step() as usual. If growth_interval unskipped iterations occur consecutively, update() multiplies the scale by growth_factor. This is technically sufficient to implement mixed precision training, so long as you manually run parts of the network in reduced precision yourself. However, doing so is not very practical; you'd have to implement of all of the FP32 sideloading yourself, somehow. If you wanted to use mixed precision training, the go-to was using apex, an NVIDIA project adding mixed precision support to PyTorch via external libraries, but word on the street is that this is extremely hard to install. PyTorch 1.6 (unreleased, so take a nightly) introduces full support with the addition of autocast, a context manager that automates running parts of the network in FP16 as appropriate. The combination of these two APIs (as documented here in the PyTorch docs) allows you to natively implement mixed precision training. So now use that! Note that though the autocast context manager is only enabled on forward it is automatically extended to backpropogation as well, e.g. both forward and backwards passes are covered by the same autocasting rules. I've built a couple of demo projects leveraging this automatic behavior, see e.g. ResidentMario/spell-unet-bob-ross. OK but what gets autocasted? Notes on the autocast op reference follow. Close reading because the details matter. These ops always go through autocasting whether they are invoked as part of a torch.nn.Module, as a function, or as a torch.Tensor method. The torch.nn.Module wrapper enables you to call Cls() (e.g. Cls.__call__()) to perform operations on an input. Part of the design philosophy of PyTorch is that modules should be fully stackable, you should be able to invoke modules inside of other modules with the same expected behavior as direct invocation, and it's good to see that continues to hold in the autograd API. Otherwise, methods in PyTorch are available as pure functions (import pytorch.functional as F) or as tensor object methods. Both APIs link to the same, and it's just a matter of style which one you prefer to use. Either/or works with autocasting. If functions are exposed in multiple namespaces, they go through autocasting regardless of the namespace. Functions can be defined or originally imported in many different files (scopes). Autoscaling works regardless of which file (scope) the function originated in. So you don't have to limit yourself to defining all of the functions you will execute inside of the context manager in the same file as your autocast call. Ops that can autocast to float16 matmul, addbmm, addmm, addmv, addr, baddbmm, bmm, chain_matmul, conv1d, conv2d, conv3d, conv_transpose1d, conv_transpose2d, conv_transpose3d, linear, matmul, mm, mv, prelu So maxtrix multiplication, convolutions, and linear operations all work in half-precision. Convolutions in particular means that you can expect a lot of benefit to CNNs. Ops that autocast to float32 pow, rdiv, rpow, rtruediv, acos, asin, binary_cross_entropy_with_logits, cosh, cosine_embedding_loss, cdist, cosine_similarity, cross_entropy, cumprod, cumsum, dist, erfinv, exp, expm1, gelu, group_norm, hinge_embedding_loss, kl_div, l1_loss, layer_norm, log, log_softmax, log10, log1p, log2, margin_ranking_loss, mse_loss, multilabel_margin_loss, multi_margin_loss, nll_loss, norm, normalize, pdist, poisson_nll_loss, pow, prod, reciprocal, rsqrt, sinh, smooth_l1_loss, soft_margin_loss, softmax, softmin, softplus, sum, renorm, tan, triplet_margin_loss float32 is single-precision, which is a level of precision suitable for a lot of operations. It's also the default floating point precision level in PyTorch. So it's not surprising that many operations were found to be stable in float32. The fact that these operations need to be performed in at least 32 bits of precision means that you should not expect time-savings when running these layers in an autocasting context. Ops that promote to the widest input type These ops don’t require a particular dtype for stability, but take multiple inputs and require that the inputs’ dtypes match. If all of the inputs are float16, the op runs in float16. If any of the inputs is float32, autocast casts all inputs to float32 and runs the op in float32. addcdiv, addcmul, atan2, bilinear, cat, cross, dot, equal, stack, tensordot Some ops not listed here (e.g., binary ops like add) natively promote inputs without autocasting’s intervention. If inputs are a mixture of float16 and float32, these ops run in float32 and produce float32 output, regardless of whether autocast is enabled. So basically there are some operations in PyTorch which transparently promote float inputs to the largest of the input types (e.g. if provided a mixture of float16 and float32 vectors they will promote to a float32 output). Some other functions disallow mixed-dtype inputs (TODO: VERIFY THAT THIS IS TRUE). Probability to keep the autocasting API simple, some other operations pick up this automatic promotion behavior when executing in an autocast context. Only CUDA ops are eligible for autocasting. Obviously since this is targeting tensor cores downwind. Ops that run in float64 or non-floating-point dtypes are not eligible, and will run in these types whether or not autocast is enabled. The tensor cores on the forthcoming Ampere architecture include additional special support for operations on integer types as well. Since no commercially available card is on this architecture yet, it doesn't make sense to think about autocast behavior in these alternative types yet. The bit about refusing to autocast float64 is interesting. float64 is double, and it's a non-default higher precision level. PyTorch is interpreting user intent here: if the user specifically asked for float64, they clearly have a need for the extra precision and we should not ignore that intent by autocasting. Only out-of-place ops and Tensor methods are eligible. In-place variants and calls that explicitly supply an out=... Tensor are allowed in autocast-enabled regions, but won’t go through autocasting. For example, in an autocast-enabled region a.addmm(b, c) can autocast, but a.addmm_(b, c) and a.addmm(b, c, out=d) cannot. For best performance and stability, prefer out-of-place ops in autocast-enabled regions. PyTorch supports inplace variants of many to most operations, but heavily discourages their use. The reasons why are covered succiently in this TDS article. TLDR: in theory, since in-place operations overwrite the computational graph in place, whilst chainable operations allocate new objects, its use can make a model more memory efficient. However, in practice, in the case the th the use of in-place operations in graphs containing free variables breaks the computation graph, unless you .clone() or .detatch() the variable first (which detaches it from the computational graph), in which case it becomes...the beginning of a new graph? The technical details are frought, and require a much deeper understanding of autograd behavior than that which I currently have. See here for a starting point. Implicit to the whole autocasting shtick is the need to convert from float32 to float16 and back again throughout the forward and backpropogation pass. However, relative to the cost of running through the network, these conversions (which largely boil down to a bunch of bit shifts) are not a significant bottleneck. Freely banging into and out of the dtypes in not a big deal. I'm sure it has special support in the hardware as well. See this SO question for some example code. Autocast is a "safe" implementation of mixed precision training. It doesn't take full advantage of every possible optimization because many of the optimizations possible are network or architecture specific and not generalizable. As the paper points out, there are a lot of cases where sometimes reducing precision works, and sometimes it causes the network to diverge. This is something important to keep in mind about this API. When spitballing how much your network will benefit from mixed precision training, take into account two things. First, the fixed cost of keeping a second copy of your weights matrix around in memory. This reduces the amount of benefit you get from small networks. Second, the composition of layers in your network. Convolutions and linear layers, which are FP16 safe, benefit. Most everything does not. I have three examples I've used: a feedforward network, a CNN, and an RNN. The feedforward network saw no benefit from the use of mixed precision. The CNN saw great benefit, around a 40% increase. The RNN is still a TODO. Autocast has miscellaneous interactions with various other library features, which are all documented on the autocast examples page. The most important interaction IMO is with the use of DistributedDataParallel. Refer to the page but TLDR with the recommended default settings it works out of the box. 