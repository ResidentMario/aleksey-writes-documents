test.parquet is the biggest file. 8806571365 bytes is 8.8 gigabytes; too many to load into a pandas DataFerame in-memory directly; doing so will crash the kernel. However, the dataset is a time series measurement of signals along particular channels, so you can just read particular signal strides at a time. train.parquet is half the size. and is readable. So 800k time-series observations on 8.7k columns. The id_measurement is the grid point being measured. The signal_id is used to linearize the output values your model generates for submission as a two-column (ID, value) csv. So we have signal observations in the columns (in packets of three per observation point, one per each phase), and 20 millisecond spaces in the rows. The target value is unary with respect to the entire observation history, so the question being asked is essentially if there was ever a phase fault in the time sequence provided. 