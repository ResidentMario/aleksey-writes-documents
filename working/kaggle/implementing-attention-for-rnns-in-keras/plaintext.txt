The invention of LSTMs revolutionized RNNs because they solved fundamental problems that vanilla RNNs had with utilizing information embedded in the sequence that is contained in tokens that are far away from one another. Attention was the next big "thing" after LSTMs, and caused the next largest jump in RNN performance on real-world problems. The bottleneck that attention addresses is the fact that each time-step in an LSTM only gets to work with the output of the previous time-step. Attention adds an additional intermediary step: each LSTM steps additionally gets input from each previous LSTM time-step as an input. There are several different ways of implementing this additional data flow. The most conceptually intuitive implementation is a weighted sum of the previous layers' data. This is what is meant by attention: the layer learns what previous states to pay attention to, and which previous states to ignore. With attention, it's even possible for a layer to completely ignore the layer immediately previous to it in favor of layers earlier in the time sequence. Hence the name "attention". Presumably because "attention" is not a standardized layer, Keras does not provide a prebuilt Attention layer. If you want to incorporate attention into your network, you instead need to go ahead and define it yourself. This is far from impossible, but does require reading and understanding the paper to some degree. Here's one example implementation of an Attention layer, taken from the notebook keras baseline model + attention 5-fold, which was a popular baseline model in one of the Jigsaw toxic comments competitions: This model is also used in my Keras LSTM Worked Example notebook. Let's try our hand at implementing (and evaluating the performance of) our own attention layer. This one is a bit simpler, and is based on the following blog post. 