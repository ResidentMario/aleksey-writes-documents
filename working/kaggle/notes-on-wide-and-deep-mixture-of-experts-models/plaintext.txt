YouTube recently published a paper outlining the architecture of their video recommendation algorithm. This paper was summarized at a high level by the following blog post: "How YouTube is recommending your next video". The YouTube recommendation algorithm is a good example of what a modern core-infrastructure machine learning model looks like. It is huge in size, contains an ensemble of different models, doesn't optimize for a single specific task but instead for a blend of different metrics, and works with sparse data inputs. This is an example of a type of production-scale model known as a wide-and-deep model. The Google AI blog has a short explainer on the principles behind wide-and-deep models: "Wide & Deep Learning: Better Together with TensorFlow". Wide and deep models have "wide" components and "deep" components. The "wide" components are linear models. The purpose of the "wide" component is memorization: learning how the target metric responds to blends of input values. The blog post provides the following example in the context of a food-selection app: The model predicts the probability of consumption P(consumption | query, item) for each item, and FoodIO delivers the top item with the highest predicted consumption rate. For example, the model learns that feature AND(query="fried chicken", item="chicken and waffles") is a huge win, while AND(query="fried chicken", item="chicken fried rice") doesn't get as much love. In this specific case we have a (large) list of n possible categorical input values, from which we are to derive n(n - 1) pair features. A model is trained to memorize which pairwise combinations of the input values are strongly correlated, and which ones are weakly correlated. This problem has two defining characteristics: the number of possible combinations of values is (1) huge and (2) highly sparse. Techniques from affinity analysis (market basket analysis) and recommendation systems could be used here. A collaborative filtering algorithm is one approach you could use; I covered this technique in an earlier notebook. This "wide" linear model is used alongside a "deep" model, as in deep learning, so a neural network of some kind. A neural network would turn the categorical values into embeddings in some high-dimensional space, then pair values with other values near it in that space. Given sufficient data, it outperforms a linear model in theory. However, it has a real-world weakness, which stems from the fact that embedddings are highly transitive, and a neural network is a significantly more expert learner than a linear model: there are actually two distinct types of query-item relationships in the data. The first type of queries is very targeted. People shouting very specific items like "iced decaf latte with nonfat milk" really mean it. Just because it's pretty close to "hot latte with whole milk" in the embedding space doesn't mean it's an acceptable alternative. And there are millions of these rules where the transitivity of embeddings may actually do more harm than good. On the other hand, queries that are more exploratory like "seafood" or "italian food" may be open to more generalization and discovering a diverse set of related items. So in the real world the optimal solution to a highly specific query is most discoverable by a learner with high memorization and low expertise, whilst the optimal solution to a highly general query is most discoverable by a learner with low memorization and high generalizability. So a model that is optimal across both types of queries is one which uses both types of models in some weighted way. This is exactly what a wide-and-deep model does! In a wide-and-deep model, there is a linear component (an ordinary feedforward network, in Google's example case) and a deep component (an LSTM RNN, in Google' example case) whose signal value for a specific query is resolved by a final sigmoid layer. This two-expert model is trainable via simultaneous backpropogation. Wide and deep models can be considered a subset of mixture of experts models. The YouTube recommendation algorithm is based on seven different component models, which vary on the width-versus-depth spectrum, but also in another way. The wide variety of component models is necessary in part because the model loss metric is actually a composite of seven different metrics, with a mixture of contributions per model on each metric as well (that's two degrees of model voting!). Looking at the model-to-metric contribution statistics, we see that e.g. the seventh metric is handled almost entirely by a single model (one that contributes little to the other metrics):  Another key component of the YouTube algorithm is the fact that the weights on the final layer, the combination vote on the metrics, is hard-coded by humans. The ultimate optimization goal is a linear combination of business metrics (or loss-optimized forms of business metrics) whose relative demerit is set by business processes and business management. This is a common scenario for these core line-of-business models. The mixture of experts model has one other major advantage, which is not cited in these articles but which is absolutely important: it is highly parallelizable. The individual experts can be trained independently of one another. The combined "topped" model assemblage can then be further trained and optimized (with a smaller learning rate, and perhaps some amount of layer freezing, as is typical with pretrained model). The following blog post has a worked example using the TensorFlow Keras API: "How to build a wide and deep model using Keras in TensorFlow". 