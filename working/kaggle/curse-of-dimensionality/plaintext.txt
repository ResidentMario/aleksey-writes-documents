The curse of dimensionality is one of the most important problems in multivariate machine learning. It appears in many different forms, but all of them have the same net form and source: the fact that points in high-dimensional space are highly sparse. The simplest possible illustrative example is in just one to two dimensions. Suppose we have two points on a line, 0 and 1. These two points are unit distance away from one another. Suppose we introduce a second axis of "data", again distributed a unit distance away. Now we have two points, $(0, 0)$ and $(1, 1)$. But the distance between the points has grown to $\sqrt{2}$! In three dimensions the two points will be $\sqrt{3}$ away, and so on. By the time we reach 10 dimensions, the two points are now a distance of 3 away from one another: thrice as far as they were when the data was 1-dimensional: More formally, consider a $p$-dimensional hypercube with unit volume. Suppose that we have $n$ data points uniformally distributed inside the hypercube. The volume (area) of a square is $e^2$, where $e$ is the edge length; the volume of a cube is $e^3$, and so on as we go up. Hence the volume of a $p$-dimensional hypercube is given by: Let $r$ be the ratio of points inside the cube which are within some neighborhood. To capture an $r$-full of points in the data, we need to grow a cube which takes up $r$ of the unit cube's volume. Since the length of an edge on the cube is simply 1, we have that: Expressed in terms of $e_p$, the edge length necessary to fill a $p$-hypercube, we have that: To capture 10% (0.1) of the samples in 2 dimensions, we need $e_2(.1) = (.1)^{1/2} = 0.31$. But in 10-d space we need $e_10(.1) = (.1)^{1/10}=0.8$, or 80 percent of the volume of the cube! The following graph illustrates this: There are two discernable effects here. One is that as $p$ grows, the chuck of the volume of the hypercube that we need to injest to get an $r$-full of points grows. Thus what starts out needing just 10% of the volume grows to needing almost 80% of the volume! The other effect is that this growth is uneven across the percentage $r$ we want to capture. For example, examine the gray line: to capture 80% of the points, we start out needing 80% of the volume and end up needing ~95% of it. This is more than we needed to start with, but a significantly slower rate of growth than the 10%-to-80% growth to get 10% of $r$! The effect this induces on the points is that we end up squashing them: not only are the points further away, but they also tend to get distributed more towards the fringes of the space. Almost like we took our points through a centrifuge. The practical consequence of these effects is widespread, and is something that crops up and causes different kinds of problems in different settings. A canonical example that's provided in Elements of Statistical Learning is what happens to nearest-neighbor classification. In nearest-neighbor classification, we assign test set data points the value given to the closest point in the training set. Now suppose that each point is given a value that is a function of its distance from 0 in the first dimension only. If we try this technique out with different numbers of dimensions, we get the following result:  The actual information in the dataset, the distance in the first dimension, is assigned by the nearest neighbor algorithm the same way in every one of these models. However, as the number of dimensions grows the points get further away from one another, and as a result the size of each neighborhod (the space of points closest to a point of interest) increases. The points being predicted are, on average, further away from the point being used the predict them. And, on average, the further away a point, the greater its first-dimensional distance is from the number it is being assigned by the classifer! As a result, even though the classifier is still fundamentally workable (the bias doesn't increase), it does a rapidly poorer job of generating good results (the variance increases greatly) (cf. Bias-Variance Tradeoff). (cf. this StackOverflow answer) This is one of countless examples where the curse of dimensionality crops up. It's important to keep in mind in the context of high-dimensional data. 