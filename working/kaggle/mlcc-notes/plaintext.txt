To diagnose convergence on a solution, graph the error per epoch (equivalently, the error after each period is over). (tangent on TensorFlow's system design) See the TensorFlow system design notebook. Some non-linear data relationships may be discovered and encoded in a way that can be taken advantage of in a model using feature engineering (particularly feature crossing). The choice of non-linear transform is known as the activation function. Hiding the details of backprop is the primary thing that TensorFlow achieves for us...from the perspective of a data scientist that is. An interesting approach to regularization with neural networks is dropout, which randomly drops a certain gradient contribution when performing backprop. Cute. Neural networks may perform multiclass assignment tasks using a one-versus-all scheme. (I don't fully get this, need to explore it further) When considering high cardinality data, like movies watched, the central concept is embedding in N-dimensional space. Use a dynamic model when there is significant trends and seasonality, e.g. flower buying recommendations or something. Use a static model when the data is relatively non-changing like e.g. image label prediction. Another important consideration is whether inferences are made offline or online. Realistically, you should use offline serving for initial prototypes and for cases with relatively static, and upgrade to online serving if there are significant gains to be made from having a tight loop like that. That's my opinion at least. Quick comment on correlations, easy enough. 