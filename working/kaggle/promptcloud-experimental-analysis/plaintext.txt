This notebook is an analysis of the results from the PromptCloud experiment. For further details on how and why this experiment was run, refer to the experiment design document. At the time at which this notebook was written, all of the PromptCloud datasets had long since fallen out of the hotness algorithm rankings, ending new user viewership on the datasets off of /datasets. The only new users seeing the content would be folks coming off of searching for it, which we expect to be a comparatively insubstantial number. Every dataset on Kaggle has a footer which shows the number of downloads and views that dataset has recieved. Because the period of highest activity has passed for the datasets in our study sample, this footer should be relatively accurate potrayals of how well they did overall. That's a good a place as any to start. Here are the numbers (taken in by hand): At the beginning of September Jacob put together the following chart, showing Views versus Downloads for the DPP team at the time: Looking purely at the distribution, it seems like some datasets are significantly intrinsically more highly successful/interesting than others. To understand why, let's continue to build on what we know. The greatest difficulty in designing the experiment was finding pairs of datasets which would perform similarly on the platform. This was not measurable until the data had run its course on the platform. Let's take a moment here to consider two kinds of dataset "interestingness": This worldview is the hypothesis we are testing in the PromptCloud experiment. Specifically, we believe that the presence of kernels on a dataset significantly increases how interesting that dataset is to our community. The ground truth on intrinsic interestingness is, all other things being equal (in particular, meaning no kernels being written on the dataset), the downloads-and-views performance of the dataset. Testing this hypothesis required choosing and working with pairs of datasets with approximately equivalent interestingness. This is a hard problem to solve: we have a limited number of datasets under consideration, and we do not known the ground truth on the matter ahead of time. I had to design the experimental groups going on gut instinct. The chart above shows the percentage difference in the number of downloads between two datasets in the same group. The high spread of the data points (from anywhere between 10% and 75%) demonstrates very high variance between tuples, which in turn is evidence of poor comparative intrinsic interestness in the dataset pairs. This about demonstrates the scale of the difficulty. The downloads delta even sometimes points the wrong way, with kernel-less datasets being more popular than ones with kernels. This is going to impact the interpretability of the results as a whole. Hence for key takeaways it's best to look at individual experimental pairs. Conclusion: eyeballing intrinsic dataset interestingness is extremely difficult. Given the level of experience we at DPP have with the platform at this point, this is a well-known difficulty. Now let's turn our attention to the experimental dataset pairs, and see where they landed overall. In the following summaries, A refers to the dataset with an EDA kernal, and B refers to datasets without. These datasets performed approximately equally in terms of views, but dataset B got twice as many downloads. However, dataset A got a bonus user-written kernel, while dataset B did not. Dataset A recieved 4.5 times as many views and downloads as B, strong evidence of an intrinsically poor pair. Both datasets recieved a single substantial user-written kernel each. Dataset A recieved 2 times as many views and downloads as B. However, only dataset B recieved a kernel. Interestingly, the dataset B kernel performed the same clean-up work that I performed in the dataset A EDA kernel. Dataset A outperformed dataset B by a factor of 2. Dataset A recieved a user-written kernel, while dataset B did not. Due to the extreme closeness of the type of data in this group (Indian hotels, booking websites, similar numbers of records) this group provides the best evidence that EDA kernels boost dataset performance. While dataset A outperformed dataset B by a factor of 2, neither dataset recieved a kernel. Dataset A outperformed dataset B by a factor of about 4; dataset A recieved a (weak) user-written kernel, dataset B recieved no activity. To dig further in, what's the deal with the lack of interpretability on views versus downloads? The more views or downloads a dataset gets in the given time window, the higher the views-to-downloads ratio. Restating this: the more interesting a dataset, the higher its views-to-downloads ratio. Furthermore, this relationship appears to be polynomial, not linear. Compare this result to an earlier visualization, generated by Jacob back in September:  This plot showed that the relationship between N(Views) and N(Downloads) was approximately ten-to-one, and that number has served as a useful rule of thumb for the team ever since then. 10 views per download is still a good rule of thumb, but a deeper analysis on this phenomenon is needed. For example, if this effect holds over the entire dataset lifecycle, it might be possible to "catch" popular datasets before they reach peak popularity. The experimental results show that our groupwise observation about downloads is probably an artifact of high variance. Investigating downloads would require further work. However: There is very weak evidence that writing an EDA kernel can up to double the number of views on a dataset. This conclusion is based on the second pairwise observation. The immediate follow-up I have is going to be using the queries I've written as part of this process to study dataset lifecycles. Here's an approximate order of approach: 