Every classifer applied to a dataset makes a fundamental trade-off between bias, the systematic error of the model (underfit), and variance, the amount of error from fitting overly well to the sample (overfit). Per the bias-variance tradeoff, these two sources of error are related; together they make up all of the error in the model. Hence, on a basal level, the task of a machine learner is to pick a model that minimizes these values. The best tool for finding what the bias-variance tradeoff of a model is is a learning curve. The x-axis on a learning curve is the number of observations provided to a model (e.g. the size of the training set). The y-axis on a learning curve is the amount of error in the model, according to some metric of your choosing. You then plot two curves on this graph: one for training scores, and one for cross-validation scores. Learning curves are great because the amount of progress the model makes as it gains more and more samples of data is a visual marker for how much bias and/or variance is inherent in the model:  In the first case, the model is systematically bad: it performs poorly on the metric no matter which split it is running on. This is an indication of an underfitted model, e.g. one that is not capturing the underlying pattern in the data. Note that it is up to you to determine what a "bad" metric score is! The second case is the best-case scenario. The model performs adequately well according to the metric, and adding more samples pushes the validation error towards the training error asymptotically (for at least part of the curve). The third case is one of high variance: the model is fitting the training set really well, but is fitting the validation set(s) poorly. This means that the model is overfitted, and needs regularization or tweaking in its hyperparameters to find a better fit. scikit-learn comes with learning curve tooling built-in. To demonstrate how to use it, I'll throw the sklearn learning_curve functionality against the metro area per-square-foot home values from the Zillow dataset. We'll use a simple LinearRegression model. This model will perform very poorly on the data. However, because least-squares regression is an unbiased estimator (given enough estimates, it will center itself on the true mean of the dataset; this is not the same as underfitting bias!), its $r^2$ score is asymptotically 0, even when the model itself is totally inappropriate for the data in question. This interesting fact is demonstrated by the plot that follows. Here is the learning_curve in action. We will use 5-fold cross validation (taking an average across all five folds as the validation erorr), and score with the r-squared score. What do we learn looking at this plot? An $r^2$ score of ~0 is awful, were you even trying to model this data? (no). Clearly this is a highly underfitted model! The validation score drops to near the train score at a sample size of around 400, and gets really close to it at a sample size of around 600. So the model has low variance. Learning curves are simple to generate and extremely useful during the model selection phase of a project. Hopefully this mini-tutorial will help you with your own model selection considerations! 