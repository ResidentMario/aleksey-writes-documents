In this notebook we'll use the NYC Building Sales dataset to explore what colinearity is, and how to measure it using a statistic known as the Variance Inflation Factor (VIF). Colinearity (or multicolinearity) is the phenomenon when two columns in a dataset are linear combinations of one another. In mathematical terms, column vectors $\vec{A}$ and $\vec{B}$ are perfectly colinear when there exists an $\alpha$ and a $\beta$ such that: For example, suppose our dataset contains two columns, "2015 Price" and "2016 Adjusted Price". The difference between these two columns might be that while the former is in terms of 2015 dollars, the latter is in terms of 2016 dollars (due to inflation). Since inflation is a constant multiple, these two columns would be perfectly colinear! Perfect colinearity will stop analytically solved ordinary least squares regressors cold, because it creates matrices without unique solutions in the math. In practice, however, perfectly colinear columns are rare. More often we will see partially colinear columns: for example, two different estimates of the odds of players winning in a tennis match. These two columns will probably be close to colinear because we expect bookies to estimate winners' odds very closely to one another. Colinearity is a special case of correlation. Two columns that are highly colinear are also highly correlated, in this specific, line-wise way. Colinearity causes problems with ordinary least squares regression. The reason why is intuitive. OLS is designed around building a line that minimizes the distance from points in n-dimensional space. If two of the dimensions in that space are very near one another, then the algorithm will bias its results towards satisfying these two variables more than satisfying the other variables, because it will get more "bang" for its buck. Importantly, from the perspective of a model fit metric, this is correct. The usual OLS metrics are also designed around the assumption that your regression isn't overfitting in this way! We will need to use a special fit statistic to determine when this is happening. VIF is that statistic. The Variance Inflation Factor, of VIF, is a way of estimating the severify of multicolinearity in your model. Suppose that we have an ordinary least squares coefficient matrix, $\beta$, a matrix of observations $X$ (as would be inputted into scikit-learn during training/prediction), a response vector $y$ (as would be inputted into scikit-learn during training), and a vector of predictions from our model $\hat{y}$ (as would be generated by scikit-learn during prediction) such that: ($\varepsilon$ is the error term) Then the variance of an individual coefficient $\beta_j$ can be estimated by the statistic: Where $R^2_j$ is the $R_2$ of the regression to $X_j$ of the remaining variables in the observation matrix $X$. The math for why things work this way is mildly complicated, and out of the scope of this brief tutorial. Basically, this formula tells us that the variance in our estimators (how different we can expect the coefficient to be, given a change or a reshuffling in the data) is related to the mean squared error (a regression fit metric we covered here), the number of samples in our dataset, the variance of the cofactor, and this second $R^2$-based term. Put another way, this formula tells us that: This second term is the variance inflation factor! Its position in this formula is why we call it that: because the VIF is proportional to the variance of our $B_j$ estimate, e.g.: And VIF is defined mathematically as: This is reasonably easy to implement. Sadly, scikit-learn doesn't have this built-in (though statsmodels does). To test out our algorithm (and our understanding of VIF), let's create an artificial dataset of columns generated from the same distribution. The following seaborn plot shows how all five of our input variables in $X$ are very close to linear with one another. As a rule of thumb, a VIF of >10 indicates extreme multicolinearity. And indeed, if we run our VIF algorithm on the dataset we've just generated, we find that's exactly what we have: Since our dataset has five variables, there are five VIF scores, one for each variable. Authorities differ (obviously) on how high the VIF score can get before you should start to worry about it. The following blog post mentions 2.5 as a cutoff value. Here's what a dataset with approximately this degree of multicolinearity looks like: Here are the VIF scores for this dataset: To interpret the VIF, it's helpful to restate it in terms of the more familiar $R^2$. $R^2$ is a measure of how well a model will do, given how well it did on the current dataset. Recall from the formula that VIF is just a between variables proportion to $R^2$. So a VIF score of 2.5 implies that: Another way to look at it is to say that a VIF of 2.5 implies that the variance of a particular coefficient is 150% larger than it would be if the predictor was completely uncorrelated with all other predictors. Since best-possible $R^2$ goes asymptotically to 1, best-possible VIF goes asymptotically to 0. The lower the VIF, the better a regression model will perform! VIF is a useful metric to compute ahead of time when you are planning on running a regression model. It can help stop you from creating poor models. If you want to run a regression model on a dataset with a high VIF, to maximize the utility of the model you will want to "fix" colinearity somehow. Examples of actions you could take are: You don't need to want to run a regression model to apply VIF to a dataset! Indeed, the VIF is a pretty useful general-purpose statistic. It can be used to determine how "bad" variables you want to model are when combined with one another. In this way it's like taking the correlation matrix of a dataset, except explicitly targeted at colinearity and with all variables under consideration, instead of general correlation. Let's go ahead and apply to some datasets. The NYC Building Sales dataset is record of one year of real estate transactions in New York City, as recorded by the NYC Department of Finance. In this dataset, there are a few variables that seem like they might be colinear: Let's see if they are: These variables are highly colinear! In fact: There exists some $\beta$ such that $\beta$ times the number of residential units is an about 80% accurate estimate of the number of total units. What about LAND SQUARE FEET and GROSS SQUARE FEET? These variables are not that highly colienar. So if we were to try to use regression to predict GROSS SQUARE FEET using LAND SQUARE FEET, we'd be only like 40% right. This is still a somewhat significant amount of correlation, so it's something that's useful to keep in mind, but fine for regression. That concludes this notebook. Hopefully you now know what a Variance Inflation Factor is, and how to use it, both in exploratory data analysis and when building your models! Until next time! 