Kalman filters are an advanced technique that provides an efficient way of computing the current state of a statistical process in a way that minimizes mean squared error. They are an example of a filter: a statistical technique for reducing the noise in a sequence of pointwise (or continuous; but pointwise in Kalman filters and in all practical applications) observations, generating a value which is close to the true value for some underlying statistical system. For a bit more on filters in general see this notebook. Kalman filters are one of the more advanced filter algorithms available, but they're also one of the most widespread. They played an important role in the computers used for the Apollo moon landing, and have found continued application in things like cleaning signals on instruments from self-driving cars and the like. This notebook contains my notes from "An Introduction to the Kalman Filter" and the corresponding scipy cookbook example. Mathematically speaking Kalman filters assume a discrete stochastic process of the form: Which is being measured by a $z_k$ such that: $w_k$ and $v_k$ are assumed to be the process noise and the measurement noise respectively, and are assumed to be normally distributed which mean zero and variance $Q$ and $R$, respectively: Kalman filters differenciate between an a priori estimate $\hat{x}^-$, and an a posteriori estimate, $\hat{x}^k$. The a priori estimate is the best guess the filter makes for a value before it is observed, whilst the the a posteriori estimate is its best guess with the additional information of $z_k$. These values have error terms and covariance terms: With all of these dynamics in place, here is the equation the Kalman filter uses to actually model the true value $\hat{x}_k$: The $z_k - H\hat{x}_k^-$ term is the residual: the difference between the observed value, $z_k$, and the value that the Kalman filter predicted in the absence of the observation, $H\hat{x}_k^-$. The term $K$ is an $n\times m$ matrix which is known as the gain. K is chosen such that it minimizes $P_k$â€”the a posteriori error covariance. K can be solved for analytically by performing a few equation substitutions, then setting $K=0$ and solving for its trace equalling 0 (e.g. finding a minimal point). This has a closed-form solution: This solution has the following revealing properties: In other words, as the measurement error covariance approaches 0, the residual is weighed less and less, and the measurement value $z_k$ is weighed more and more. Going the other way, the predictor $H \hat{x}_k^-$ is more trusted, and $z_k$ is less trusted. What this equation is essentially doing, then, is balancing how much trust it places in the observed (measured) value, versus how much trust it places in the modeled value! This is a big part of why Kalman filters have proven so useful. They incorporate a limited form of model accuracy feedback into their design, allowing them to adjust the relative merit of prediction versus observation in an online way. From a computational perspective Kalman filters are advantageous because they are recursive, and do not require recomputing all past information in order to generate an update. This is an advantage over many other filters, which are more computationally intensive. They remind me of recurrent neural networks in this regard, though Kalman filters are obviously orders of magnitude less complex computationally. Note that the implementation proceeds in two steps. First a time update is computed, resulting in a predicted value $z_k$. Next, a measurement update is computed, which corrects the $z_k$ value using the modeled value. The full runthrough is as follows:  The values $R$ and $Q$ are important to the model, and must be set prior to run. $R$ can generally be set by observing the phenomenon in an offline manner for a period prior to the introduction of the Kalman filter. $Q$ is a hyperparameter which must be tuned, as it cannot be observed directly. Interestingly enough the paper suggests using another Kalman filter to tune $Q$ in an online manner. In a neural learning context that's basically pretraining! The Kalman filter presented here is a linear one. Kalman filters may be extended more loosely to non-linear systems using a functional model for $x_k$: $x_k = f(x_{k-1}, u_{k-1}, w_{k-1}$. This form of the Kalman filter is much more ad hoc than the original linear Kalman filter because it doesn't follow probabilistic assumptions as closely (not that those matter). But the Kalman filter can perform reasonably well even when it is reasonably far from correct assumptions, as long the measurement error is relatively low, because it relies on the same sort-of-works-anyway Bayesian principles that power the naive Bayes classifier. For more on that including the derivation read the paper. Here is the demo, taken directly from the scipy cookbook demo: 