Linear Discriminant Analysis is a popular technique for performing dimensionality reduction on a dataset. Dimensionality reduction is the reduction of a dataset from $n$ variables to $k$ variables, where the $k$ variables are some combination of the $n$ variables that preserves or maximizes some useful property of the dataset. In the case of Linear Discriminant Analysis, the new variables are chosen (and the data reprojected) in a way that maximizes the linear separability of a certain set of classes in the underlying data. In other words, given a dataset with $n$ variables, including an embedded set of labels that we want to predict, we can apply LDA to the data and reduce it to $k$ components, where those components are chosen in such a way that they maximize our ability to "draw lines" to distinguish the classes. So for example, an ideal application of a two-component LDA reduction will look like this:  The technique was originally introduced by Fischer himself in the 1930s, making it one of the oldest techniques in the book. An LDA transform is useful as a preprocessing step when modeling classes because it transforms the space in such a way that algorithms which then go and draw those boundaries, like support vector machines, perform much better on the transformed data than on the original projections. However, it is also useful as an EDA technique. In this application, LDA can be compared to PCA. PCA is another dimensionality reduction technique (which I cover in detail in this notebook) that creates new variables that maximize the variance of the underlying dataset. As such, it functions in the absence of data labels (it is an unsupervised technique). LDA, meanwhile, is based on categorical labels, and creates new variables that maximize the linear distinguishability of the underlying dataset. PCA is an interesting technique to try out on a broad variety of datasets because it will teach us how the variables are interact with one another, finding axes in the dataset which make intrinsic sense and might potentially even be worth creating new features for. When we have meaningfully labelled data, LDA is another great technique to try out. Applying LDA to a dataset will tell us how linearly separable our dataset is (which, depending on the context, may also be a commentary on howmeaningful our labels are). That, in turn, is a good marker of how hard we will have to work to generate a model with reasonable classification performace. Furthermore, by looking at what variables load highly, we can determine what elements of the dataset are the strongest signals for each of our classes A mathematical sketch of LDA is as follows. Suppose that $f_k(x)$ is the conditional probability that a point with features $x$ is in the class $G=k$. Let $\pi_k$ is be the probability that any one of these classes will occur (e.g. so that $\sum_k \pi_k  = 1$). Baye's theorem holds that: Assume that all of the classes have a common covariance matrix ($\sum_k = \sum_k \forall k$) and that the classes are distributed as multivariate Gaussian distributions (e.g. each class of points is distributed normally across each of the $n$ variables). Then we may look at the log-odds of two classes ($k$ and $l$), and find that: What this all means mathematically is unimportant. What matters is that, due to the equal covariance matrix condition, the math simplifies to an equation linear in $x$ (the last line). Thus this is a statement that, given multivariate normal distributions with a common covariance matrix, the log-odds between any two classes is a linear function. That in turn implies that the decision boundaries between any $k$ and $l$ (the set $\{x \: | \: P(G=k|X=x) = P(G=l|X=x)\}$, e.g. the set of points such that the probability of a point being in either one of these classes is 50/50) is linear as well. Convexity in turn implies that the separators between the classes are hyperplanes. Hence the optimal solution for this classification problem will be: Which is a convex function of $x$, and hence can be proveably solved using gradient descent or related optimization algorithms. So for the given set of conditions (Gaussian distributions and equal covariance matrices), LDA remaps the data to an ideally linearly separable space. These conditions are rarely met in practice exactly, but many classification task datasets are "close enough" to separability that LDA is a useful technique to try out on them. In this notebook we will try out using LDA for exploring the Pokemon dataset. Our goal is to predict the type of Pokemon based only on its stat totals. Let's see what happens. In order to avoid stat munging due to Pokemon with combined types, we're going to focus only on Pokemon with a single type (e.g. no dual types allowed). Notice also that we're normalizing the data before applying LDA. Normalization isn't strictly necessary, as you will get the same result without it, just with differently sized numbers. I apply normalization as a preprocessing step here in order to make the class coefficients larger (and a bit easier on the eyes). Q: What is the deal with these priors? PCA provides a components_ attribute to the fitted reducer, which allows us to directly access the vector components. LDA does not provide this attribute. This is because in LDA, the methodology for transforming a vector is a bit more complicated than a simple w.T * x reprojection. An LDA instead provides a coef_ attribute, which is analogous, albeit more mathematically complicated. The magnitudes of the components in the coef_ tell us how heavily each of the features loads towards the seperability of that class. If a particular class has a particularly high-magnitude coefficient (direction, positive or negative, notwithstanding) then that variable signals that class very well. That variable will factor very heavily into the LDA preprojection. A low-magnitude coefficient, meanwhile, corresponds with a weak signal, and hence will be mostly rubbed out in the reprojection. If a class has mostly low-magnitude coefficients, that means that it is not easily linearly separable! That class is relatively close to the mean of the dataset or (in the weaker cases) relatively close to a subset of other classes in the dataset. The heatmap that follows demonstrates what this maps out to: In this heatmap we see classes which are probably easier to separate, given their large variable coefficients, as well as classes which are probably much harder. A good example of a separable class is the rock type. Rock type Pokemon load very strongly on all of Special Attack, Special Defense, Speed, and HP, implying that a combination of these stats makes up the Rock archetype. Other highly separable classes are the ghost type and the fighting type, which both sport some high-magnitude coefficients. ice and water are two classes that have very low class separability. They both have coefficients that are mostly close to 0. We can summarize this heatmap by looking at the absolute coefficient totals for each of the classes. Again, we see that rock and ghost is much more separable than water and ice. The y-values in both the heatmap and the bar plot can be treated as indicial. Higher is better, but the numbers themselves are not particularly interpretable (at least, my math isn't strong enough to interpret them immediately; feel free to think otherwise in the comments). To see what this difference translates to and to understand how well we perform overall, we need to move on to applying our LDA. To start with, as with any dimensionality reduction technique, it's important to note that each additional component used by the model adds less and less "gain" to the reconstructions. For example, here are the top three explained variances of the LDA decomposition: Recall that PCA picks values which maximize these values directly. LDA picks values that maximize the differences between classes, so the explained_variance_ratio_ will not correlate exactly with the usefulness of this or that particular vector. However, in practice LDA creates axes which are reasonably close to the axes created by PCA (maximizing explained variance and maximizing class separability are reasonably similar tasks). Hence the explained variance scores are still a useful meterstick, and sharp breaks in the amount of variance explained by each component is useful for "cutting off" how many components we will include in our dataset (as I demonstrated in the PCA notebook). This is a convoluted mess! What do we learn? We've learned that different classes of Pokemon are not very linearly distinguishable by stats alone. In summary this plot tells us that classifying Pokemon using stats alone is a non-linear problem. This in turn tells us that given the current set of features, predicting Pokemon type is a very hard classification problem. Most problems that are not linearly separable are very hard, unless (1) there are unusually complicated data dependencies like spiral structures or (2) we are able to gather additional (useful) features. Domain knowledge in Pokemon tells us that this is probably true, so in a sense this is something I already knew. In fact, I tested this very same concept using PCA and SVM in another notebook I wrote long ago; you should read that notebook to see the effect that such an attribute soup has on our attempts to classify our data! In this notebook I covered LDA (Linear Discriminant Analysis) in depth. LDA is a dimensionality reduction technique which is a first tool of choice when working with classification tasks. As an EDA technique it tells us a lot about the complexity of our problem, and tells us which classes are most easily distinguishable and why. As a preprocessing technique it improves model performace for any useful model we apply to the dataset afterwards, but particularly for linear ones, like SVMs. For more quantitatively oriented details on this technique, check out this excellent blog post. See also this Reddit post for the inspiration for this notebook. 