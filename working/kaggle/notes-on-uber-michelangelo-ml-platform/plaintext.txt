They point out the following specific workflow: Provide standard tools for building data pipeliens to generate feature and label data sets for training and feature-only data sets for predicting. From a design perspective, make it easy for users to adopt and follow best practices. In M there is a distinction between online and offline pipelines. Offline pipelines are batch prediction, online are near-real-time prediction. A bespoke data management layer provides a "Feature Store", based on the insight that many features are shared across teams and platforms. The offline pipelines are connected to an HDF5-based data lake. This is accessible to Spark and Hive compute jobs. Batch jobs run on a schedule or trigger, and are given intrinsic access to data quality monitoring tooling to perform that part of their job. The online pipelines cannot use the HDF5 data lake because it is too slow. A DSL is used for feature extraction and transformation (bleh). Large-scale distributed offline training of various algorithms is provided (very similar to e.g. AWS SageMaker). Versioned model objects and metrics reports are all perma-stored. Model deployments that are offline are managed via a Spark job. Model deployments which are online are spun on a variety of machines behind a load balancer and comm via RPC messages. Model upgrading is handled in an interesting way. Assuming models have the same call surface, a new model can be inserted in place of an old one immediately (presumably the container is blocked for a while during the upgrade procedure). This is in contrast to e.g. what Kubernetes does, where it spins up a new node instead. You can also run two models side-by-side in a single container image. Probably better to do this the Kubernetes way though... Machine learning models are stateless and share-nothing, making them trivially parallelizable. This post talks through M PyML, a library Uber wrote that allows you to run Python code -based ML scripts (just like SageMaker custom jobs). PyML has four components: model.py (required), requirements.txt (optional), packages.txt (optional), and setup.py (optional). Compare to SageMaker, which is less opinionated (seems better in that regard). A Client object is used to package the model artifact and dependencies, turn it into a Docker container, and send it to the Uber registry. Architecturally, the same Docker image with its same standard interface is used for both offline and online prediction services:  In the offline case, Hive is both the input and output. PyML supports two input types: DataFrames and Tensors. This is based on the observation that most of the time data scientists want one or the other. Again, note that SageMaker makes no such gaurantees. On reproducibility: As part of the PyML model production deployment process, we version and manage the model artifacts. However, PyML currently does not version the artifacts and pipelines that lead to the model artifacts in the first place. This includes the data that was used to train the model, the applied transformations, and, of course, the training script. In the future, we will give our users the option to version a PyML model’s training code as well as its source data together with the model artifacts, allowing users at the company to reproduce a PyML model from scratch. Future directions include tighter integration with the Feature Store and integration with Uber's A/B testing platform. Widely varying requirements for ML problems and limited expert resources make organizational design particularly important—and challenging—for machine learning. While some ML projects at Uber are owned by teams with multiple ML engineers and data scientists, others are owned by teams with little to no technical expertise. Similarly, some problems can be solved by novices with commonly available out-of-the-box algorithms, while other problems require expert investigation with advanced techniques (and often don’t have known solutions). Getting the right people working on the right problems has been critical to building high quality solutions and deploying them consistently and successfully in production. The challenge is in allocating scarce expert resources and amplifying their impact across many different ML problems. For example, if a new project requires computer vision know-how, what organizational structure will allow Uber to effectively allocate expert resources in a way that is aligned with company priorities? A separate ML platform team (data engineering, hint hint) manages the actual tooling. Product teams own model launch processes, due to the usual mega-corporate risks (legal et al). Of course in a more normal-sized company this would be highly inefficient. Office hours and educational efforts (a recurring theme with these large tech companies, see e.g. AirBnB). The article restates its own brief version of "The Machine Learning Workflow" article flow. "ML as software engineering" subheading. Summarizing key lessons learned: 