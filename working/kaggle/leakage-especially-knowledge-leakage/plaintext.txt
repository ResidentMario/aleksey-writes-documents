Leakage is one of the scariest things in machine learning (particularly competitions). Leakage makes your models look good, until you put them into production and realize that they're actually roundly terrible. To quote the Kaggle wiki entry on the subject: Data Leakage is the creation of unexpected additional information in the training data, allowing a model or machine learning algorithm to make unrealistically good predictions. Leakage is a pervasive challenge in applied machine learning, causing models to over-represent their generalization error and often rendering them useless in the real world. It can caused by human or mechanical error, and can be intentional or unintentional in both cases. Leakage is particularly bad because it invalidates or weakens cross validation scoring. The accuracy of cross validation as a prediction for how well our model will do in validation or on production data is incredibly important; so much so that it's often said that "above all, trust your CV". If we undermine that, we undermine most of the tools and techniques in our toolbox! The most obvious form of leakage is when a variable in a dataset is derived from the target variable in some way. For example, if we are predicting annual_gdp, a column with GDP in 2016 dollars, standardized_gdp, would be an example of a leak, because it's just the same data transformed a little bit. In order to build a real model and not a linear transform, we would need to remove this column from our model entirely. Again from the Kaggle wiki: One concrete example we've seen occurred in a prostrate cancer dataset. Hidden among hundreds of variables in the training data was a variable named PROSSURG. It turned out this represented whether the patient had received prostate surgery, an incredibly predictive but out-of-scope value. The resulting model was highly predictive of whether the patient had prostate cancer but was useless for making predictions on new patients. With some practice working with and inspecting machine learning features, this kind of "variable leak" is catchable, but it becomes tedious when the feature matrix has enough predictors in it. Domain knowledge helps a ton here. Leakage is the number one problem in machine learning competitions because it can be weaponized by model-makers in a way that would never make sense in a production system. This is "out-of-core leakage". For an example of what this looks like, see this old Kaggle post explaining why one leak caused a competition identifying right whales to be reset. They're very challenging to catch because even experienced competition-runners (like the Kaggle team) can't match the time and depth competitors can bring to probing datasets for weaknesses. Which brings us to knowledge leakage, which is what I want to cover in more depth in this notebook. I'll actually just be going over the information presented in this fantastic blog post on the subject, so you should probably read that first. To guard against overfitting, machine learning relies heavily on cross validation and related holdout and parameter search schemes. The effectiveness of the technique relies on our building a model on a training data, then testing it for fitness on training data that it's never seen before. This is only an effective technique if we can prevent information about our test data from leaking into our training data. In theory this is easy: just don't use observations from the test data in the training data. However, there are things we can do during the pre-processing before we train a model that injects information about our test data into the training process! Doing this will increase our cross validation accuracy on the data we train on, but will worsen our accuracy in practice on validation or production data. Let's demo how this can happen (NB: we're reimplementing the blog post code here; some things have changed in the library in the meanwhile however, so this code is a little different from that which originally ran). We'll build a $100 \times 10000$ feature matrix: that is, 100 observations across 10000 synthetic features. This is a massively overdetermined feature matrix. Then we'll perform feature selection: we'll measure the correlation of each of the columns with the target column, and take the top two scorers as our model inputs. We'll train on those, and measure what our mean squared error (MSE) is (for more on model fit metrics click here). Our mean squared error is pretty good, and we trust our CV, so we think this is a result reflective of practical performace. However, is it really? Can you spot the error? It's subtle. The reason we picked a matrix with so many features is because it accentuates the error we've made with the procedure here. By measuring the correlation of the columns and taking the two highest scorers before doing cross validation, we actually injected incidental information about which variables are most highly correlated in both the train and test sets. Hence when we run the cross validation, we've "pre-selected" incidental correlation that we know beforehand performs well in the test set. We picked a lot of variables to make this effect easily noticable (with 10000 variables, some of them are going to end up quite correlated with the target). We can see how strong of an effect this creates by doing this same variable selection after a train-test split: It looks like knowledge leaking almost halved our mean squared error! The correct approach to dealing with this problem is to think harder about how we will structure our pipeline. Best-fit variable selection like this should live inside of our cross validation; that is, it should only be done after we've already done train-test splitting. This will at least give us a more realistic index on performance: Knowledge leakage is a difficult problem to address completely. The one thing I recommend doing to avoid this problem is being conscientious about using pipelines, like the one scikit-learn provides, to hande pre-processing and training as one contiguous unit (the scikit-learn user guide in fact lists "safety" in this regard as one of the three reasons to use pipelining). For small to moderately-sized datasets, I do not think that knowledge leakage is a huge problem. Pipelining over feature selection has its own problems (it introduces overfitting into cross validation?). The amount of error you introduce into your model via knowledge leaking is relatively small: maybe even a rounding error on your overall model accuracy. However, it becomes a problem when there are lots of variables, especially when the feature matrix is overdetermined (more variables than observations). In these cases you do want to be careful about how you design your pre-processing. When in doubt, I recommend running an exercise like the one I demonstrated here on your dataset. See how much of a difference knowledge leaking makes for a dataset shaped like yours! 