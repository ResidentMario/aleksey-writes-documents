The transformer decoder is a natural fit for the word embedding learning task because it works backwards only: e.g. the token at a certain position in the sentence only has access to the previous tokens. This is considered a good thing because it's been shown, in practice, that a sufficiently complex network that naively includes the context of the posterior sequence of words in determining a word suffers from target leakage (see "Leakage, especially knowledge leakage"). AFAICT, this has to do with the fact that the task asked of the model is "predict the next word in the sequence", not "predict the next word in the sequence given all of the words that come both before and after it". "Everybody knows bidirectional conditioning would allow each word to indirectly see itself in a multi-layered context." ELMO has access to both prior and posterior information. However, it uses a bidirectional LSTM, which is a weaker context transfer learner (given a sufficiently long sequence) than self-attention is. This is a sort of implicit regularization built into ELMO that BERT, with its over-eager learner, doesn't have. BERT gets around this by modifying the task. Whereas GPT-2 learns on the "predict next" task directly, BERT learns on the task "learn the word in a sentence in which 15% of the words are masked out". The masking is a form of regularization; it withholds just enough at preventing the algorithm from cheating through rote memorization. It will also rarely replace a word with a different (probably wrong) word (basically reverse teacher forcing?). 