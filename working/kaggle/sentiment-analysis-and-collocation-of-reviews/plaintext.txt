In this notebook we apply two techniques to the reviews for the Boston-area AirBnBs in our dataset: sentiment analysis and collocation. First we note the highly skewed distribution of reviews on the Internet: many many positives, not many negatives. This holds just as true on AirBnB as everywhere else. There's an XKCD for this. Ok, let's try out sentiment analysis. Sentiment analysis is a technique in natural language processing which aims to retrieve the "sentiment" of a piece of textâ€”positive, negative, or neutral. This is an easy way of summarizing the contents of a piece of text, and one that is easily understood. Note, however, that sentiment analysis is a difficult problem. Humans agree on the sentiment of sentences only 80% of the time, and the best classifiers can get around that level of accuracy, but we're going to just use a built-in analyzer in the nltk (natural language toolkit) Python library. So I don't expect our results to be astonishingly good, but let's see what we get... Our reviews contain both null reviews and reviews in other languages. langdetect makes this trivially easy, but it doesn't install on Kaggle for whatever reason. nltk can do this too, but for whatever reason it doesn't have a built-in for it. We'll use the following bit of code to filter out non English-language reviews, borrowed from elsewhere: Generate our scores. How do we score on... Neutrality Positivity Negativity. Almost none of the texts are classified as having significant amounts of negativity! In fact, a significant amount of them are given exactly 0.0 negativity. These charts tell us about the characteristics of the off-the-shelf sentiment classifier that we are used and its performance on our dataset. Although the compound score is supposed to be the best estimate of overall sentiment (not shown in the charts above), the fact that negativities are ranked so lowly hints that we're doing a not so great job with this. Let's look at our positive-negativity reviews. A lot of these aren't negative at all. Here are two that are: Some more fiddling with queries... Here's an example of the kind of (funny, sarcastic) review that seriously trips our classifier up: Here's another one. In this text's case even though we would say the sentiment with regards to the lister is positive, the sentiment of the overall paragraph is negative because of the renter's unfortunate experience with food poisoning, being "horrendously sick", etc. This is a limitation inherent in all sentiment classification tasks. The best way to get around this is to use a technique called chunking to extract what sentiment is attached to what thing in the text, but that gets complicated very quickly. Here are two more bad reviews because why not: We'll actually stop here. It's pretty clear that our sentiment analyzer is not doing a good enough job separating the wheat from the chaff to use our results for anything! That's unfortunate, but understandable. There's a number of pre-processing techniques that we could apply to our dataset to make our sentiment analyzer work better (Google it!). We could also try a different sentiment analyzer (like the IBM or HP ones, available via API), particularly one perhaps better suited for the "Internet reviews" domain, and see if that would get better results. According to Wikipedia "a collocation is a sequence of words or terms that co-occur more often than would be expected by chance." What we want to attempt now is to use nltk to find collocations which have a high amount of importance in the text, and we'd like to take and display them as "summaries" of our texts. How do we tell when a particular combination of words is important? One way of doing it is look at those word's pointwise mutual information.  This is a metric which attached significance to words which appear next to one another in the text, for whom such co-occurrences are far-above-averagely-common, and which are otherwise rarely used in the language. According to this metric, for example, the words "puerto" and "rico" have a very high PMI, while the words "to" and "in" have a very low one. If you use Yelp! a lot you are probably familiar with Yelp's so-called review highlights. These kick in after a location has had a certain reasonably large amount of reviews written, and show, by default, snippets of three reviews mentioning a combination of words which appears especially often in reviews for the location. Here's an example of these highlights in action. An answer on StackOverflow says that these highlights are probably implemented using precisely the techniques spoken about above. What we're going to now try and do is replicate Yelp! review highlights with AirBnB review highlights! Let's try and find interesting word combinations for an example listing, just to see if it's possible. In this case we're picking an ID with 200 reviews to it, a substantial number which should hopefully let us mine good subject commonalities between them. Note that in this case our "combinations of words" means bigrams: pairs of two words which appear right next to each other in the text. This can be extended to n-grams of arbitrary size, if you're so inclined, and Yelp! uses n-gram sizes between 1 and 3, but for simplicity's sake we're going to stick to bigrams (2-grams) here. Ok great. How many reviews do we have to work with per location? To process the words we're going to use a BigramCollocationFinder, which expects all of the text from our reviews, tokenized into individual words, as input. To do that we're going to use the nltk word_tokenize method on the words, then run a couple of maps on the result to tweak a couple of things: remove punctionation marks and recombine contractions that word_tokenize splits up (word_tokenize will render didn't as ["did", "n't"] for example, which we don't want. Ok great! Let's see how many words we're working with for each of our reviews. There's going to be some sort of cut-off in terms of the number of words that, were we to use this result in production, we would need to find. Yelp! seems to put that cutoff at 20 or so reviews; below that there's not enough information for highlighting to work. Not knowing any more about how they do things, we're just going to apply our collocation finder to all of the review texts. First we're going to retrieve a list of bigrams that appear in the review text at least three times. Then we'll pick the three "best" bigrams, where "best" means the large PMI. That's what the function below does. Let's see what our results look like! Not bad! It could definitely use improvement, but we're already seeing some interesting topics recur here. Let's generate "Yelp! style" top-level highlights and print them to see what we get. For reference I'll also provide listing URLs. The basic nltk sentiment analysis built-in did not do a good job analyzing the sentiments in our sample of AirBnB reviews. Without knowing more details about how the classifier was trained (there is a paper you can read FYI) I can't say for sure why that is, exactly, but it's nevertheless an interesting limitation to keep in mind, as most Internet review texts are going to be pretty similar to the AirBnB one. Perhaps other analyzers would do a better job. Collocation with nltk, on the other hand, worked brilliantly! It turns out to be something that's pretty easy to do but which generates reasonably good results with just a little bit of elbow grease. You can apply this technique to just about about any reservoir of review texts out there, so keep it in mind because it's a useful tool to have under your belt! 