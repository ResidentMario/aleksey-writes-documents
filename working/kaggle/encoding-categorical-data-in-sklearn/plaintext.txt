scikit-learn, pandas, and numpy all come with various utilities for transforming data into categorical types. I personally am particularly fond of pandas.get_dummies. This notebook explores category_encoders, a scikit-contrib module specifically dedicated to categorical encoders, which is the recommended way of applying more complicated categorical transforms on your data. It also comes with a host of features useful for advanced use cases. This is that notebook. Naturally one might ask: what is the point of categorical encoding? When storing data, categorical encoding can be useful as a technique for reducing dataset size on disk (via dictionary encoding). However, the primary usefulness of categorical encoding, and the reason that it is such a well-developed set of techniques within classical statistics, is that it can be used to improve the explanatory power of statistical models. This is done by transforming input data into an output representation that is more ammenable to modeling. Encoders have many of the same advantages and disadvantages as the techniques of dimensionality reduction overall. They make data easier to model, but also potentially lose information which can be taken advantage of by more complex models, like neural networks. For this reason they are not used very often outside of regression settings. One-hot encoding is the simplest type of categorical encoding. It takes each unique class in a list of class observations, and turns that into a True/False matrix where each unique class has its own column and its own binary value. This is the most common encoding used for e.g. preprocessing boolean data as input to a keras neural network. If you want to carry over the category name, use use_cat_names parameter. Otherwise the category index mapping will be used; this is what was done above! Ordinal encoding transforms a column of class names into a column of class integers. Ordinal encoding scheme maps columns to integers randomly, unless you give it a mapping yourself, in which case it will use your mapping instead. This encoding is called ordinal because the output values are ordinal (e.g. sortable categorical), even if the input values are not. The two transformers we have seen so far have been very simple, and have the property that they are 1:1, e.g. every single input category is represented by a single output category. The rest of the transformers available in the library are more sophisticated, and may choose or choose not to merge particular categories together based on statistical tests that are applied to the dataset at fitting time. The idea here is that by choosing a categorical encoding that is appropriate for the given dataset and what we want to do with it, we may have the benefits of categorization (e.g. dictionary-mapped encoding) whilst still preserving those properties of the dataset which are analytically valuable. A bit of a digression into statistics... A contrast is a linear combination of variables whose coefficients add up to zero. In other words, the quantity $\sum_{i=1}^t a_i\theta_i$ is a contrast iff $\sum_{i=1}^t a_i = 0$. An example of a case in which this property holds is the difference of two weighted means. A constrast is furthermore orthogonal if it has the additional property that $\sum_{i=1}^t a_i b_I = 0$. Each target or source category is known as a level. Many of the more advanced encoding schemes available in this library are what are known as contrast coders. They are formulated using a well-defined contrast matrix, and may or may not also be orthogonal. PolynomialEncoder takes categorical values are input: And outputs adjusted categorical values as output: These values are chosen by fitting a polynomial regressor to the data, solving for the linear/quadratic/cubic coefficients that minimizes the adjusted $r^2$ (or some other test statistic) of the regression, and replacing the original categorical values with the new coefficient values. Some explanations of words: For algorithmic purposes, the coefficients chosen are subject to the constraint that they form an orthogonal contrast. In this example case this looks thusly: Polynomial regression is a good encoding scheme if you wish to apply a parametric model (particularly a regression model) to the data afterwards. However, it can only be used for ordinal categorical variables. Helmert coding is a contrast coding scheme that encodes every category as the difference between the mean value of that category and the mean of the mean values of the groups that follow. For example, suppose that there are four categorical values $\mu_n$. After applying Helmert encoding, the new $\mu_1$ level would be a function of its own mean and the mean of the mean observed values of $\mu_2 \ldots \mu_4$. The new $\mu_2$ level would be a function of its own mean and the mean of the mean observed values of $\mu_3$ and $\mu_4$. And so on. Helmert coding is a good fit for scenarios where there are significant "stacked" differences between groups in the data. See here for an extended mathematical explanation. Binary encoding is similar to one-hot, except it outputs to a bytestring. The above are all examples of dictionary encoders, e.g. encoders that rely on a dictionary mapping from the original value to the new value. A hashing encoder maps values using a one-way hashing function instead. This means that this encoder doesn't save on space as much when the arity is low, like the others do, but also means that there is no dictionary the opposite is true when the arity is high. BaseN encodes the column in base-N. When arity > n, more than one column will be generated, with each column representing one digit of the output representation. When arity < n, there will be just one column, so the representation is equivalent to ordinal encoding. When n = 1 this encoding is equivalent to one-hot encoding. An important point about base-N is that when a column gets broken up into multiple columns, the information that was previously encoded in a single column is now "broken" up across two (or more). This is a form of induced non-linearity, and many models cannot take advantage of information split across a column like this. Decision trees are a good example of a type of model that does not care, and may even benefit from this split, because it uses splitting rules. Support vector machines are also very good at overcoming this shortcoming, I suspect. But old-school regression models will suffer. Backwards difference is another contrast coding scheme. In backwards difference coding, the mean of the dependent variable (y) for a level is compared with the mean of the dependent variables (X) for the immediately previous level. Backwards difference coding can be contrasted with Helmert coding. Helmert coding constructs new values using the mean of the remaining values (there is also backwards Helmert coding, which does the same with the means of the previous values), whilst backwards difference does so with just one class at a time. Backwards difference coding is therefore an attractive option for the encoding of data that has pairwise relationships between adjacent values, because this pairwise information will be preserved in the choice of the output value. Target encoding is a supervised categorical encoding technique (e.g. unlike certain other encoders, it uses information from your target variable y). In the case that the target variable is categorical, features are replaced with a blend of posterior probability of the target given particular categorical value and the prior probability of the target over all the training data. The first element, Posterior probability is the probability of output observation $y$ given input data $X$, e.g. $p(y|X)$. The second element, prior probability of the target over all the training data, is the probability of $y$ occurring at all: $p(y)$. Why do we blend these two variables? In small datasets, there are very few observations of $y|X$, so our estimate of $P(y|X)$ is unreliable. For example, consider the following dataset: In this dataset there are no observations of y=True|X=10, so $P(y|X)$ is completely undefined! It is easy to imagine another, slightly larger dataset where there are such observations, but very few of them, which makes our estimate of this quantity unreliable. In either case, a "safer" estimate for $P(y|X)$ is $P(y)$. If on the other hand we have a lot of observations like these, we have a lot of condifence that our estimate is reliable, so this isn't appropriate to do. Target encoding uses a weighted mean of these two values internally, emphasizing the second value when there is little support and the first when there is a lot of it. If the target column is continuous (or more accurately, near-continuous) instead, posterior and base probability is substitued for expected value. The same principles and rationale applies. Target encoding is generally considered to be a pretty good encoding scheme because the values it maps to are weighted averages of the target value. Mapping categorical values close to their expected value in this way is helpful in both probabilistic algorithms, like naive Bayes, as well as in regression. For this reason it is probably the second most common encoding scheme of the ones in this post, after one-hot. A good reference on target encoding is this blog post. Leave-one-out is a very slight modification on target encoding. It makes the change that the current row's observation is excluded from consideration when calculating $P(y|X)$ and/or $P(y)$. This very slightly reduces the amount of information that the target encoder has to work with, but also increases the robustness of coding scheme against outliers. This is a similar technique at least in name to leave-one-out cross validation, which is a similar modification on k-fold cross validation. CatBoost is an categorical encoder taken from the CatBoost gradient boosted machine library which performs leave-one-out encoding "on the fly". I'm not entirely sure what is intended by "on the fly", but I suspect it means that the value assigned to each observation only uses the set of observations observed so far. This would cause some noise in the earlier parts of the dataset, but also makes the algorithm streaming, as you don't have to load (or even be able to fit) the entirety of the dataset into memory. Of course in the case of category_encoders, you're using a DataFrame, so it pretty much has to be in memory anyway... Because this algorithm is streaming, it's important that the data be randomly permuted, e.g. that there is no information about the target variable in the sort order of the DataFrame. M-estimate is a simplified version of a target encoder with one tunable parameter (m) instead of two (min_samples_leaf and smoothing). It has certain statistical implications that I didn't really dig into. Sum coding is reversed backwards difference contrast coding. Instead of using the difference of factor levels, it uses their sums. I have little information beyond that. Weight of evidence is a statistical measure of the level of (binary) seperation between two groups under certain conditions. It evolved out of applications of logistic regression to credit default risk. Given that $E$ is a boolean event, e.g. something that happened, and $\neg E$ is something that did not happen, the WOE formula is: Taking the logorithm of a (positive) value has the effect of squashing that value down, e.g. from a potentially very large number to something more reasonable. It's easy to see how this value came to be useful in credit default settings, since in this setting the failure or defect rate is the default rate, which is a rare event, order of 1/1000 or rarer. It's also attractive if you're using logistic regression because look, a logarithm! The weight of evidence encoder stars by dividing the categorical intervals into many different buckets. It then calculates the weight of evidence for each bucket. Buckets that are located next to one another contiguously and also have close weight of evidence values are then categorically combined, until only buckets that have relatively distinct weight of evidence values remain. Finally, the category level value is replaced with the weight of evidence value. Weight of evidence encoding is akin to choosing "good splits" in a histogram. It achieves the following two things: It has the following limitation that it requires a binary target variable to work. A good reference on weight of evidence is this blog post. The James-Stein encoder is the target encoder with an enhanced formula for estimating $\text{mean}(y_i)$, the canonical James-Stein equation. I didn't dig into this too much, but the category_encoders documentation has way more detail about this encoder than the other ones, and is a good read: http://contrib.scikit-learn.org/categorical-encoding/jamesstein.html. 