The linear activation function is the simplest activation function. It performs a skew transform of the original dataset, and so cannot be used to discover or incorporate features which do not have a linear relationship with the data values. I cover linear activation in more detail here. Non-linear activation functions come in many shapes. The most general class of non-linear activation function is so-called sigmoid functions, which are distinguishable by having an S-shaped value curve. The most well-known sigmoid function is the logistic function: The logistic function is generally useful enough and important enough from a theoretical standpoint that it is sometimes just called the sigmoid function. This is what keras calls it, for example. Some notes on what makes the logistic function interesting and what properties it has are located here, this is just a refresher. Notice that convergence using the sigmoid function took much longer than convergence using the linear function on this toy dataset. We can expect this to be true in general because the slope of the sigmoid activation functions near zero is always going to be much smaller than the slope of a linear activation function near the same point. An alternative to the logistic function is arctan or "tanh". This curve has different properties: tanh and sigmoid are actually scale transformations of one another. They have the relationship that if $g(x) = \frac{e^x}{1 + e^x}$ is the logistic function, $tanh(x) = 2g(2x) - 1$. Hence they address the same space of possible outcomes, and so will have the same (tuned) performance. In practice, tanh has some minor advantages and disadvantages that are more architectural. tanh has a steeper slope than sigmoid, which means that it will iterate (and perhaps converge) more quickly when the gradient is near-zero. tanh is centered on 0 instead of the sigmoid 0.5, and has the range $(-1, 1)$ instead of $(0, 1)$. This means that it is a more "natural" fit for target values in the range $(-1, 1)$, as these variables may be inserted into a tanh layer without intermediate computation. On the other hand, values that are in the range $(0, 1)$ are a better fit for sigmoid. Hard sigmoid is just a sigmoid activation function which clips input values that are outside the range $(-2.5, 2.5)$ to be 0 and 1, respectively. If the input data is a scaled normal distribution, this corresponds with 2.5 standard deviations from the mean, e.g. values that are ~99% unlikely. Hard sigmoid is otherwise the same as regular sigmoid. Unfortunately keras does not provide hard tanh, which is also a thing. You can always perform that clipping yourself however. Covered here. The rectified linear function is a piecewise function which staples together a flat and a linear activation function. It is non-linear, but very much not sigmoid: ReLU is an odd-looking activation function, but it's more or less the state of the art for today's neural networks because its design addresses an important problem in deep learning: the vanishing and exploding gradient problem. If you examine e.g. the logistic activation function you will see that the slope of the function is very small towards the edges of the curve, and very large towards the middle of the curve. As a result, when improvement in the performance of the model depend on values that are far away from the mean, the steps the learner takes to gradient descend become very small, as the gradient on that part of the curve is very small. This results in an asymptotically decreasing step size, requiring significantly more epochs to reach a final state. Similarly, when improvement in the model depends on values very near the middle of the curve, the step size becomes very large. Large step sizes cause a different kind of problem: they may cause the model to jump pass the optimal point and end up on a similar gradient facing the other direction. In the next step, the learner will spend most of its step energy countering this effect, ending up near the first point...and the process will continue to repeat across backpropogation passes. This can cause your learner to get "stuck", greatly slowing down descent because most of the energy of the descent gets sunk into jumping backwards and forwards past the point of optimality. There are a lot of different ways of addressing these problems. For example, you could tune the learning rate in response to bad behavior in the model: make it larger when gradients are small, and smaller when gradients are large. But one way is to use an activation function which is less vulnerable to this problem, one that doesn't have small or large gradients on its curve. ReLU provides this. No matter where we are on the curve, so long as the input value is greater than 0 we are moving with a constant gradient of 1. The tradeoff that ReLU creates is that whenver the input value is less than 0, no weight update is performed. This induces sparsity in the model. With large neural networks using ReLU activations, it's common for as much as half of the nodes in the network to reach a fully zero gradient. In other words, these nodes are always fed values that are less than zero by their predacessor nodes, so they perform no weight update and just pass the value along to the next nodes they are connected to. Sparsity is a double-edged sword. One the one hand, sparse learning is considered A Good Thing, as it has a lot of nice properties and implications for the output model: Information disentangling. One of the claimed objectives of deep learning algorithms (Bengio,2009) is to disentangle the factors explaining the variations in the data. A dense representation is highly entangled because almost any change in the input modifies most of the entries in the representation vector. Instead, if a representation is both sparse and robust to small input changes, the set of non-zero features is almost always roughly conserved by small changes of the input. Efficient variable-size representation. Different inputs may contain different amounts of information and would be more conveniently represented using a variable-size data-structure, which is common in computer representations of information. Varying the number of active neurons allows a model to control the effective dimensionality of the representation for a given input and the required precision. Linear separability. Sparse representations are also more likely to be linearly separable, or more easily separable with less non-linear machinery, simply because the information is represented in a high-dimensional space. Besides, this can reflect the original data format. In text-related applications for instance, the original raw data is already very sparse. Distributed but sparse. Dense distributed representations are the richest representations, being potentially exponentially more efficient than purely local ones (Bengio, 2009). Sparse representationsâ€™ efficiency is still exponentially greater, with the power of the exponent being the number of non-zero features. They may represent a good trade-off with respect to the above criteria. (taken from a paper linked to by this SO answer) On the other hand, a neuron once zeroed cannot be unzeroed. If we take too large a step in the negative direction during training, we may end up with a zeroed-out node by accident. For example, suppose we reach a local minimum in the cost optimization surface which places a particular nodal input precariously close to 0. We then make a gradient jump that passes the net-zero point; the node is now permanently deadened. When we eventually converge to a global minimum, we may find that if that node that we deadened wasn't unresponsive, the additional information in this new context could actually have been used to improve the performance of our model. Instead, because the node was deadened prematurely, we are left with a suboptimal solution. This is known as the dying ReLU problem. Because of the dying ReLU problem, ReLU neural layers require more skill and thought to train than simpler model layers! Leaky ReLU is an attempt at rectifying the dying ReLU problem by adapting the function shape. The shape is the same, but instead of using 0 on the negative part of the curve, the Leaky ReLU uses a very small positive value: The Leaky ReLU doesn't have the sparsity properties of the ReLU, but it provides an "escape hatch" which, given enough training time, will enable models to reactivate deadened nodes and start learning on them again. In practice, Leaky ReLU has been shown to work very well for certain kinds of problems, but to be very inconsistent overall. Softplus (not shown here) is a smooth function that looks much like ReLU, but which has the advantage of full differenciability. ReLU, invented later, has displaced softplus in deep networks however, as emperical evidence however has shown that having a differenciable surface doesn't win you anything in practice. Thus softplus has fallen out of use in favor of ReLU and Leaky ReLU, which also happen to be faster to train (because their gradient is so easy to compute). 