Suppose that we are interested in better understanding the relationship between the number of units in buildings in New York City and their general size. How well can we predict the number of units in a building in Manhattan, on average, using variables like BldgArea, NumFloors, and AssessLand? To find out, let's apply a fairly sophisticated and very well-loved algorithm to the problem: the elastic net. Elastic net is a combination of two modifications on the basic kind of machine learning model possible, ordinary least squares regression. In OLS, we build a model which minimizes the sum of the squared distances between the model predictions and the data actuality on a line (the residuals). This model is not very robust against colinearity, however, and colinearity tends to occur in lots of different real-world datasets. Hence the need to "fix" this bias somehow. The two most popular ways of addressing this problem are ridge regression and lasso regression. Both of these algorithms work by adding a second term, or penalty, to the cost minimized by the algorithm In the ridge case, the penalty added is what is called an L2 norm; it is on the order of the square of the residuals. In lasso regression the penalty is an L1 norm, an absolute value on the order of the residuals themselves. Because these two algorithms work in different ways, they have fundamentally different properties. Ridge regression basically works by smoothing out a curve, removing its biases towards outliers and towards data with the same information in multiple dimensions. Lasso regression works by both manipulating the coefficients of the variables and by removing variables from the model entirely! In other words, lasso regression implicitly performs model selection, which ridge regression does not do. Both algorithms rely on a "strength of correction" hyperparameter, $\rho$ (NB: $\lambda$ in previous notebooks). Elastic net is a generalization of these two algorithms that actually combines them, by folding all three terms into the cost function: the
OLS cost, the ridge L2 penalty, and the lasso L1 penalty. This notebook will assume familiarity with ridge and lasso regression. For more on these algorithms see this and this notebook. In elastic net, the objective of the function is to building a linear model which minimizes the following cost: Where $\alpha$ and $\rho$ are our two previously mentioned hyperparameters; $n$ is the number of observations; $X$ is the matrix of observations; $w$ is the vector of weights; $|| \cdot ||$ is the L1 or L2 norm (see this notebook for more on this notation); and y is the observed response matrix. When $\rho=0$, this is the cost function for ridge regession. When $\rho=1$, this is the cost function for lasso regression. And when $\alpha=0$, this is the cost function for plain old ordinary least squares. This is actually the most important power of the elastic net. If we play our cards right and check all three of these options along the way while training our model, elastic ridge regression performs, at worst, at least as good as the best LinearRegression (OLS), Ridge, or Lasso model! The algorithm used for solving this problem is extremely non-trivial. The most popular way to implement this is to use coordinate descent. However this is more an "art" notebook than a "science" one so we will simply rely on the form of this algorithm built into scikit-learn! Let's jump into our application! First we will select the apartment-classed buildings from the Manhattan bit of the NYC Buildings dataset, and grab both our target and feature matrices. As with any other scikit-learn built-in classifier, producing a prediction is as simple as a .fit & .predict away. We can plot the result to see what the errors are. Our predictor looks naively accurate-ish to plus-minus 20 units. We can improve this performance significantly at very little cost by making use of hyperparameter search: we can compute a lot of different models (100 different ones by default in ElasticNetCV), and test each model to see which one performs best, and pick that one. I covered this idea extensively in a previous notebook: here. This model explains 88% of the variance in the dataset, going off of the $R^2$ score... ...which, all things considered, is pretty good. An elastic net will always perform as good, or better, than ordinary least squares, ridge, and lasso regression. For this reason it's used more often, in practice, than either ridge or lasso regression alone. In  scikit-learn the latter two algorithms are actually implemented as special cases of the ElasticNet algorithm! How do we play our cards right? ElasticNetCV will, by default, perform cross-validation on the $\alpha$ parameter. However, it cannot be used to do this on $\rho$ (l1_ratio). To select the optimal $\rho$, we will need to optimize two different hyperparameters, simultaneously. This is now cross-validation in two dimensions: a grid search! In the next notebook we will use the scikit-learn GridSearch facility to do exactly that. In this notebook we stepped through setting up and running a rudimentary ElasticNet on the NYC Buildings dataset. The next notebook will look at an application of this same algorithm to this same dataset, this time with grid search and normalization in tow. 