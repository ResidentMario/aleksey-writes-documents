Suppose that we are performing some sort of regression task, and using a machine learning algorithm to do it. That algorithm works by minimizing some kind of cost function: the lower the total cost, the better the solution "looks" to the algorithm, and ultimately we (hopefully) converge on the solution with the lowest cost. Since in regression our targets are interval variables, this cost function involves some kind of "distance" between the value predicted, and the value it is actually assigned in the data. The same is true of the metrics we use to measure performance: think about the $R^2$ score, for example, which is basically quadratically penalized Euclidean distance. In other words, in the regression setting, distance matters. And since the distance between two points is intrinsically closely tied to the probability that those two points are "close" to one another, the machine learning algorithms used for regression tend to have reasonably accurate probability estimates. The same is not true of classification. In classification we use (and algorithms optimize for) all-or-nothing cost functions and metrics. For example, accuracy only measures how often we are correct, and says nothing about how confident we are in being correct. Meanwhile, Gini inequality, used by decisions trees, optimizes for taking the biggest strides towards being as correct as possible.  This is because in classification tasks there is intrinsically no such thing as "distance".  It doesn't make sense to say that the distance between a photo of a bumblebee and a photo of a honey bee is 5, at least not in the same way that it makes sense to say that the distance between the price of this one-bedroom AirBnB and the median price of an average one-bedroom AirBnB in the same neighborhood is $50 (for certain problems error-correcting output codes seem like an interesting approach to this, though). As a result, classification algorithms tend to have much weaker and more systematically biased probability prediction forecasts. For settings in which only the class being assigned matters, and we do not care about how confident we are in that assignment being correct, this is fine. If we do care about our confidence (for example, we are competing in a Kaggle competition whose metric is log-loss), this is a major problem. In this notebook I look at classifier algorithm probability forecasting: performance, biases, how to diagnose it, and how to fix your outputs by calibrating your results. This notebook is my notes on the "Probability calibration" section of the sklearn user guide. We'll need some data. For the purposes of this notebook I'll use (versions of) the following synthetic, two-class classification dataset, generated by the make_classification method packaged into sklearn: A great way of checking how a classifier's probability forecasting is performing on your dataset of interest is using a so-called calibration curve. The recipe for building a calibration curve is as follows: The calibration curve works by sorting the probabilities assigned to the records being predicted by the probability reported by the classifier. It then bins the values, and calculates two things. The fraction_of_positives is the percentage of records in the chosen bin which actually belong to the dominant class. This is determined by looking at what values these points are actually assigned in $y$, and it is the emperical truth. The second thing returned, mean_predicted_value, is the mean probability of these points belonging to the dominant class reported by the algorithm. If the classifier is forecasting probabilities (via predict_proba, if the classifier supports this method; more on that later) that are accurate, then we expect the percentage of dominant class classifications and the mean probabilities assigned to the dominant classes in each bin to be close to one another. If it is not doing so accurately, then we will see these two values diverge. We can then display our results in a simple line plot to inspect what we got: We see here that LogisticRegression generates probability predictions that are extremely close to optimal. Some of this is due to the simplicity of the dataset that we are using, but most of this is due to logistic regression itself. Logistic regression tends to have quite accurate probability predictions because it is optimizing log-odds, which is just a convenient restatement of class probability! In other words, probability figures directly in the cost function that LogisticRegression solves for, and hence, unsurprisingly, the algorithm produces unbiased probability estimates. Here's an example of an algorithm which does this poorly: GaussianNB. Naive Bayes algorithms will tend to push probabilities relatively near to, but not all the way at, 0 and 1. This is due to the fact that this algorithm assumes conditional independence of every feature in the dataset, a fairly ridiculous assumption that breaks down very quickly when there is redundancy in the dataset (e.g. columns containing similar information, and hence being non-independent). In fact, the Naive Bayes algorithm is a great demonstration of how probability is not strongly relevant to pure classification tasks (assigning 0s and 1s). From my notes on the algo (here): In practice the probabilities generated by the naive Bayes classifier are highly suspect. However, when we use these probabilities to classify, assigning each point to the  $y$  class that maximizes its likelihood, we get a result that works remarkably well remarkably often. Here is an sklearn plot comparing the calibration curves for a number of the most popular classification algorithms (originally here):  Explaning the shapes of the curves for the other two algorithms in this graph, the documentation says: RandomForestClassifier shows the opposite behavior: the histograms show peaks at approximately 0.2 and 0.9 probability, while probabilities close to 0 or 1 are very rare. An explanation for this is given by Niculescu-Mizil and Caruana [4]: "Methods such as bagging and random forests that average predictions from a base set of models can have difficulty making predictions near 0 and 1 because variance in the underlying base models will bias predictions that should be near zero or one away from these values. Because predictions are restricted to the interval [0,1], errors caused by variance tend to be one-sided near zero and one. For example, if a model should predict p = 0 for a case, the only way bagging can achieve this is if all bagged trees predict zero. If we add noise to the trees that bagging is averaging over, this noise will cause some trees to predict values larger than 0 for this case, thus moving the average prediction of the bagged ensemble away from 0. We observe this effect most strongly with random forests because the base-level trees trained with random forests have relatively high variance due to feature subsetting." As a result, the calibration curve also referred to as the reliability diagram (Wilks 1995 [5]) shows a characteristic sigmoid shape, indicating that the classifier could trust its "intuition" more and return probabilties closer to 0 or 1 typically. And: Linear Support Vector Classification (LinearSVC) shows an even more sigmoid curve then the RandomForestClassifier, which is typical for maximum-margin methods (compare Niculescu-Mizil and Caruana [4]), which focus on hard samples that are close to the decision boundary (the support vectors). Note that both of the examples we've generated thus far have used the predict_proba function provided by the underlying classifier. Some classifiers, however, do not provide this function by default. In particular, to get predict_proba with SVC (support vector classifier), you must specifically pass proba=True; and the probability must be generated via bootstrapping, a very expensive process that will greatly slow down how quickly the algorithm is trained. Knowing that biases exist in the probabilities returned by our classifiers, do we have some way of correcting for them? Yes! In fact, this is a well-known problem, and one that has been solved in practice principally in one of two ways. By the way, note that probability calibration works on any ranked scores, not just on probabilities. So for example, in the support vector classifier case, seeing how expensive probabilities are to generate, we may choose instead to use "distance from the nearest support vector" as our score. The classical, parametric approach to probability calibration is called Platt scaling. Let $x_i$ be a record of interest, and let $f(x_i)$ be the probability assigned to the record by the classifier. Arbitrarily let's label the two classes $-1$ and $+1$, such that $\text{sign}(f(x_i))$ is the class assigned to that record by the classifier. What we want is $P(y=1 \: | \: x_i)$; the probability that, given that we observe $x_i$, we belong to the class $y=1$. Then solve for the following equation: This is the logistic regression equation! $A$ and $B$ are scaling parameters, to be determined at fitting time (using some kind of maximum likelihood estimation algorithm), which control how the scaling is applied. They are calculated by applying a maximum likelihood estimation algorithm, one which solves for the smallest difference between the mean probability and the true probability within each neighborhood or bin (the precise measurement is an implementation detail). Thus all Platt scaling does really is apply a logistic transformation to the classifier probability outputs. Looking at the calibration curves for support vector classifiers and random forests we see that all of these algorithms have a sigmoid shape to the error in their probability predictions. The logistic transformation is a convenient way of, given prediction errors in this shape, smoothing the probabilities out into a linear distribution, resulting in probabilities that are (hopefully) closer to the truth. The alternative non-parametric approach to this problem is isotonic scaling. Isotonic regression is a regression algorithm which fits a piecewise constant, non-decreasing function to a distribution of data. Here is an example of a fitted isotonic regression result:  Isotonic regression is occassionally useful on its own, but it finds its primary application in solving this scaling problem. Whilst Platt scaling applies a logistic transform to the scores generated by the underlying classifier, isotonic regression applies isotonic regression to them, but the end goal (minimize cumulative neighborhood probability error) is the same. Both forms of classifier probability calibration are provided by the handy CalibratedClassifierCV transform. Because in both cases we are essentially performing machine learning on top of machine learning, it's important, to avoid overfitting, that the data used for the correction be disjoint from the data used for training the classifiers. Which of the two algorithms you use to perform the calibration can be specified at declaration time (sigmoid, which is Platt, or isotonic). This transform comes with built-in cross validation; by default three folds are used and averaged, but you may specify your own number, if you so desire. The plots that follow show recipes for applying these transformations to your data, and the result that they have on the calibration curves (and thus, the goodness-of-fit of the probabilities). In this example the RandomForestClassifier was already returning relatively accurate probability scores, so a correction wasn't strictly necessary. Still, we see that applying the Platt transformation did perhaps further reduce the bias. This plot is more interesting. In the blue we have the calibration curve for the original GaussianNB classifier. In the yellow is the recalibrated curve we generated with a Platt correction. We see here that the Platt correction actually did more harm than good! When we look at the original calibration curves for the various algorithms, we see that for the data used in that example, GaussianNB did not truly have the same sigmoidal error structure that SVC and RandomForest have. For this data, it overshoots the low probabilities by too little, and undershoots the high-probability data by too much. Platt calibration works when the amount of bias on either side is close to the same, and it fails (rather to my surprise) here. Isotonic calibration, on the other hand, really gets the job done. We see here that the isotonic adjusted probabilities, in red, are a very close match to the true mean probabilities. In this notebook we discussed the role that probability plays (and doesn't play) in classification algorithms, and two strategies for calibrating probabilities or more general ranked scores created by our classifiers so as to better fit the true probability distribution of the data. Isotonic regression seems to have better fitting performance, and in the general case it does. However, keep in mind that this is only true for sufficiently large datasets (ones with over 10,000 samples). Isotonic calibration has a strong tendancy to overfit., as non-parametric methods are want to do,, so you should only use it for such problems. 