Lasso regression is ordinary least squares plus an L1 norm. In terms of the math, this translates to: In ridge regression, the cost function has a derivative. Therefore it's possible to solve ridge regression using gradient descent (we demonstrated this in a previous notebook). However, lasso regression involves an absolute value, which is not differenciable. So we can't use gradient descent to solve it! It also, unlike ridge regression, doesn't have an analytic solution. So there's no easy matrix formula either! In this notebook, we'll implement a univariate lasso regression algorithm using soft thresholding. This solution is taken from this blog post. To start with, recall (from multivariate calculus) that the Taylor approximation of a convex function at any point is stricly smaller than the value of the function at that point itself. This can be stated formally as: Lemma 1: Given a convex function $f  : S \to \mathbb{R}$ mapping onto a convex real set $S$, $\exists g \in \mathbb{R}$ such that: The trouble is that $f$ (the cost function) is not a differenciable function in lasso regression. Interestingly, we can get around this by defining something known as a subdifferential. Definition 1: The subdifferential $\partial f$ of a convex function $f$ is given by: A subdifferential is basically any slope value that's less than the actual slope value between two points. Because a slope can't be infinite, we know that an infinite amount of subdifferentials exist for any two points on a function curve. For example, if the difference between $f(b)$ and $f(a)$ is $n$, possible subdifferentials include $n / 2$, $2n / 3$, $3n / 4$, and so on. We will also do some hand-waving in the proof, by using the Moreau-Rockafeller theorem. This goes like so: Moreau-Rockafellar theorem: $\partial [f(x) + g(x)] = \partial f(x) + \partial g(x)$ Now the proof. Proof: A soft-threshold approximation for $\min_\beta \{ (y-x\beta)^2 + \lambda | \beta | \}$, the univariate lasso cost function. We can use this function in gradient descent to approximate it to win it! (note that in keeping with scikit-learn semantics, in the implementation below we replace $\lambda$ with $\alpha$). Because we're solving a univariate equation, it actually doesn't matter what value of $\alpha$ we choose, the result will always converge to a linear regressive solution. 