In a previous notebook we used the scikit-learn ElasticNet algorithm to predict the number of units in an apartment building using various information known to us from the NYC Buildings dataset about the building's size. Neat! If you haven't seen it yet, start there, as this is the second notebook in the series. In this notebook we'll take things one step further by doing this prediction with grid search cross validation as well. Before we do that however we need to address a big problem with our model. There is a problem with our model, however. Take a look at the $\alpha$ value that the parameter search we used in the last notebook selected: This value is huge. The reason why is pretty simple: it has to be pretty large to account for the fact that our data is in terms of some random whole numbers (between 1 and ~100 units in a building, or more). Different kinds of numerical data is spread across different possible values. Sometimes the numbers are really big: populations measured in millions, or economies measured in trillions. Sometimes the numbers are really small. Taken on its face, the hyperparameter search doesn't know what kind of spread the data has. scikit-learn has some built-in robustness to deal with this issue, so this $\alpha$ is actually pretty good; but good corrective $\alpha$ numbers can get into figures with 40+ digits, if the values being normalized are in units that are big enough. At some point you can even get an integer overflow! We can use feature scaling to make the model parameters more sane. This will also enable us to compare the values outputted by our model against those outputted by other models, aiding in cross-interpretability. Let's look at a few feature scaling approaches.We'll model these on NumFloors: The simplest kind of feature scale is just rebounding everything inside of a [0, 1] or [-1, 1] box, using the min and the max. This is simply called "rescaling": Doing this redistributes our data to be between 0 and 1. Here's a version that does essentially the same thing, but normalizes the mean to 0: Although it's sizeably more complicated, standardizing the data is probably the most common transformation used in machine learning. The new $x'$ is both scaled (by the standard deviation) and recentered (so that the mean point is now at 0). This transformation is extremely popular because it will transform any normally distributed data to the $N(\bar{x}=0, \sigma^2(x) = 1)$ distribution: the centered, unscaled normal distribution. Since the normal distribution is so important to machine learning (and all of statistic really), and so much data is distributed normally or quasi-normally, this is a really useful transformation to make use of. We can use any one of these rescaling algorithms pretty much interchangeably. For no particular reason, let's pick standardization. If we run a cross-validated parameter search now, here's what we get: Looking at this $\alpha$ value, we can immediately say that the amount of regularization we are doing is relatively insignificant. This is a very small $\alpha$, and recall from the equation that this is the number that gets multiplied against the L1 and L2 terms. That this number is this small indicates that the regular OLS term is doing most of the work! That being said, when we inspect the result we see that our cross-validated elastic net is still in fact doing some feature selectionâ€”it's throwing out the two least important variables (by giving them coefficients of 0): What does a "high" value of $\alpha$ look like? For normalized data, the following chart (from this extremely recommendable article) is a good rule of thumb. It demonstrates the effect that various values of $\alpha$ have on ridge regression applied to a polynomial regressor:  With feature scaling in our toolbelt we can move on to understanding grid search! The ElasticNetCV algorithm we introduced in the previous notebook is great because it finds us the optimal $\alpha$ (just about); but $\alpha$ is only one of our hyperparameters. We also have a $\rho$ (l1_ratio) to worry about. We can use grid search to optimize both of these hyperparameters simultaneously. The idea behind grid search is simple. We take a sequence of values that we want to try out in one dimension; then we take a sequence of values that we want to try out in another dimension. And possibly so on, if we have three or even more hyperparameters to try out. We try each possible combination of values, and return the parameterized model that performed the best, according to whatever our fitness metric was. If we want to test 10 $\alpha$ values and 10 l1_ratio values, for example, we would need to test $10 \times 10 = 100$ different models. Grid search is a very simple but very powerful idea that can be expressed in not that many lines of code: We use it thusly: As shown above, we get a model with an alpha of 0.001 and a l1_ratio of 1. In other words, grid search picked ridge regression with a small $\alpha$ value as the best model! The coefficients demonstrate why: it seems that this is the $\alpha$ which best pruned the input variables to just the right ones, reduced the overall variable noise, and increased the test-set accuracy overall. The scikit-learn equivalent to our hand-implemented GridSearchCV (which comes with many more features obviously) is: In the meantime, the lasso regression model is our winner! 