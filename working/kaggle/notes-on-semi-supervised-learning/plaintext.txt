For some kinds of data you will run into the problem of having many samples, but not having labels for all of those samples, only for a subset of them. This situation occurs particularly often in research contexts, where it's often easy to get a small number of labelled data points, via hand-labelling, but significantly harder to gather the full dataset, if the full dataset is sufficiently large. This is known as the semi-supervised learning problem. It is semi-supervised because it lies in between unsupervised learning, which does not use labels, and supervised learning, which requires them. In a semi-supervised learning problem you don't have all the labels or none of them, only some of them. If the set of labels is sufficiently large, standard machine learning algorithms like kNN and SVM classification may be used to assign classes to unlabeled data. If the number of labels available is very small, however, the performance of these algorithms can be suboptimal. A set of techniques and algorithms specific to this problem exists. These are the semi-supervised learning techniques. sklearn implements a pair of classifiers for this task. These are LabelPropagation and LabelSpreading. The requisite section of the sklearn documentation (http://scikit-learn.org/stable/modules/label_propagation.html#semi-supervised) is a wee bit light on the details, but basically (1) LabelSpreading is a regularized version of LabelPropagation and (2) they work by building a self-similarity matrix on the dataset, then classifying each unlabeled point by finding the existing labeled point it is most similar to. Thus these algorithms generalize to a wide variety of spaces, like e.g. circles:  On the other hand, they are very computationally expensive. If there are $n$ points in the dataset, building a self-similarity matrix requires $O(n^2)$ comparison operations. Thus these algorithms do not scale well! All that being said, semi-supervised learning is not that different from missing data imputation. I would argue that the two problems are just rephrasings of one another: semi-supervised learning is classifying missing dependent variables, while missing data imputation is classifying missing predictor variables. So these algorithms can be used to do that as well. Since imputing missing data is likely to be a vastly more common problem than generalizing labels, it is worth keeping these techniques in mind, especially for missing data imputation from a small number of samples. For this reason I include them in my notes on "Simple techniques for missing data imputation". I will demonstrate these algorithms in action. First, we'll use the following synthetic dataset. The clusters will be tuned to have different degrees of class separation, but will basically all be distributed according to the following template: Here is the recipe for applying LabelPropagation to the data. If you are familiar with the sklearn style, this should seem intimately familiar by now: Now some code: In the plots above I show class clusters with different levels of class separation, from almost none to almost complete. In this plot I show, for each of these levels, the performance of the LabelPropagation labeling algorithm: how well it performs on each class, and also which points exactly it gets wrong. Note that the presence of a handful of wayward points in the dominant clusters (blue and white) is actually a suprising artifact of make_classification. As you can see, class differenciability is extremely important for successful labeling. This hopefully isn't surprising. Respective to using standard machine learning algorithms for this task, the one adaptation that these semi-supervised algorithms make is that they generalize better when there are very few labels. Here I demonstrate the LabelSpreading algorithm. As you can see, the performance of this algorithm on this test dataset is almost synonymous with that of LabelPropagation (and again, note that the misclassified points in 3 and 4 are mostly make_classification artifacts!). Note that the algorithms provide regularization and kernel hyperparameters that I will not tune or explore here. For this synthetic dataset it will not make much of a difference, but the higher the level of noise in the dataset the more important parameter tuning becomes. I tried applying this algorithm to a real dataset, the Open Powerlifting Database, to assign Division information to fields lacking it. However, this turned out to be foolish, as the class clusters were not sufficiently well-distributed. The accuracy was nearly 0. Oh well! Here's the code for this: Semi-supervised learning is a restatement of the missing data imputation problem which is specific to the small-sample, missing-label case. This problem gets its own name likely because it is so commonly encountered in research and dataset generation contexts. It's a useful tool to know about more generally for missing data imputation from a limited sample size, but the algorithms have poor performance characteristics for larger samples. In those cases, perhaps try applying machine learning to the problem directly. 