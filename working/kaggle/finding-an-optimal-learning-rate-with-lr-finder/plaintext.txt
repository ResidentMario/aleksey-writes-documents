lr_finder is a utility function included in the neural network library that comes complimentary with the fast.ai course. The function was initially specified as a "useful heuristic" in "Cyclic learning rates for training neural networks"â€”although the subject of the paper is somewhat different from lr_finder. The function has a Keras implementation in the form of keras_lr_finder. Implementing the LR finder algorithm isn't difficult. It trains the model one at a time, resetting the fitted result back to the original random values after each batch is fitted, then incrementing the learning rate in the search space logorithmically on its path to the maximum learning rate. The number of data points tried in the logorithmic search space is equivalent to epochs * n_batches. Here is the implementation, ripped directly from the helper library: Let's demonstrate how this works using the snake eyes dataset. The point of optimality in the initial training rate is the point on the training rate plot where the slope is steepest in the downwards direction. That's because this plot shows the loss after one epoch. Points before the steepest slope are training too slowly. Points after the steepest slope are at risk of training too quickly: usually, but not always (it didn't happen in this demo case), they will fall off the mountain in terms of loss because they jump past the point of optimality. In this case we see that a good initial learning weight value is halfway between 10e-3 and 10e-2 (0.01 and 0.001). That's around 0.005. The learning rate that you derive this way is probably only good as a starting point. You can still benefit from the usual downtuning procedures for optimizing the learning rate later in model training. To learn more about learning rate tuning read the blog post "Estimating an optimal learning rate for a deep neural network". 