
 Ludwig is Uber's AutoML solution. It's open source, and very likely the best way to perform AutoML on Spell (in fact, we've already had at least one user deploy it on Spell)—hence the write-up. It's often marketed as a "no code ML" tool, to cash in on that particular hype cycle. 
 Ludwig uses datatype-specific encoders and decoders, along with combiners. Encoders map raw input data (e.g. images, text, categories) to tensors. Decoders perform map tensors to outputs (e.g. binary categories values), or perhaps to intermediate values which are to be re-encoded. Combiners apply ops to the encoded data (e.g. sheer or concat). A preprocessing pass in Ludwig consists of some DAG of these steps. Some examples cited in the README are: Here's it is in picture form: The combination of operations you will use can be specified using either a CLI or using the Python library. In the CLI case, a model definition YAML is used, which has the following basic shape: Then: Ludwig has separate train, predict, and visualize CLI paths. These configs can get very very complicated. Under the hood, a TF2 model is (usually?) used; the YAML specification is basically a macro-language for parameterizing that TF2 model. This style of heavily parameterizable model is consistent with a sufficiently "heavily-developed" model—e.g. the model I used for the Bob Ross has its own extensive YAML specification. In other words, you can essentially think of Ludwig as a bigger blacker box. Using Ludwig means entrusting Ludwig with all of your model parameterization. This has all of the advantages and disadvantages of taking your model building to an even higher level of abstraction.  On the plus side, making changes like switching to multi-task learning is as simple as changing a config, and you have a bolt-on visualization suite you can use. On the minus side, you give up fine-grained control of the code. 
 These notes come from a detailed reading of the User Guide. 
 Preprocessing files Ludwig applies a preprocessing pass to re-encode data expressed in its raw form at source into tensor format suitable for model input. When writing a pure PyTorch model, you could either do this work ahead of time and save it to a file (e.g. transform a CSV into in an HDF5 of tensors in a preprocessing pass), or do it just-in-time by constructing a Dataset yourself. When using Ludwig, Ludwig does it for you. As part of its implementation, it generates some intermediate files: an HDF5 file with the raw tensors, and a JSON file containing idx2str, str2idx, and str2freq mappings (as is needed for decoding purposes). These intermediate files are generated once, the first time the model is built, and then reused thereafter. Changing any parameters that affect the encoding with invalidate the cached files and cause them to be regenerated. You can skip generating these files completely using the --skip_save_processed_input flag. 
 Train-test split By default, Ludwig will use a 70-20-10 train-validation-test random split. If the input -dataset has a split column, whose values are in the range {0,1,2}, the contents of that column will be used instead (unless you specify --force_split). There are configuration options for specifying the percentages and for specifying that you want stratified sampling. Alternatively you can pass {--training_set, --validation-set, --test_set} instead of -dataset. Both the validation and test set are optional. If the validation set is omitted, the model will be trained for the full number of epochs. If it is specified, early stopping will be used (again, this behavior is the default, and adjustable with CLI options). 
 Disk output Ludwig has a two-tier organizational hierarchy: experiment → model → run. You specify these via (optional, but highly recommended) --experiment_name and --model_name flags. Training artifacts are written to ./results by default (change this using the --output_directory flag). The footprint of this directory is rather large: Notably, the tfevents file is pluggable with TensorBoard. 
 Checkpointing and resumption Ludwig tracks the best model seen so far and the latest model trained by epoch. The former is a common model optimization tactic (the default, pretty much), the latter, a mechanism for restarts. Model resumption is achieved using the --model_resume_path argument. To load a model from a checkpoint and apply a new training regimen to it (e.g. to fine-tune a model), use --model_load_path instead. Random seed parameterization is achieved using the --random_seed option. TensorFlow has that weird idiosyncrasy where training on GPU involves some asynchronicity which makes training non-deterministic, so certain layers will effectively invalidate your random seed. 
 GPU consumption Since Ludwig is TensorFlow-based, it has the annoying TensorFlow behavior of preallocating all GPU memory immediately. You can specify a soft memory limit using --gpu-memory-limit, which will only preallocate the chosen amount of memory, making TensorFlow make requests to the OS for any additional GPU memory it wants over the course of execution (this is the default memory management mode in PyTorch, and it makes GPU consumption much easier to track). 
 Model hyperparameters Part of the service that Ludwig provides to you is sane defaults: Adam optimizer with default parameters and early stopping. These can be tuned by changing config settings. The corresponding section of the docs has plenty of details on this process. 
 The predict API is broadly similar to the train one. It's simpler, really. Many of the same arguments apply. The --batch_size argument is considered useful for optimizing scoring time (you generally want to go as big as will fit in memory). For evaluation, use the evaluate API. This is again pretty simple and pretty readable. Obviously evaluate has the additional requriment (relative to predict) that the data passed to it must contain the target variable y. Then there is the experiment API. This has additional parameter, --k_folds, which specifies how many folds to use for a KFold cross validation scheme. KFold cross validation combines train and evaluate steps, so this API has options related to both. There is a serve option. Whatever. 
 
 The input_features field in the configuration contains a list of dictionaries, each one containing, at a minimum, the name and type fields. Each type (choose from the list above) has a default encoder, but you can overwrite that choice by specifying a different encoder as a key. The parameter list available to the encoder is determined by the type of encoder chosen. 
 Ludwig has a concept of tied weights in the encoder specification. This is a tied_weights parameter on a config that specifies, by name, which other field(s) the encoder should be shared with. This can be used to e.g. encode two different bodies of text in a dataset using the same encoder. The example provided is:   